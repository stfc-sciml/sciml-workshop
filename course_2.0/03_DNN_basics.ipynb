{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN: the Basics\n",
    "\n",
    "In this notebook, we will learn the basics of a Deep Neural Network (DNN) based on [Keras](https://keras.io/), a high-level API for building and training deep learning models, running on top of [TensorFlow](https://www.tensorflow.org/), an open source platform for machine learning. \n",
    "\n",
    "We will use the `fashion-mnist` dataset, which is useful for quick examples when learning the basics. In [03_DNN_practical.ipynb](03_DNN_practical.ipynb), we will follow the same principles to practice with a more relevant scientific dataset. To understand how a DNN works, we will implement a fully connected DNN from scratch in [03_DNN_start_from_scratch.ipynb](03_DNN_start_from_scratch.ipynb), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# check version\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "To start with, we load the `fashion-mnist` dataset from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# normalise images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# string labels\n",
    "string_labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# print info\n",
    "print(\"Number of training data: %d\" % len(train_labels))\n",
    "print(\"Number of test data: %d\" % len(test_labels))\n",
    "print(\"Image pixels: %s\" % str(train_images[0].shape))\n",
    "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly plot some images and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot an image in a subplot\n",
    "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n",
    "    plt.subplot(nrows, ncols, iplot + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.xlabel(label, c='k', fontsize=12)\n",
    "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# ramdomly plot some images and their labels\n",
    "nrows = 4\n",
    "ncols = 8\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n",
    "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n",
    "    subplot_image(train_images[idata], label, nrows, ncols, iplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by DNN\n",
    "\n",
    "Here we will create and train a DNN model to classify the images in `fashion-mnist`. With Keras, the task can be divided into three essential steps:\n",
    "\n",
    "1. Build the network architecture;\n",
    "2. Compile the model;\n",
    "3. Train the model.\n",
    "\n",
    "These steps may be repeated for a few times to improve the quality (accuracy and stability) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the network architecture\n",
    "\n",
    "Our first DNN will be a simple multi-layer perceptron with only one hidden layer, as shown in the following figure:\n",
    "\n",
    "\n",
    "![dense.jpeg](https://i.ibb.co/0yrVL28/dnn.png)\n",
    "\n",
    "\n",
    "\n",
    "In general, a network of this kind should include an input layer, some hidden layers and an output layer. In this example, all the layers will be `Dense` layers.\n",
    "\n",
    "\n",
    "### The input layer\n",
    "\n",
    "We first need to determine the dimensionality of the input layer. In this case, we flatten (using a `Flatten` layer) the images and feed them to the network. As the images are 28 $\\times$ 28 in pixels, the input size will be 784.\n",
    "\n",
    "\n",
    "### The hidden layers\n",
    "\n",
    "We use one hidden layer in this case and use `ReLU` as its activation function:\n",
    "\n",
    "> $R(x)=\\max(0,x)$\n",
    "\n",
    "**NOTE**: Different activation functions are used for different tasks. Remember that `ReLU` generally performs well for training a network, but it can *only* be used in the hidden layers.\n",
    "\n",
    "\n",
    "### The output layer\n",
    "\n",
    "We usually encode categorical data as a \"one-hot\" vector. In this case, we have a vector of length 10 on the output side, where each element corresponds to a class of apparel. Ideally, we hope the values to be either 1 or 0, with 1 for the correct class and 0 for the others, so we use `sigmoid` as the activation function for the output layer:\n",
    "\n",
    "> $S(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the summary of the model using `model.summary()`. The number of trainable parameters of a layer = $P\\times N+N$, where $P$ is the size of its precedent layer and $N$ its own size. Here $P\\times N$ accounts for the weights of the $P\\times N$ connections and $N$ for the biases of this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile the model\n",
    "\n",
    "Next, we need to compile our model. This is where we specify all sorts of hyperparameters associated with the model. Here are the most important ones:\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss is the objective function to be minimised during training. Gradients of the loss with respect to the model parameters, calculated by backpropagating the errors, determine the direction to update the model parameters. Follow [03_DNN_start_from_scratch.ipynb](03_DNN_start_from_scratch.ipynb) to learn the details about *backpropagation* and *gradient descent*. In this case, we will use `SparseCategoricalCrossentropy` as the loss. The term *sparse* means that the output vector is sparse, with many more zeros than ones in the one-hot encoding.\n",
    "\n",
    "\n",
    "### Optimiser\n",
    "\n",
    "An optimiser is an algorithm determining how the model parameters are updated based on the loss. One critical hyperparameter for the optimiser is the *learning rate*, which determines the magnitude to update the model parameters (also see [03_DNN_start_from_scratch.ipynb](03_DNN_start_from_scratch.ipynb) for details). In many applications, *Adam* is usually a good choice at the beginning. We will use Adam in this example.\n",
    "\n",
    "\n",
    "### Metric\n",
    "\n",
    "The metrics do not affect the training result but monitor the training process to give us a sense of how well the model is improving after seeing more data. We can also use them to choose between models at the end. In our case, we will monitor the `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "Now we can start to train the model with `fashion-mnist`. This is done by calling the `model.fit()` method, where we need to specify a few more parameters:\n",
    "\n",
    "### Epochs\n",
    "\n",
    "It is the number of times that the model will run through the entire dataset during training.\n",
    "\n",
    "\n",
    "### Batch size\n",
    "\n",
    "It determines how many data will be used at a time to determine the gradient used for parameter update. Follow [03_DNN_start_from_scratch.ipynb](03_DNN_start_from_scratch.ipynb) to learn more about *Batch*, *Mini-batch* and *Stochastic Gradient Descent*.\n",
    "\n",
    "\n",
    "### Validation data\n",
    "Accuracy and loss can be computed and logged with a validation dataset passed to `model.fit()`. To make predictions with a confidence equivalent to that for training, the accuracy for the validation data should not differ too much from that for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "training_history = model.fit(train_images, train_labels, epochs=50, batch_size=32, \n",
    "                             validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training history\n",
    "\n",
    "We can examine the training history by plotting accuracy and loss against epoch for both the training and the test data. \n",
    "\n",
    "Notice that the accuracies for the training and the test data diverge as the model trains. This is a classic symptom of [overfitting](https://en.wikipedia.org/wiki/Overfitting), that is, our model corresponds too closely to the training data so that it cannot fit the test data with an equivalent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.figure(dpi=100, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n",
    "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "# plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history.history['loss'], label='Loss on training data')\n",
    "plt.plot(training_history.history['val_loss'], label='Loss on test data')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rgularise and re-train\n",
    "\n",
    "Dropout, also called dilution, is a regularisation technique to mitigate against overfitting by randomly omitting a certain amount of neurons from a layer. Here we will rebuild our model with `Dropout` between the hidden and the output layers. Let us see whether this can negate the overfitting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network architecture\n",
    "model_reg = Sequential()\n",
    "model_reg.add(Flatten(input_shape=(28, 28)))\n",
    "model_reg.add(Dense(128, activation='relu'))\n",
    "model_reg.add(Dropout(0.4))\n",
    "model_reg.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model_reg.compile(optimizer='adam',\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "training_history_reg = model_reg.fit(train_images, train_labels, epochs=50, batch_size=32, \n",
    "                                     validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.figure(dpi=100, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history_reg.history[acc_str], label='Accuracy on training data')\n",
    "plt.plot(training_history_reg.history['val_' + acc_str], label='Accuracy on test data')\n",
    "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data (no dropout)')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "# plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history_reg.history['loss'], label='Loss on training data')\n",
    "plt.plot(training_history_reg.history['val_loss'], label='Loss on test data')\n",
    "plt.plot(training_history.history['val_loss'], label='Loss on test data (no dropout)')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make predictions\n",
    "\n",
    "Finally, we can use our trained model to make predictions. Here we show some wrong predictions for the test data, from which we may get some ideas about what kinds of images baffle our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test images to make predictions\n",
    "pred_lables = model_reg.predict(test_images).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of wrong predictions\n",
    "id_wrong = np.where(pred_lables != test_labels)[0]\n",
    "print(\"Number of test data: %d\" % test_labels.size)\n",
    "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n",
    "\n",
    "# plot the wrong predictions\n",
    "nrows = 4\n",
    "ncols = 8\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n",
    "    label = \"%d: %s\" % (test_labels[idata], string_labels[test_labels[idata]])\n",
    "    label2 = \"%d: %s\" % (pred_lables[idata], string_labels[pred_lables[idata]])\n",
    "    subplot_image(test_images[idata], label, nrows, ncols, iplot, label2, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Change some hyperparameters in `model.compile()` and `model.fit()` to see their effects (see reference of [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)); \n",
    "* Use two hidden layers (with dropout), e.g., respectively with sizes 256 and 64, and see whether the accuracy can be improved or not;\n",
    "* Change the output from 0-1 binary to probability, i.e., the one-hot vector represents the probabilities that an image belongs to the classes; this can be achieved by 1) removing `activation='sigmoid'` from the output layer and 2) appending a `Softmax` layer after the trained model, e.g.,\n",
    "```python\n",
    "probability_model = Sequential([model_reg, keras.layers.Softmax()])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
