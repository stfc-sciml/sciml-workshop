{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Practical: Ag Detection by Muon Spectroscopy\n",
    "\n",
    "In this notebook, we attempt to solve a real problem in physics using a fully connected DNN.\n",
    "\n",
    "We have a set of spectra from Muon spectroscopy experiments, from which we would like to detect whether or not a certain element is present in a sample. In this notebook, we are going to train a neural network to detect the presence of Ag. Through this practice, we will encounter and overcome two pitfalls in deep learning, **class imbalance** and **overfitting**. We will also explore **early stoping** and saving checkpoints from the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# check version\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n",
    "\n",
    "# helpers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# need some certainty in data processing\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Boilerplate\n",
    "\n",
    "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. To access the data, you need to:\n",
    "\n",
    "1. Run the first cell;\n",
    "2. Follow the link when prompted (you may be asked to log in with your Google account);\n",
    "3. Copy the Google SDK token back into the prompt and press `Enter`;\n",
    "4. Run the second cell and wait until the data folder appears.\n",
    "\n",
    "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will take no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables passed to bash; do not change\n",
    "project_id = 'sciml-workshop'\n",
    "bucket_name = 'sciml-workshop'\n",
    "colab_data_path = '/content/sciml-workshop-data/'\n",
    "\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    google_colab_env = 'true'\n",
    "    data_path = colab_data_path\n",
    "except:\n",
    "    google_colab_env = 'false'\n",
    "    ###################################################\n",
    "    ######## specify your local data path here ########\n",
    "    ###################################################\n",
    "    data_path = './sciml-workshop-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n",
    "\n",
    "# running locally\n",
    "if ! $1; then\n",
    "    echo \"Running notebook locally.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# already mounted\n",
    "if [ -d $2 ]; then\n",
    "    echo \"Data already mounted.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# mount the bucket\n",
    "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "apt -qq update\n",
    "apt -qq install gcsfuse\n",
    "gcloud config set project $3\n",
    "mkdir $2\n",
    "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "### Read raw data\n",
    "\n",
    "The raw data, which include the constituent elements and the Muon spectra of the samples, are stored in the pickle file `muon/Ag_muon_data.pkl`. We load this file into a `pandas` dataframe and take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_pickle(data_path + 'muon-data/ag-muon-data-tight.pkl')\n",
    "#print dimensions\n",
    "print('Number of samples in the dataset: %d' % len(df['Spectra']))\n",
    "print('Length of spectra for each sample: %d' % len(df['Spectra'][0]))\n",
    "\n",
    "# print the first few data\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, the `Elements` and the `Spectra` columns show respectively the elements and the spectra of the samples. There are 224,000 samples in the dataset, and each spectrum is a series of 300 positive reals. \n",
    "\n",
    "To get a feel for the complexity of picking out signals with Ag in multinary samples, we can plot some random spectra for three representative cases: \n",
    "\n",
    "* no Ag\n",
    "* pure Ag\n",
    "* Ag-Si binary\n",
    "\n",
    "Note that we are plotting only the first half of each spectrum. Change `[0:150]` to `[:]` to show the full spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions to select data\n",
    "conditions = [\n",
    "# no Ag\n",
    "('no Ag', np.where(['Ag' not in elements for elements in df['Elements']])[0]),\n",
    "# pure Ag\n",
    "('pure Ag', np.where([['Ag'] == elements for elements in df['Elements']])[0]),\n",
    "# Ag-Si\n",
    "('Ag-Si binary', np.where([['Ag', 'Si'] == elements for elements in df['Elements']])[0])\n",
    "]\n",
    "\n",
    "# plot\n",
    "ncond = len(conditions)\n",
    "nplot = 4 # number of plots per condition\n",
    "fig, axs = plt.subplots(nplot, ncond, dpi=200, figsize=(ncond * 5, nplot * 2), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=.1, hspace=.2)\n",
    "for icond, cond in enumerate(conditions):\n",
    "    for iplot, idata in enumerate(np.random.choice(cond[1], nplot)):\n",
    "        axs[iplot, icond].plot(df['Spectra'][idata][0:150], c='C%d' % icond)\n",
    "        axs[iplot, icond].set_xlabel('Sample %d: %s' % (idata, cond[0]), c='C%d' % icond)\n",
    "        axs[iplot, icond].set_ylim(0, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract training data\n",
    "\n",
    "The input data for our network will be the `Spectra` column, and we can use the `to_list()` method to convert it to a numpy array. A quick inspection over the spectra shows that the second half of each spectrum has mostly vanished and is thus uninformative; therefore, we will *only use the first half of each spectrum for training*. Such simple data preprocessing can help to reduce model scale and improve both accuracy and performance. \n",
    "\n",
    "The output data for our network will be a binary-valued one-hot vector: 0 for no Ag in the sample and 1 otherwise. One-hot encoding can be achieved by a simple for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### input ######\n",
    "# convert the 'Spectra' column to numpy\n",
    "train_x = np.array(df['Spectra'].to_list())[:50000]\n",
    "# only take the first half for training\n",
    "train_x = train_x[:, 0:1000]\n",
    "\n",
    "###### output ######\n",
    "# one-hot encoding: whether Ag is in 'Elements'\n",
    "train_y = np.array(['Ag' in elements for elements in df['Elements']]).astype(int)[:50000]\n",
    "\n",
    "# print data shapes\n",
    "print(\"Shape of input: %s\" % str(train_x.shape))\n",
    "print(\"Shape of output: %s\" % str(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ag-detection by DNN\n",
    "\n",
    "## 1. Try out a network\n",
    "\n",
    "\n",
    "### Build and compile\n",
    "\n",
    "Based on what we have learnt in [03_DNN_basics.ipynb](03_DNN_basics.ipynb), design a simple neural network with `Dense` layers to detect Ag in the spectra. In general, it is not a straightforward task to determine the number of hidden layers and the number of neurons in each layer, which usually involves some trial and error. In this case, our output size is 1, so we'd better add a small layer before it, such as one with size 8; then, between this hidden layer and the input layer (with size 150), we may add another hidden layer with size 64, approximately halfway between 8 and 150. \n",
    "\n",
    "Next, compile the model. We can keep using `adam` for the `optimizer` and `['accuracy']` for the `metrics`. For the `loss`, since we are fitting to a range between 0 and 1, we can choose `binary_crossentropy`.\n",
    "\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# print model summary\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# print model summary\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Since we have not separated a subset of data for validation, we can pass `validation_split=0.2` to `model.fit()`, which then will use the *final* 20% of the dataset for validation. Let us do 20 epochs first.\n",
    "\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# train the model\n",
    "training_history = model.fit(train_x, train_y, epochs=20, batch_size=64, \n",
    "                             validation_split=0.2)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "training_history = model.fit(train_x, train_y, epochs=10, batch_size=64, \n",
    "                             validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history\n",
    "\n",
    "For convenience, we define a function to plot a training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to plot training history\n",
    "def plot_history(training_history):\n",
    "    # plot accuracy\n",
    "    plt.figure(dpi=100, figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n",
    "    plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    # plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history.history['loss'], label='Loss on training data')\n",
    "    plt.plot(training_history.history['val_loss'], label='Loss on test data')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the training history of the current model. They will look bizarre at this stage, as explained in the forthcoming section.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# plot training history\n",
    "plot_history(training_history)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plot_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class imbalance\n",
    "\n",
    "In the above history plot, notice how the accuracy of the model quickly converges to a certain value and then stops improving completely. This value can be well above 50% (above 95% by the suggested answer), implying that the model is actually picking up something from the dataset. More strangely, the accuracy for the validation data can even be higher than that for the training data (always 100% by the suggested answer). These odd results indicate that something could be wrong within our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution\n",
    "\n",
    "Let us inspect the distribution of the data using `plt.hist(train_y)`, paying special attention to the validation part (the final 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of data\n",
    "plt.figure(dpi=100)\n",
    "plt.hist(train_y, label='Whole dataset')\n",
    "plt.hist(train_y[-len(train_y)//5:], label='Validation subset')\n",
    "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('number of data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show that our dataset is dominated by samples labelled 0 or \"no Ag\", which account for over 95% of the data. Thus, if the model simply learns to *guess* \"no Ag\" in every sample, it can achieve 95% accuracy (and 100% for the validation subset) without learning anything meaningful. This problem is known as **class imbalance**.\n",
    "\n",
    "To avoid this, we must balance the classes. There are a number of strategies we can take:\n",
    "\n",
    "* Upsample the minority class;\n",
    "* Downsample the majority class;\n",
    "* Change the performance metric.\n",
    "\n",
    "The best available option for our problem is to downsample the majority class, which can be easily achieved with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find original indices of 0 ('no Ag') and 1 ('with Ag')\n",
    "id_no_Ag = np.where(train_y == 0)[0]\n",
    "id_with_Ag = np.where(train_y == 1)[0]\n",
    "\n",
    "# downsample 'no Ag' to the number of 'with Ag' by np.random.choice\n",
    "id_no_Ag_downsample = np.random.choice(id_no_Ag, len(id_with_Ag))\n",
    "\n",
    "# concatenate 'with Ag' and downsampled 'no Ag'\n",
    "id_downsample = np.concatenate((id_with_Ag, id_no_Ag_downsample))\n",
    "\n",
    "# shuffle the indices because they are ordered after concatenation\n",
    "np.random.shuffle(id_downsample)\n",
    "\n",
    "# finally get the balanced data\n",
    "train_x_balanced = train_x[id_downsample]\n",
    "train_y_balanced = train_y[id_downsample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-exam the histograms of the balanced dataset after downsampling the majority:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of downsampled data\n",
    "plt.figure(dpi=100)\n",
    "plt.hist(train_y_balanced, label='Whole dataset')\n",
    "plt.hist(train_y_balanced[-len(train_y_balanced)//5:], label='Validation subset')\n",
    "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('number of data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train the model\n",
    "\n",
    "Now we can re-train the model with the balanced dataset. Simply change `train_x` and `train_y` to `train_x_balanced` and `train_y_balanced` in `model.fit()` and repeat all the steps in [1. Try out a network](#1.-Try-out-a-network). Note that, to avoid the influence of the initial model state (weights and biases) left by the previous training (such as the one trained with the imbalanced dataset), we have to first re-define and re-compile the model before calling `model.fit()`. A larger `epochs` can be used because we now have much fewer data.\n",
    "\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import time\n",
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "tic = time.clock()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2)\n",
    "print('run time: {:5.3f} s'.format(time.clock() - tic))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "tic = time.process_time()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2)\n",
    "print('run time: {:5.3f} s'.format(time.process_time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plot_history(training_history_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "The history shows that our model is sightly overfitting, with the accuracy for the training data converging to 0.99 and that for the test data to 0.96. This can be alleviated by a dropout layer with a rate of 0.1 after the first hidden layer: try adding `model.add(Dropout(0.001))` after `model.add(Dense(64, ...))`. We also lower the learning rate to `5e-5` - failure to do this in this instance can lead to the model becoming trapped in a local minimum.\n",
    "\n",
    "**Note** This dropout rate is actually rather small, but is best for this particular instance. Generally a dropout rate of between `0.1 - 0.5` is applied, but if you notice that the model fails to learn with a higher rate, then lower rates can be tried. If you like you can see the effecet of a rate of `0.1` in this model. \n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(Dropout(0.001))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# re-train the model\n",
    "tic = time.clock()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2)\n",
    "print('run time: {:5.3f} s'.format(time.clock() - tic))\n",
    "    ```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# re-train the model\n",
    "tic = time.process_time()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2)\n",
    "print('run time: {:5.3f} s'.format(time.process_time() - tic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plot_history(training_history_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The droput has brought the training and validation losses closer to one another. However we also see that there are large oscillations in the validation performance. This is realted to the rather small training and validation set sizes. How can we make sure that we recover the model with the best performance on validation?\n",
    "\n",
    "We can use a **callback** to tell the algorithm to save the model every time there is a new best validation loss. We can then also tell the training algorithm to cease if the validation loss has not improved for _n_ steps; below we set up a callback to monitor the `val_accuracy` and to stop if this has not improved for 50 steps, then restore the weights of the best model. \n",
    "\n",
    "```\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "The callbacks are then specified as a list of all callbacks you defined and passed to the `fit` function _via_ the keyword `callbacks`.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "\n",
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(Dropout(0.001))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# define the callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "    \n",
    "# re-train the model\n",
    "tic = time.clock()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2,\n",
    "                                      callbacks=[callback])\n",
    "print('run time: {:5.3f} s'.format(time.clock() - tic))\n",
    "\n",
    "\n",
    "# plot training history\n",
    "plot_history(training_history_balanced)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# re-define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "# re-compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# define the callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# re-train the model\n",
    "tic = time.clock()\n",
    "# re-train the model\n",
    "training_history_balanced = model.fit(train_x_balanced, train_y_balanced, \n",
    "                                      epochs=500, batch_size=256, validation_split=0.2,\n",
    "                                      callbacks=[callback])\n",
    "print('run time: {:5.3f} s'.format(time.clock() - tic))\n",
    "\n",
    "\n",
    "# plot training history\n",
    "plot_history(training_history_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Build a DNN to detect the presence of all the elements. To do this, you may go through the following steps:\n",
    "\n",
    "1. Find all the elements appearing in the dataset; the answer will be `['Zn', 'Sb', 'Si', 'Fe', 'Ag', 'Cu', 'Bi']`.\n",
    "2. Balance the dataset: if one of the elements appears much less times than the others, it is better to ignore it. Doing everything correctly, you will find the number of samples containing each element as shown in the following table. Therefore, we may ignore Ag in this network.\n",
    "\n",
    "\n",
    "|  Element | # Samples |\n",
    "|---|---|\n",
    "|Zn| 91935|  \n",
    "|Sb| 91880|  \n",
    "|Si| 91672| \n",
    "|Fe| 91446|\n",
    "|Ag| 8617|\n",
    "|Cu| 92133|\n",
    "|Bi| 91796|\n",
    "    \n",
    "3. Do one-hot encoding for the element list `['Zn', 'Sb', 'Si', 'Fe', 'Cu', 'Bi']`; if a sample contains Fe and Sb, e.g., the one-hot vector for this sample will be `[0, 1, 0, 1, 0, 0]`.\n",
    "4. Build and train a DNN (with an input size of 150 and an output size of 6) to detect the presence of the six elements.\n",
    "\n",
    "Do not feel disappointed if the accuracy of the DNN turns out low, as some of the elements are physically indistinguishable from the given spectra. Nevertheless, our DNN can still mine useful information from the dataset. By comparing the predictions and the ground truth, you may notice that **only one element** is dominating the training, that is, the network can correctly predict the presence of this element while the predictions for the other elements are basically random. This indicates that the given set of spectra is characterised by this element (not considering Ag). Figure out what the element is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
