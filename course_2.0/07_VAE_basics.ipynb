{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders: the Basics\n",
    "\n",
    "Variation autoencoders (VAEs) are an extended type of autoencoders (AEs). A VAE can enhance the robustness of content generation by regularising the encodings distribution in the latent space. In this notebook, we will go through the fundamentals of VAEs (motivation, theory and Keras-based implementation) using the `mnist-digits` dataset. We will also learn two useful extensions of VAEs: the disentangled VAEs ($\\beta$-VAEs) and the conditional VAEs. \n",
    "\n",
    "This notebook involves the minimal math to understand the VAEs. Check [06_VAE_theory.ipynb](06_VAE_theory.ipynb) for a more detailed theory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# check version\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# need certainty to explain some of the results\n",
    "import random as python_random\n",
    "python_random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "In this notebook, we will use the `mnist-digits` dataset. It is simpler than the `mnist-fashion` dataset, allowing us to use only two features in the latent space so that we can conveniently visualise and examine the encodings distribution in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalise images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# print info\n",
    "print(\"Number of training data: %d\" % len(train_labels))\n",
    "print(\"Number of test data: %d\" % len(test_labels))\n",
    "print(\"Image pixels: %s\" % str(train_images[0].shape))\n",
    "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot an image in a subplot\n",
    "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n",
    "    plt.subplot(nrows, ncols, iplot + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.xlabel(label, c='k', fontsize=12)\n",
    "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# ramdomly plot some images\n",
    "nrows = 4\n",
    "ncols = 20\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n",
    "    subplot_image(1 - train_images[idata], '', nrows, ncols, iplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders and Regularity of Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we need a VAE? To answer this question, let us start with an ordinary AE and see what is unsatisfactory when we use it to generate new images. \n",
    "\n",
    "## 1. Build and train an autoencoder\n",
    "\n",
    "Based on what we have learnt in [05_autoencoder_basics.ipynb](05_autoencoder_basics.ipynb), we can quickly build an AE with `Dense` layers. First, we need to specify the latent dimension or the size of the bottleneck; for `mnist-digits`, we can use 2.\n",
    "\n",
    "**NOTE**: The code in this notebook supports any latent dimension, but some of the descriptions are only for `latent_dim=2`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent dimension\n",
    "latent_dim = 2\n",
    "\n",
    "# other constants\n",
    "n_digits = 10\n",
    "n_img = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder\n",
    "\n",
    "The encoder contains four layers, an input layer with size 28$\\times$28, two hidden layers with sizes 128 and 16, respectively, and the latent output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder\n",
    "image_input = keras.Input(shape=(n_img, n_img))\n",
    "x = layers.Flatten()(image_input)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "latent_output = layers.Dense(latent_dim)(x)\n",
    "encoder_AE = keras.Model(image_input, latent_output)\n",
    "encoder_AE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder contains four layers that are reciprocal to those of the encoders, taking the latent representation as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the decoder\n",
    "latent_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(latent_input)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(n_img * n_img, activation=\"sigmoid\")(x)\n",
    "image_output = layers.Reshape((n_img, n_img))(x)\n",
    "decoder_AE = keras.Model(latent_input, image_output)\n",
    "decoder_AE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autoencoder\n",
    "\n",
    "Joining up the encoder and the decoder, we obtain the AE network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the AE\n",
    "image_input = keras.Input(shape=(n_img, n_img))\n",
    "latent = encoder_AE(image_input)\n",
    "image_output = decoder_AE(latent)\n",
    "ae_model = keras.Model(image_input, image_output)\n",
    "ae_model.summary()\n",
    "\n",
    "# compile the AE\n",
    "ae_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the autoencoder\n",
    "\n",
    "Now we can train our AE with the `mnist-digits` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the AE\n",
    "ae_model.fit(train_images, train_images, epochs=50, batch_size=128, \n",
    "             validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect how the images are distributed in the latent space. \n",
    "\n",
    "### Encode images\n",
    "\n",
    "First, we encode the images by our AE. After that, each image becomes a 2D point (when `latent_dim=2`) in the latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode images by AE\n",
    "train_encodings_AE = encoder_AE.predict(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "We can plot the 2D points and colour them by their true labels. \n",
    "\n",
    "**NOTE**: If `latent_dim>2`, use `feature_x` and `feature_y` to specify the two latent dimensions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of encodings in the latent space\n",
    "def scatter_plot_encodings_latent(encodings, labels, feature_x=0, feature_y=1):\n",
    "    plt.figure(dpi=100)\n",
    "    scat = plt.scatter(encodings[:, feature_x], encodings[:, feature_y], c=labels, s=.5, cmap='Paired')\n",
    "    plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n",
    "                         title='Image labels', bbox_to_anchor=(1.5, 1.)))\n",
    "    plt.xlabel('Feature %d' % feature_x)\n",
    "    plt.ylabel('Feature %d' % feature_y)\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.show()\n",
    "    \n",
    "# scatter plot of encodings by AE\n",
    "scatter_plot_encodings_latent(train_encodings_AE, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Histogram plot\n",
    "\n",
    "Also, for each digit and each feature, we can plot the density histogram of encodings -- note that we are using the same feature range (range of $x$-axis) in all the histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# histogram plot of encodings in the latent space\n",
    "def hist_plot_encodings_latent(encodings, labels, digit, dim, ax, feature_range, color):\n",
    "    # extract\n",
    "    encodings_digit = encodings[labels == digit, dim]\n",
    "    # histogram\n",
    "    ax.hist(encodings_digit, bins=60, density=True, color=color, alpha=1)\n",
    "    # mean and std dev\n",
    "    mean = np.mean(encodings_digit)\n",
    "    std = np.std(encodings_digit)\n",
    "    ax.axvline(mean, c='k', ls='--')\n",
    "    ax.set_xlabel('Feature %d, Digit %d\\n${\\cal N}(\\mu=%.1f, \\sigma^2=%.1f)$' % \n",
    "                  (dim, digit, mean, std * std), c='k')\n",
    "    # feature range\n",
    "    ax.set_xlim(feature_range)\n",
    "    \n",
    "# histogram plot of encodings by AE\n",
    "feature_range = (-np.max(np.abs(train_encodings_AE)), np.max(np.abs(train_encodings_AE)))\n",
    "for dim in range(latent_dim):\n",
    "    print('Latent feature %d:' % dim)\n",
    "    fig, axes = plt.subplots(2, 5, dpi=100, figsize=(15, 4), sharex=True)\n",
    "    plt.subplots_adjust(hspace=.4)\n",
    "    for digit in range(10):\n",
    "        hist_plot_encodings_latent(train_encodings_AE, train_labels, digit, dim, \n",
    "                                   axes[digit // 5, digit % 5], feature_range, 'C%d' % dim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularity of the latent space\n",
    "\n",
    "Both the scatter plot and the histogram plots show that the data distributions in the latent space are rather *irregular*. Some of the digits have very wide distributions (such as 1 and 7) and some very narrow distributions (such as 2 and 3). \n",
    "\n",
    "Remember that our goal of training this AE is neither dimensionality reduction nor denoising but to generate new images out of the original dataset. Image generation is done by the decoder, taking the latent representation as the input. An irregular latent space makes image generation less controllable and robust. Taking our case for example, two shortcomings are likely to emerge:\n",
    "\n",
    "1. **Controllability**: sampling the entire latent space, we will generate much more of the widely distributed digits than the narrowly distributed ones; instead, if we narrow down the latent space for sampling, we may loss some of the widely distributed ones;\n",
    "\n",
    "2. **Robustness**: images that do not resemble any of the digits will be generated by sampling the gaps between the distributions of the digits; such gaps increase with the sampled range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new images\n",
    "\n",
    "The following function generates new images by randomly sampling the latent space within a specified feature range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images from the latent space\n",
    "def generate_images_latent(decoder, n_generation, feature_range):\n",
    "    # randomly sample the latent space\n",
    "    latent = []\n",
    "    for dim in range(latent_dim):\n",
    "        if len(np.array(feature_range).shape) == 1:\n",
    "            # only one range provided; used it for all dimensions\n",
    "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n",
    "                                            n_generation * n_generation))\n",
    "        else:\n",
    "            # range provided for each dimension\n",
    "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n",
    "                                            n_generation * n_generation))\n",
    "    latent = np.array(latent).T\n",
    "    \n",
    "    # decode images\n",
    "    decodings = decoder.predict(latent)\n",
    "    \n",
    "    # create and fill a large figure\n",
    "    figure = np.zeros((n_img * n_generation, n_img * n_generation))\n",
    "    for iy in np.arange(n_generation):\n",
    "        for ix in np.arange(n_generation):\n",
    "            figure[iy * n_img : (iy + 1) * n_img, ix * n_img : (ix + 1) * n_img] = decodings[iy * n_generation + ix]\n",
    "            \n",
    "    # plot figure\n",
    "    plt.figure(dpi=100, figsize=(n_generation / 3, n_generation / 3))\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the generated images look like. Here we choose a range of `[-25, 25]` for both the two features (when `latent_dim=2`), which encompasses all the digits and most of the data points. The two shortcomings can be observed:\n",
    "\n",
    "1. Only a very few instances of the narrowly distributed digits are generated, such as 2 and 3;\n",
    "2. Many images do not resemble any of the digits; note that the severely rotated digits should be recognised as non-digits in this context.\n",
    "\n",
    "Feel free to try some other ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images by AE\n",
    "generate_images_latent(decoder_AE, n_generation=40, feature_range=[-25, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Overfitting is the essential reason behind an irregular latent space of a naive AE, that is, the neural networks for encoding and decoding try their best to fit the data from end to end without caring about how the latent space is organised with respect to the original data. A VAE can regularise the latent space by imposing additional distributional properties on the latent space.\n",
    "\n",
    "The following figure summarises **the two extensions** from an AE to a VAE:\n",
    "\n",
    "1. Unlike a naive AE that encodes an input data $x$ as a single point $z$ in the latent space, a VAE encodes it as a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, and the latent feature $z$ is sampled from this distribution and then passed to the decoder;\n",
    "\n",
    "2. An AE only minimises the reconstruction error $\\lVert x-x'\\rVert^2$ to fit the data, whereas a VAE minimises the sum of the reconstruction error and the KL divergence (Kullbackâ€“Leibler divergence) between the latent distribution $\\mathcal{N}(\\mu, \\sigma^2)$ and the standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "How does a VAE regularise the latent space? The loss function provides a straightforward answer: in addition to fitting the data by minimising the reconstruction error, it also drags the latent distribution to a standard normal distribution. The final model is a trade-off between the two effects. Also, because each input image is encoded as a Gaussian blob instead of a single point, the gaps in the latent space can be filled by such blurring so that meaningless decodings can be reduced.\n",
    "\n",
    "![ae-vae.png](https://i.ibb.co/mJVqkNy/aevae.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build and train a VAE\n",
    "\n",
    "Now we will implement a VAE for `mnist-digits`. The rigorous theory is more complicated than explained above, which can be found in [06_VAE_theory.ipynb](06_VAE_theory.ipynb). \n",
    "\n",
    "\n",
    "### The encoder\n",
    "\n",
    "To implement the probabilistic encoder, we first need a custom function to sample the latent distribution, as implemented by the `Sampling` class. Note that here we are using $\\ln\\sigma$ instead of $\\sigma$ in the network; otherwise, the implementation will be complicated as we have to impose positiveness on $\\sigma$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling z with (z_mean, z_log_var)\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "# build the encoder\n",
    "image_input = keras.Input(shape=(n_img, n_img))\n",
    "x = layers.Flatten()(image_input)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z_output = Sampling()([z_mean, z_log_var])\n",
    "encoder_VAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "encoder_VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The docoder is the same as that of AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the decoder\n",
    "z_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(z_input)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(n_img * n_img, activation=\"sigmoid\")(x)\n",
    "image_output = layers.Reshape((n_img, n_img))(x)\n",
    "decoder_VAE = keras.Model(z_input, image_output)\n",
    "decoder_VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The VAE\n",
    "\n",
    "To add the KL divergence to the loss, we create a class `VAE` derived from `keras.Model` and overwrite its `train_step()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE class\n",
    "class VAE(keras.Model):\n",
    "    # constructor\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    # customise train_step() to implement the loss \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z)\n",
    "            # reconstruction error by binary crossentropy loss\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime)) * n_img * n_img\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # loss = reconstruction error + KL divergence\n",
    "            loss = reconstruction_loss + kl_loss\n",
    "        # apply gradient\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # return loss for metrics log\n",
    "        return {\"loss\": loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"kl_loss\": kl_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build, compile and train our VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the VAE\n",
    "vae_model = VAE(encoder_VAE, decoder_VAE)\n",
    "\n",
    "# compile the VAE\n",
    "vae_model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# train the VAE\n",
    "training_history_VAE = vae_model.fit(train_images, train_images, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both `reconstruction_loss` and `kl_loss` are logged, so we can plot them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.figure(dpi=100, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history_VAE.history['loss'], label='Loss')\n",
    "plt.plot(training_history_VAE.history['reconstruction_loss'], label='Reconstruction loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history_VAE.history['kl_loss'], label='KL loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the latent space\n",
    "\n",
    "Next, we can inspect the latent space following the same steps we did for the AE. Clearly, the latent distributions become much more regular than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode images by VAE\n",
    "train_encodings_VAE = encoder_VAE.predict(train_images)[2]\n",
    "\n",
    "# scatter plot of encodings by VAE\n",
    "scatter_plot_encodings_latent(train_encodings_VAE, train_labels)\n",
    "\n",
    "# histogram plot of encodings by VAE\n",
    "feature_range = (-np.max(np.abs(train_encodings_VAE)), np.max(np.abs(train_encodings_VAE)))\n",
    "for dim in range(latent_dim):\n",
    "    print('Latent feature %d:' % dim)\n",
    "    fig, axes = plt.subplots(2, 5, dpi=100, figsize=(15, 4), sharex=True)\n",
    "    plt.subplots_adjust(hspace=.4)\n",
    "    for digit in range(10):\n",
    "        hist_plot_encodings_latent(train_encodings_VAE, train_labels, digit, dim, \n",
    "                                   axes[digit // 5, digit % 5], feature_range, 'C%d' % dim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new images\n",
    "\n",
    "Finally, we can generate new images with our VAE. The result shows that, compared to the AE, the numbers of the generated digits have become more in unison and the number of non-digit images has been decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate images by AE\n",
    "generate_images_latent(decoder_VAE, n_generation=40, feature_range=[-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Variational Autoencoders\n",
    "\n",
    "Disentangled VAEs or $\\beta$-VAEs are an extended type of VAEs. As explained before, the loss of a VAE consists of two parts, the reconstruction error and the KL divergence: the former tries to fit the network with data and the later to regularise the latent space. It is thus natural to introduce a hyperparameter to weigh these two forces, which gives rise to a disentangled VAE. As you can see in the following cartoon, a factor $\\beta$ is applied to the KL divergence, and this $\\beta$ is usually greater than 1. In practice, the value of $\\beta$ has to be tuned for the best quality of content generation.\n",
    "\n",
    "So, why we call it \"disentangled\"? Consider the limit $\\beta\\rightarrow\\infty$: in this case, the distributions of all the latent features will simply converge to $\\mathcal{N}(0,1)$, regardless of the data, so they are completely disentangled. In other words, the latent features become increasingly disentangled as $\\beta$ increases. \n",
    "\n",
    "\n",
    "![bvae.png](https://i.ibb.co/mSmVrHy/bvaee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a $\\beta$-VAE\n",
    "\n",
    "It is straightforward to generalise our VAE implementation: just adding $\\beta$ as a hyperparameter to the constructor of the `VAE` class and apply it to the KL divergence in the `train_step()` method. The differences are highlighted in the code.\n",
    "\n",
    "\n",
    "### The encoder and decoder\n",
    "\n",
    "They are exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder\n",
    "image_input = keras.Input(shape=(n_img, n_img))\n",
    "x = layers.Flatten()(image_input)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z_output = Sampling()([z_mean, z_log_var])\n",
    "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "\n",
    "# build the decoder\n",
    "z_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(z_input)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(n_img * n_img, activation=\"sigmoid\")(x)\n",
    "image_output = layers.Reshape((n_img, n_img))(x)\n",
    "decoder_BVAE = keras.Model(z_input, image_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\beta$-VAE\n",
    "\n",
    "We will use $\\beta=10$ for demonstration. Try some other values (particularly a very large value such as 1000) and see what happens. Note that we are logging the KL loss scaled by $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BVAE class\n",
    "class BVAE(keras.Model):\n",
    "    # constructor\n",
    "    ########################################################\n",
    "    ######## NEW: passing beta as an extra argument ########\n",
    "    ########################################################\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(BVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "\n",
    "    # customise train_step() to implement the loss \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z)\n",
    "            # reconstruction error by binary crossentropy loss\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime)) * n_img * n_img\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # loss = reconstruction error + KL divergence\n",
    "            #######################################\n",
    "            ######## NEW: scale KL by beta ########\n",
    "            #######################################\n",
    "            loss = reconstruction_loss + self.beta * kl_loss\n",
    "        # apply gradient\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        #######################################\n",
    "        ######### NEW: log scaled KL ##########\n",
    "        #######################################\n",
    "        # return loss for metrics log\n",
    "        return {\"loss\": loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"beta_kl_loss\": self.beta * kl_loss}\n",
    "\n",
    "# build the BVAE\n",
    "########################################\n",
    "######## NEW: pass beta to BVAE ########\n",
    "########################################\n",
    "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=10.)\n",
    "\n",
    "# compile the BVAE\n",
    "bvae_model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# train the BVAE\n",
    "training_history_BAVE = bvae_model.fit(train_images, train_images, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.figure(dpi=100, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history_BAVE.history['loss'], label='Loss')\n",
    "plt.plot(training_history_BAVE.history['reconstruction_loss'], label='Reconstruction loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history_BAVE.history['beta_kl_loss'], label='KL loss scaled by beta')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate the results\n",
    "\n",
    "The same as before, we can evaluate the latent distributions and generate new images using our $\\beta$-VAE. As expected, the distributions become more close to normal distributions because $\\beta=10$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode images by BVAE\n",
    "train_encodings_BVAE = encoder_BVAE.predict(train_images)[2]\n",
    "\n",
    "# scatter plot of encodings by BVAE\n",
    "scatter_plot_encodings_latent(train_encodings_BVAE, train_labels)\n",
    "\n",
    "# histogram plot of encodings by BVAE\n",
    "feature_range = (-np.max(np.abs(train_encodings_BVAE)), np.max(np.abs(train_encodings_BVAE)))\n",
    "for dim in range(latent_dim):\n",
    "    print('Latent feature %d:' % dim)\n",
    "    fig, axes = plt.subplots(2, 5, dpi=100, figsize=(15, 4), sharex=True)\n",
    "    plt.subplots_adjust(hspace=.4)\n",
    "    for digit in range(10):\n",
    "        hist_plot_encodings_latent(train_encodings_BVAE, train_labels, digit, dim, \n",
    "                                   axes[digit // 5, digit % 5], feature_range, 'C%d' % dim)\n",
    "    plt.show()\n",
    "\n",
    "# generate images by BVAE\n",
    "generate_images_latent(decoder_BVAE, n_generation=40, feature_range=[-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoders\n",
    "\n",
    "In many applications, we hope to generate contents based on the labels. For example, we hope to generate images for a given digit from `mnist-digits`. Conditional VAEs serve this purpose. In a conditional VAE, the labels are sent to both the encoder and the decoder as an extra input, as shown in the following figure. \n",
    "\n",
    "<img src=\"https://i.ibb.co/0Mj3Q8k/cvaec.png\" width=100% height=100% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a conditional VAE\n",
    "\n",
    "The simplest way to implement a conditional VAE is to concatenate the labels to both the input data and the latent features. We will use this approach for our implementation. The one-hot encodings of the labels will be concatenated, which is a vector of size 10 (the condition dimension). \n",
    "\n",
    "### The encoder\n",
    "\n",
    "The encoder is the same as that of the VAE except that\n",
    "\n",
    "1. the input size is increased by the condition dimension; \n",
    "2. we no longer flatten the images inside the network because of the concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of condition\n",
    "condition_dim = n_digits\n",
    "\n",
    "# encoder\n",
    "###############################################################\n",
    "######## NEW: input size is increased by condition_dim ########\n",
    "###############################################################\n",
    "image_input = keras.Input(shape=(n_img * n_img + condition_dim))\n",
    "x = layers.Dense(128, activation='relu')(image_input)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z_output = Sampling()([z_mean, z_log_var])\n",
    "encoder_CVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "encoder_CVAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The same as the encoder, the input size of the decoder is increased by the condition dimension, and the reconstructed images will be flattened externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "###############################################################\n",
    "######## NEW: input size is increased by condition_dim ########\n",
    "###############################################################\n",
    "z_input = keras.Input(shape=(latent_dim + condition_dim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(z_input)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "image_output = layers.Dense(n_img * n_img, activation=\"sigmoid\")(x)\n",
    "decoder_CVAE = keras.Model(z_input, image_output)\n",
    "decoder_CVAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conditional VAE\n",
    "\n",
    "The conditional VAE is the same as the VAE except that\n",
    "\n",
    "1. the condition dimension is passed to the constructor;\n",
    "2. the labels are concatenated to the latent features before they are decoded;\n",
    "3. the labels are truncated from the input data to compute the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE class\n",
    "class CVAE(keras.Model):\n",
    "    # constructor\n",
    "    #################################################################\n",
    "    ######## NEW: passing condition_dim as an extra argument ########\n",
    "    #################################################################\n",
    "    def __init__(self, encoder, decoder, condition_dim, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.condition_dim = condition_dim\n",
    "\n",
    "    # customise train_step() to implement the loss \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            ####################################################################\n",
    "            ######## NEW: apply conditions to encodings before decoding ########\n",
    "            ####################################################################\n",
    "            z_cond = tf.concat([z, x[:, -self.condition_dim:]], axis=1)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z_cond)\n",
    "            ###################################################################\n",
    "            ######## NEW: truncate conditions for reconstruction error ########\n",
    "            ###################################################################\n",
    "            # reconstruction error by binary crossentropy loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(x[:, :-self.condition_dim], x_prime)) * n_img * n_img\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # loss = reconstruction error + KL divergence\n",
    "            loss = reconstruction_loss + kl_loss\n",
    "        # apply gradient\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # return loss for metrics log\n",
    "        return {\"loss\": loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"kl_loss\": kl_loss}\n",
    "\n",
    "# build the CVAE\n",
    "cvae_model = CVAE(encoder_CVAE, decoder_CVAE, condition_dim)\n",
    "\n",
    "# compile the CVAE\n",
    "cvae_model.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we need to concatenate the images and their labels to generate the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one-hot encoding labels\n",
    "train_labels_onehot = np.eye(10)[train_labels]\n",
    "\n",
    "# flatten images\n",
    "train_images_flattened = train_images.reshape((len(train_images), n_img * n_img))\n",
    "\n",
    "# concatenate labels to images\n",
    "train_images_conditioned = np.concatenate((train_images_flattened, train_labels_onehot), axis=1)\n",
    "\n",
    "# train the VAE\n",
    "cvae_model.fit(train_images_conditioned, train_images_flattened, \n",
    "               epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate the results\n",
    "\n",
    "The latent distributions can be assessed in the same way. Note that *each digit now has its own latent distributions because of the conditioning* (all resemble a $\\mathcal{N}(0,1)$), so the latent space is highly regular and disentangled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode images by CVAE\n",
    "train_encodings_CVAE = encoder_CVAE.predict(train_images_conditioned)[2]\n",
    "\n",
    "# scatter plot of encodings by CVAE\n",
    "scatter_plot_encodings_latent(train_encodings_CVAE, train_labels)\n",
    "\n",
    "# histogram plot of encodings by BVAE\n",
    "feature_range = (-np.max(np.abs(train_encodings_CVAE)), np.max(np.abs(train_encodings_CVAE)))\n",
    "for dim in range(latent_dim):\n",
    "    print('Latent feature %d:' % dim)\n",
    "    fig, axes = plt.subplots(2, 5, dpi=100, figsize=(15, 4), sharex=True)\n",
    "    plt.subplots_adjust(hspace=.4)\n",
    "    for digit in range(10):\n",
    "        hist_plot_encodings_latent(train_encodings_CVAE, train_labels, digit, dim, \n",
    "                                   axes[digit // 5, digit % 5], feature_range, 'C%d' % dim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate new images for any given digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate images from the conditioned latent space\n",
    "def generate_images_conditioned_latent(decoder, n_generation, feature_range, condition_digit, ax):\n",
    "    # randomly sample the latent space\n",
    "    latent = []\n",
    "    for dim in range(latent_dim):\n",
    "        if len(np.array(feature_range).shape) == 1:\n",
    "            # only one range provided; used it for all dimensions\n",
    "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n",
    "                                            n_generation * n_generation))\n",
    "        else:\n",
    "            # range provided for each dimension\n",
    "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n",
    "                                            n_generation * n_generation))\n",
    "    latent = np.array(latent).T\n",
    "    \n",
    "    # condition the latent space\n",
    "    condiont_one_hot = np.eye(10)[condition_digit]\n",
    "    latent = np.concatenate((latent, np.repeat([condiont_one_hot], len(latent), axis=0)), axis=1)\n",
    "    \n",
    "    # decode images\n",
    "    decodings = decoder.predict(latent).reshape((len(latent), n_img, n_img))\n",
    "    \n",
    "    # create and fill a large figure\n",
    "    figure = np.zeros((n_img * n_generation, n_img * n_generation))\n",
    "    for iy in np.arange(n_generation):\n",
    "        for ix in np.arange(n_generation):\n",
    "            figure[iy * n_img : (iy + 1) * n_img, ix * n_img : (ix + 1) * n_img] = decodings[iy * n_generation + ix]\n",
    "            \n",
    "    # plot figure\n",
    "    ax.imshow(figure, cmap=\"Greys_r\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "# generate images by CVAE\n",
    "fig, axes = plt.subplots(5, 2, dpi=100, figsize=(20, 50), )\n",
    "for digit in range(10):\n",
    "    generate_images_conditioned_latent(decoder_CVAE, n_generation=20, feature_range=[-1, 1], \n",
    "                                       condition_digit=digit, ax=axes[digit // 2, digit % 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Repeat the above processes with a larger `latent_dim`; you will find that, compare to `latent_dim=2`,\n",
    "    * the improvement from AE to VAE and that from VAE to $\\beta$-VAE both become more significant;\n",
    "    * the variability and the resolution of the generated images can be improved (with more epochs for training). \n",
    "    \n",
    "\n",
    "2. Use a VAE, $\\beta$-VAE or conditional VAE to generate images from the `mnist-fashion` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
