{"cells": [{"cell_type": "markdown", "metadata": {"id": "-LuENyXopiFY"}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-bR-cwWxpliE", "outputId": "280eb56e-fedb-43d4-843f-d2a1bb8d0593"}, "outputs": [], "source": ["import gym\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# Configuration parameters for the whole setup\n", "seed = 42\n", "gamma = 0.99  # Discount factor for past rewards\n", "max_steps_per_episode = 10000\n", "env = gym.make(\"CartPole-v1\")  # Create the environment\n", "env.seed(seed)\n", "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"]}, {"cell_type": "markdown", "metadata": {"id": "9ZQBqGdaNchR"}, "source": ["## Environment"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "UsxybIIYKmqQ"}, "outputs": [], "source": ["# Run a demo of the environment\n", "observation = env.reset()\n", "cum_reward = 0\n", "cart_frames = []\n", "for t in range(5000):\n", "    # Render into buffer. \n", "    cart_frames.append(env.render(mode='rgb_array'))\n", "    action = env.action_space.sample()\n", "    observation, reward, done, _ = env.step(action)\n", "    if done:\n", "        break"]}, {"cell_type": "markdown", "metadata": {"id": "dQG2c3uTNkUh"}, "source": ["## Display"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "faKfGYCVKnk8"}, "outputs": [], "source": ["from matplotlib import animation, rc\n", "rc('animation', html='jshtml')\n", "fig, ax = plt.subplots(figsize = (5, 3))\n", "im = ax.imshow(cart_frames[0])\n", "def updatefig(frame):\n", "    im.set_array(cart_frames[frame])\n", "    return im,\n", "anim = animation.FuncAnimation(fig, updatefig, \n", "                             frames = range(1, len(cart_frames)), interval=120, blit=True)\n", "anim"]}, {"cell_type": "markdown", "metadata": {"id": "RTfXhyGnNmUa"}, "source": ["## Network"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5nnzWm42prZv"}, "outputs": [], "source": ["num_inputs = 4\n", "num_actions = 2\n", "num_hidden = 128\n", "\n", "inputs = layers.Input(shape=(num_inputs,))\n", "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n", "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n", "critic = layers.Dense(1)(common)\n", "\n", "model = keras.Model(inputs=inputs, outputs=[action, critic])"]}, {"cell_type": "markdown", "metadata": {"id": "TanTxgiaNo9b"}, "source": ["## Training"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "sgClKQ4IpsxW", "outputId": "3e2c8167-0ead-41bb-cfa8-a090bb24db85"}, "outputs": [], "source": ["optimizer = keras.optimizers.Adam(learning_rate=0.01)\n", "huber_loss = keras.losses.Huber()\n", "action_probs_history = []\n", "critic_value_history = []\n", "rewards_history = []\n", "running_reward = 0\n", "episode_count = 0\n", "\n", "while True:  # Run until solved\n", "    state = env.reset()\n", "    episode_reward = 0\n", "    with tf.GradientTape() as tape:\n", "        for timestep in range(1, max_steps_per_episode):\n", "            # env.render(); Adding this line would show the attempts\n", "            # of the agent in a pop up window.\n", "\n", "            state = tf.convert_to_tensor(state)\n", "            state = tf.expand_dims(state, 0)\n", "\n", "            # Predict action probabilities and estimated future rewards\n", "            # from environment state\n", "            action_probs, critic_value = model(state)\n", "            critic_value_history.append(critic_value[0, 0])\n", "\n", "            # Sample action from action probability distribution\n", "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n", "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n", "\n", "            # Apply the sampled action in our environment\n", "            state, reward, done, _ = env.step(action)\n", "            rewards_history.append(reward)\n", "            episode_reward += reward\n", "\n", "            if done:\n", "                break\n", "\n", "        # Update running reward to check condition for solving\n", "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n", "\n", "        # Calculate expected value from rewards\n", "        # - At each timestep what was the total reward received after that timestep\n", "        # - Rewards in the past are discounted by multiplying them with gamma\n", "        # - These are the labels for our critic\n", "        returns = []\n", "        discounted_sum = 0\n", "        for r in rewards_history[::-1]:\n", "            discounted_sum = r + gamma * discounted_sum\n", "            returns.insert(0, discounted_sum)\n", "\n", "        # Normalize\n", "        returns = np.array(returns)\n", "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n", "        returns = returns.tolist()\n", "\n", "        # Calculating loss values to update our network\n", "        history = zip(action_probs_history, critic_value_history, returns)\n", "        actor_losses = []\n", "        critic_losses = []\n", "        for log_prob, value, ret in history:\n", "            # At this point in history, the critic estimated that we would get a\n", "            # total reward = `value` in the future. We took an action with log probability\n", "            # of `log_prob` and ended up recieving a total reward = `ret`.\n", "            # The actor must be updated so that it predicts an action that leads to\n", "            # high rewards (compared to critic's estimate) with high probability.\n", "            diff = ret - value\n", "            actor_losses.append(-log_prob * diff)  # actor loss\n", "\n", "            # The critic must be updated so that it predicts a better estimate of\n", "            # the future rewards.\n", "            critic_losses.append(\n", "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n", "            )\n", "\n", "        # Backpropagation\n", "        loss_value = sum(actor_losses) + sum(critic_losses)\n", "        grads = tape.gradient(loss_value, model.trainable_variables)\n", "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n", "\n", "        # Clear the loss and reward history\n", "        action_probs_history.clear()\n", "        critic_value_history.clear()\n", "        rewards_history.clear()\n", "\n", "    # Log details\n", "    episode_count += 1\n", "    if episode_count % 10 == 0:\n", "        template = \"running reward: {:.2f} at episode {}\"\n", "        print(template.format(running_reward, episode_count))\n", "\n", "    if running_reward > 195:  # Condition to consider the task solved\n", "        template = \"Solved at episode {} with reward {:.2f}!\"\n", "        print(template.format(episode_count, running_reward))\n", "        break"]}, {"cell_type": "markdown", "metadata": {"id": "kMj3Tqp3Nsol"}, "source": ["## Inference"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "jY1axn8ZJ7HD", "outputId": "514c04a4-733d-456c-cfbf-9ec0cdb48d87"}, "outputs": [], "source": ["cart_frames2 = []\n", "episode_count = 0\n", "while True:  # Run until solved\n", "    state = env.reset()\n", "    episode_reward = 0\n", "    with tf.GradientTape() as tape:\n", "        for timestep in range(1, max_steps_per_episode):\n", "            # env.render(); Adding this line would show the attempts\n", "            # of the agent in a pop up window.\n", "            state = tf.convert_to_tensor(state)\n", "            state = tf.expand_dims(state, 0)\n", "\n", "            # Predict action probabilities and estimated future rewards\n", "            # from environment state\n", "            action_probs, critic_value = model(state)\n", "\n", "            # Sample action from action probability distribution\n", "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n", "\n", "            # Apply the sampled action in our environment\n", "            cart_frames2.append(env.render(mode='rgb_array'))\n", "            state, reward, done, _ = env.step(action)\n", "            episode_reward += reward\n", "\n", "            if done:\n", "                break\n", "\n", "        # Update running reward to check condition for solving\n", "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n", "\n", "\n", "    if running_reward > 190:  # Condition to consider the task solved\n", "        template = \"Episode {} with reward {:.2f}!\"\n", "        print(template.format(episode_count, running_reward))\n", "        break"]}, {"cell_type": "markdown", "metadata": {"id": "7LsvoBSpNvO-"}, "source": ["## Display"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 677}, "id": "IsaSsg-eKX5g", "outputId": "8d70d210-607f-4c9a-e1ec-7f479082e0a2"}, "outputs": [], "source": ["from matplotlib import animation, rc\n", "rc('animation', html='jshtml')\n", "fig, ax = plt.subplots(figsize = (5, 3))\n", "im = ax.imshow(cart_frames2[0])\n", "def updatefig(frame):\n", "    im.set_array(cart_frames2[frame])\n", "    return im,\n", "anim = animation.FuncAnimation(fig, updatefig, \n", "                             frames = range(1, len(cart_frames2)), interval=120, blit=True)\n", "anim"]}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 1}