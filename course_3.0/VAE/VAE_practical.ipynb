{"cells": [{"cell_type": "markdown", "metadata": {"id": "XBJKoD0jw8ED"}, "source": ["# VAE Practical: Inelastic Neutron Scattering\n", "\n", "In [CNN_practical.ipynb](../CNN/CNN_practical.ipynb), we have used the inelastic neutron scattering (INS) dataset to practice classification with a convolutional neural network (CNN). In this notebook, we try to make a disentangled variational autoencoder ($\\beta$-VAE) to generate new images from the INS dataset, using CNNs for encoding and decoding. \n", "\n", "Compared to a simple VAE, a $\\beta$-VAE only introduce one hyperparameter $\\beta$ to the loss function to balance the effects of the reconstruction loss and the variational loss. A simple VAE is the special case with $\\beta=1$. Learn details of $\\beta$-VAE in [VAE_advanced.ipynb](VAE_advanced.ipynb)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "u66arUoew8EI", "outputId": "8dac0caf-ad05-4660-dee6-a55856aabbf3"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import h5py\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {"id": "fUYOVWdCw8EK"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5TdJ_pxPw8EK"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "rN7w2egzw8EK", "outputId": "e04e1fc1-ccba-4346-b016-6c6fb08377ef"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "Xh5Iigrnw8EL"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "mN9KpPs6w8EL"}, "source": ["# The dataset\n", "\n", "This section is the same as in [CNN_practical.ipynb](../CNN/CNN_practical.ipynb)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "OzGm5f-qw8EL", "outputId": "9e76b23e-87e1-426b-c123-447cac280f7d"}, "outputs": [], "source": ["# define image size\n", "IMG_HEIGHT = 20\n", "IMG_WIDTH = 200\n", "N_CHANNELS = 1\n", "N_CLASSES = 2\n", "\n", "# generator\n", "def hdf5_generator(path, buffer_size=32):\n", "    \"\"\" Load data INS data from disk\n", "    \n", "    Args:\n", "        path: path of the HDF5 file on disk\n", "        buffer_size: number of images to read from disk\n", "    \"\"\"\n", "    with h5py.File(path, 'r') as handle:\n", "        n_samples, h, w, c = handle['images'].shape\n", "        for i in range(0, n_samples, buffer_size):\n", "            images = handle['images'][i:i+buffer_size, ..., :1]\n", "            labels = handle['labels'][i:i+buffer_size]\n", "            yield images, labels\n", "\n", "# training data\n", "train_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=data_path + 'ins-data/train.h5'), \n", "                                               output_types=(tf.float32, tf.float32),\n", "                                               output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n", "                                                              (None, N_CLASSES,)))\n", "\n", "# test data\n", "test_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=data_path + 'ins-data/test.h5'), \n", "                                              output_types=(tf.float32, tf.float32),\n", "                                              output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n", "                                                             (None, N_CLASSES,)))\n", "# print\n", "print(train_dataset)\n", "print(test_dataset)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 498}, "id": "DM4X2L2xw8EN", "outputId": "bfcf3461-79ec-49cd-96e2-189d3e81c073"}, "outputs": [], "source": ["# load the first buffer (with 32 data by default)\n", "images, labels = list(test_dataset.take(1))[0]\n", "\n", "# plot some images and labels from it\n", "nplot = 10\n", "fig, axes = plt.subplots(nplot // 2, 2, figsize=(16, nplot / 1.5), dpi=100)\n", "for ax, image, label in zip(axes.flatten(), images, labels):\n", "    ax.matshow(np.squeeze(image))\n", "    ax.set_xlabel('0: Dimer' if label[0] < .5 else '1: Goodenough', c='k')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])"]}, {"cell_type": "markdown", "metadata": {"id": "Zm4ClTDyw8EN"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "G2zGtQBnw8EN"}, "source": ["# $\\beta$-VAE for  Image Generation\n", "\n", "This $\\beta$-VAE can be a combination of the CNN architecture in [CNN_practical.ipynb](../CNN/CNN_practical.ipynb) and the $\\beta$-VAE implementation in [VAE_advanced.ipynb](VAE_advanced.ipynb).\n", "\n", "\n", "## 1. Encoder and decoder\n", "\n", "First, we need to specify the latent dimension:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "bhaAlf94w8EN"}, "outputs": [], "source": ["# latent dimension\n", "latent_dim = 32"]}, {"cell_type": "markdown", "metadata": {"id": "0Ngdh9iaw8EO"}, "source": ["Now, extend the CNN in [CNN_practical.ipynb](../CNN/CNN_practical.ipynb) to an encoder and a decoder. \n", "\n", "**Suggested Answer for Encoder** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# sampling z with (z_mean, z_log_var)\n", "class Sampling(keras.layers.Layer):\n", "    def call(self, inputs):\n", "        z_mean, z_log_var = inputs\n", "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n", "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n", "\n", "# build the encoder\n", "image_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS))\n", "x = layers.Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same')(image_input)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Flatten()(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "encoder_BVAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ItQZedxIw8EO", "outputId": "e8e23acc-1f30-4a75-b867-ef81795ecf14"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Suggested Answer for Decoder** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the decoder\n", "z_input = layers.Input(shape=(latent_dim,))\n", "x = layers.Dense(800, activation=\"relu\")(z_input)\n", "x = layers.Reshape((2, 25, 16))(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.ZeroPadding2D((1, 0))(x)\n", "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.Conv2DTranspose(8, kernel_size=(5, 5), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "image_output = layers.Conv2DTranspose(1, kernel_size=(3, 3), activation='linear', padding='same')(x)\n", "decoder_BVAE = keras.Model(z_input, image_output)\n", "decoder_BVAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "s47xYjHFw8EP", "outputId": "13e2110b-9a52-42fe-f009-d6de111e33ff"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "E4dfGc60w8EP"}, "source": ["## 2. The `BVAE` class\n", "\n", "The `BVAE` class can be the same as implemented in [VAE_basics.ipynb](VAE_basics.ipynb) except that we need to pass and use $\\beta$. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# BVAE class\n", "class BVAE(keras.Model):\n", "    # constructor\n", "    ########################################################\n", "    ######## NEW: passing beta as an extra argument ########\n", "    ########################################################\n", "    def __init__(self, encoder, decoder, beta, **kwargs):\n", "        super(BVAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.beta = beta\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            # decoding\n", "            x_prime = self.decoder(z)\n", "            # reconstruction error by binary crossentropy loss\n", "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime))\n", "            reconstruction_loss *= IMG_HEIGHT * IMG_WIDTH\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            #######################################\n", "            ######## NEW: scale KL by beta ########\n", "            #######################################\n", "            loss = reconstruction_loss + self.beta * kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"beta_kl_loss\": self.beta * kl_loss}\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "V_l6qvnUw8EQ"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "GMdO2RQYw8EQ"}, "source": ["## 3. Build and train the `BVAE` model\n", "\n", "Now, build the `BVAE` model and train it with the INS dataset. Let us first use $\\beta=5$ and start with 50 epochs. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the BVAE\n", "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=5.)\n", "\n", "# compile the BVAE\n", "bvae_model.compile(optimizer='adam')\n", "    \n", "# train the BVAE\n", "training_history_BAVE = bvae_model.fit(train_dataset, epochs=50, batch_size=32)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "DvGPnrtEw8ER", "outputId": "5a0ec50a-3cf6-4138-97db-ceb4e74dcafc"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "JdDaOg9uw8ER"}, "source": ["## 4. Generate images\n", "\n", "Finally, we can generate new images using the decoder. After 50 epochs, the generated images resemble the original ones but look pretty vague. We can increase the definition by using more convolutional layers and a larger latent dimension (and thus more epochs) and by tuning the value of $\\beta$.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# generate images from the latent space\n", "def generate_images_latent(decoder, n_generation, feature_range):\n", "    # randomly sample the latent space\n", "    latent = []\n", "    for dim in range(latent_dim):\n", "        if len(np.array(feature_range).shape) == 1:\n", "            # only one range provided; used it for all dimensions\n", "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n", "                                            n_generation))\n", "        else:\n", "            # range provided for each dimension\n", "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n", "                                            n_generation))\n", "    latent = np.array(latent).T\n", "    \n", "    # decode images\n", "    decodings = decoder.predict(latent)\n", "    \n", "    # plot generated images\n", "    fig, axes = plt.subplots(n_generation // 2, 2, figsize=(16, n_generation / 2), dpi=100)\n", "    for ax, image in zip(axes.flatten(), decodings):\n", "        ax.matshow(image[:, :, 0])\n", "        ax.set_xticks([])\n", "        ax.set_yticks([])\n", "    plt.show()  \n", "\n", "# generate random images sampled between [-1, 1]\n", "generate_images_latent(decoder_BVAE, n_generation=30, feature_range=[-1, 1])\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "TAinw3rgw8ER", "outputId": "92218371-70f6-4bc8-d449-eb472bdc9e94"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "htmuMNvew8ES"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "i_pu65anw8ES"}, "source": ["## Exercises:\n", "\n", "1. Tune `latent_dim` and `beta` (and use more epochs) to improve the quality of image generation.\n", "2. Implement a conditional VAE for this INS dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "DHB2jU5_w8ES"}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "VAE_practical.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}