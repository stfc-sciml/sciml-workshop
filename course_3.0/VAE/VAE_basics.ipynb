{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Variational Autoencoders: the Basics\n", "\n", "Variation autoencoders (VAEs) are an extended type of autoencoders (AEs). A VAE can enhance the robustness of content generation by regularising the encodings distribution in the latent space. In this notebook, we will go through the fundamentals of VAEs (motivation, theory and Keras-based implementation) using the `mnist-digits` dataset. \n", "\n", "We will also learn two useful extensions of VAEs: the disentangled VAEs ($\\beta$-VAEs) and the conditional VAEs in [VAE_advanced.ipynb](VAE_advanced.ipynb)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')\n", "\n", "# fix the seed for consistent interpretation of the results\n", "import random as python_random\n", "python_random.seed(0)\n", "np.random.seed(0)\n", "tf.random.set_seed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The Dataset\n", "\n", "In this notebook, we will use the `mnist-digits` dataset. It is simpler than the `mnist-fashion` dataset, allowing us to use only two latent features so that we can conveniently visualise and examine the encodings distribution in the latent space."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0].shape))\n", "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image, cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=12)\n", "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images\n", "nrows = 4\n", "ncols = 20\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    subplot_image(1 - train_images[idata], '', nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Autoencoders and Regularity of Latent Space"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Why we need a VAE? To answer this question, let us start with an ordinary AE and see what is unsatisfactory when we use it to generate new images. \n", "\n", "## 1. Build and train an autoencoder\n", "\n", "Based on what we have learnt in [autoencoder_basics.ipynb](../AE/autoencoder_basics.ipynb), we can quickly build an AE with `Dense` layers. First, we specify the latent dimension or the size of the bottleneck; for `mnist-digits`, we can use 2."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# latent dimension\n", "latent_dim = 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The encoder\n", "\n", "The encoder contains four layers, an input layer with size 28$\\times$28, two hidden layers with sizes 128 and 16, respectively, and the latent output layer:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the encoder\n", "image_input = keras.Input(shape=(28, 28))\n", "x = layers.Flatten()(image_input)\n", "x = layers.Dense(128, activation='relu')(x)\n", "x = layers.Dense(16, activation=\"relu\")(x)\n", "latent_output = layers.Dense(latent_dim)(x)\n", "encoder_AE = keras.Model(image_input, latent_output)\n", "encoder_AE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The decoder\n", "\n", "The decoder also contains four layers that are reciprocal to those of the encoders, taking the latent representation as the input:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the decoder\n", "latent_input = keras.Input(shape=(latent_dim,))\n", "x = layers.Dense(16, activation=\"relu\")(latent_input)\n", "x = layers.Dense(128, activation=\"relu\")(x)\n", "x = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n", "image_output = layers.Reshape((28, 28))(x)\n", "decoder_AE = keras.Model(latent_input, image_output)\n", "decoder_AE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The autoencoder\n", "\n", "Joining up the encoder and the decoder, we obtain the AE network:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the AE\n", "image_input = keras.Input(shape=(28, 28))\n", "latent = encoder_AE(image_input)\n", "image_output = decoder_AE(latent)\n", "ae_model = keras.Model(image_input, image_output)\n", "ae_model.summary()\n", "\n", "# compile the AE\n", "ae_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the autoencoder\n", "\n", "Now we can train our AE with the `mnist-digits` dataset:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the AE\n", "ae_model.fit(train_images, train_images, epochs=50, batch_size=128, \n", "             validation_data=(test_images, test_images))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Inspect the latent space"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us inspect how the images are distributed in the latent space. \n", "\n", "### Encode images\n", "\n", "First, we encode the images by our AE. After that, each image becomes a 2D point (because `latent_dim=2`) in the latent space. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encode images by AE\n", "train_encodings_AE = encoder_AE.predict(train_images)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Scatter plot\n", "We can plot the points in the latent space and colour them by their true labels:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# scatter plot of encodings in the latent space\n", "def scatter_plot_encodings_latent(encodings, labels):\n", "    plt.figure(dpi=100)\n", "    scat = plt.scatter(encodings[:, 0], encodings[:, 1], c=labels, s=.5, cmap='Paired')\n", "    plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n", "                         title='Image labels', bbox_to_anchor=(1.5, 1.)))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.gca().set_aspect(1)\n", "    plt.show()\n", "    \n", "# scatter plot of encodings by AE\n", "scatter_plot_encodings_latent(train_encodings_AE, train_labels)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  Histogram plot\n", "\n", "Also, for each digit, we can plot the density histograms of the encodings along the two latent dimensions -- note that we are using the same feature range ($x$-axis) in all the histograms:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# histogram plot of encodings in the latent space\n", "def hist_plot_encodings_latent(encodings, labels, digit, dim, ax):\n", "    # extract\n", "    encodings_digit = encodings[labels == digit, dim]\n", "    # histogram\n", "    ax.hist(encodings_digit, bins=60, density=True, color=['g', 'b'][dim], alpha=.5)\n", "    # mean and std dev\n", "    mean = np.mean(encodings_digit)\n", "    std = np.std(encodings_digit)\n", "    ax.axvline(mean, c='r')\n", "    ax.set_xlabel('Digit %d, Feature %s\\n~${\\cal N}(\\mu=%.1f, \\sigma=%.1f)$' % \n", "                  (digit, ['X', 'Y'][dim], mean, std), c='k')\n", "    \n", "# histogram plot of encodings by AE\n", "fig, axes = plt.subplots(5, 4, dpi=100, figsize=(15, 12), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for digit in range(10):\n", "    hist_plot_encodings_latent(train_encodings_AE, train_labels, digit, 0, \n", "                               axes[digit // 2, digit % 2 * 2 + 0])\n", "    hist_plot_encodings_latent(train_encodings_AE, train_labels, digit, 1, \n", "                               axes[digit // 2, digit % 2 * 2 + 1])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Regularity of the latent space\n", "\n", "Both the scatter plot and the histogram plots show that the data distributions in the latent space are rather *irregular*. Some of the digits have very wide distributions (such as 1 and 7) and some very narrow distributions (such as 2 and 3). \n", "\n", "Remember that our goal of training this AE is neither dimensionality reduction nor denoising but to generate new images out of the original dataset. Image generation is done by the decoder, taking the latent representation (`X` and `Y` in the plots) as the input. An irregular latent space makes image generation less controllable and robust. Taking our case for example, two shortcomings are likely to emerge:\n", "\n", "1. **Controllability**: sampling the entire latent space, we will generate much more of the widely distributed digits than the narrowly distributed ones; instead, if we limit the range of the latent space, we will loss some characteristics of the widely distributed ones;\n", "\n", "2. **Robustness**: images that do not resemble any of the digits will be generated by the gaps between the distributions of the digits; such gaps increase with the range of the latent space."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Generate new images\n", "\n", "The following function generates new images by uniformly sampling the latent space within a specified range (`[x0, x1]`, `[y0, y1]`). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images from the latent space\n", "def generate_images_latent(decoder, x0, x1, dx, y0, y1, dy):\n", "    # uniformly sample the latent space\n", "    nx = round((x1 - x0) / dx) + 1\n", "    ny = round((y1 - y0) / dy) + 1\n", "    grid_x = np.linspace(x0, x1, nx)\n", "    grid_y = np.linspace(y1, y0, ny)\n", "    latent = np.array(np.meshgrid(grid_x, grid_y)).reshape(2, nx * ny).T\n", "\n", "    # decode images\n", "    decodings = decoder.predict(latent)\n", "    \n", "    # display a (nx, ny) 2D manifold of digits\n", "    figure = np.zeros((28 * ny, 28 * nx))\n", "    for iy in np.arange(ny):\n", "        for ix in np.arange(nx):\n", "            figure[iy * 28 : (iy + 1) * 28, ix * 28 : (ix + 1) * 28] = decodings[iy * nx + ix]\n", "            \n", "    # plot figure\n", "    plt.figure(dpi=100, figsize=(nx / 3, ny / 3))\n", "    plt.xticks(np.arange(28 // 2, nx * 28 + 28 // 2, 28), np.round(grid_x, 1), rotation=90)\n", "    plt.yticks(np.arange(28 // 2, ny * 28 + 28 // 2, 28), np.round(grid_y, 1))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.imshow(figure, cmap=\"Greys_r\")\n", "    plt.grid(False)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us see how the generated images look like. We choose a range of $-25<X<25$ and $-20<Y<30$, which encompasses all the digits and most of the data points. The two shortcomings can be observed:\n", "\n", "1. Only a very few instances of the narrowly distributed digits are generated, such as 2 and 3;\n", "2. Many images do not resemble any of the digits; note that the severely rotated digits should be recognised as non-digits in this context.\n", "\n", "Feel free to try some other ranges. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images by AE\n", "generate_images_latent(decoder_AE, x0=-20, x1=20, dx=1, y0=-20, y1=20, dy=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Variational Autoencoders\n", "\n", "Overfitting is the essential reason behind an irregular latent space of a naive AE, that is, the neural networks for encoding and decoding try their best to fit the data from end to end without caring about how the latent space is organised with respect to the original data. A VAE can regularise the latent space by imposing additional distributional properties on the latent space.\n", "\n", "The following figure summarises **the two extensions** from an AE to a VAE:\n", "\n", "1. Unlike a naive AE that encodes an input data $x$ as a single point $z$ in the latent space, a VAE encodes it as a normal distribution $\\mathcal{N}(\\mu, \\sigma)$, and the latent representation $z$ is sampled from this distribution and then passed to the decoder;\n", "\n", "2. An AE only minimises the reconstruction error $\\lVert x-x'\\rVert^2$ to fit the data, whereas a VAE minimises the sum of the reconstruction error and the KL divergence (Kullback\u2013Leibler divergence) between the latent distribution $\\mathcal{N}(\\mu, \\sigma)$ and the standard normal distribution $\\mathcal{N}(0, 1)$.\n", "\n", "How does a VAE regularise the latent space? The loss function provides a straightforward answer: in addition to fitting the data by minimising the reconstruction error, it also drags the latent distribution to a standard normal distribution. The final model is a trade-off between the two effects. Also, because each input image is encoded as a Gaussian blob instead of a single point, the gaps in the latent space can be filled by such blurring so that meaningless decodings can be largely avoided.\n", "\n", "![ae-vae.png](https://github.com/stfc-sciml/sciml-workshop-v3/blob/master/course_3.0_with_solutions/markdown_pic/ae-vae.png?raw=1)\n", "\n", "\n", "**\ud83d\udcda Theory for VAEs** \n", "\n", "<details> <summary>Reveal / Hide</summary> \n", "<p>\n", "    \n", "\n", "\n", "\n", "### Derivation of ELBO\n", "\n", "Posterior Probability $p(z|x)$, which can be expressed as:\n", "\n", "\\begin{eqnarray}\n", "p(z|x)&=&\\frac{p(x|z)p(z)}{p(x)}\\nonumber\\\\\n", "&=& \\frac{p(x|z)p(z)}{\\int p(x|z)p(z)}\n", "\\end{eqnarray}\n", "\n", "where $\\int p(x|z)p(z)$, which is the marginal, can be intractable and cannot be computed directly. One way  to compute the overall solution $p(z|x)$ is using Monte Carlo methods (such as sampling). The method used in this notebook (and the underlying VAE paper) is  ***variational inference***. \n", "\n", "The idea is to identify another proxy distribution $q(z|x)$ that reasonably approximates $p(z|x)=p(x|z)p(z)$. i.e. if the KL-divergence between two pdfs, $q(x)$ and $p(z|x)$ is denoted by\n", "\n", "$$\\mathrm{KL}(q(x)||p(z|x))$$\n", "\n", "\n", "it can be minimized by selecting an alternative pdf $q(z|x)$, which is a good proxy for $p(z|x)$. But \n", "\n", "\\begin{eqnarray}\n", "\\mathrm{KL}(q(z|x)||p(z|x)) &=& -\\int q(z|x)\\log\\frac{p(z|x)}{q(z|x)} dz\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{p(x)q(z|x)} dz\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz + \\int_{z} q(z|x)\\log p(x)dz \\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)} +  \\log p(x)\\int_{z} q(z|x)dz\\nonumber\\\\   \n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz +  \\log p(x)\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) \n", "\\end{eqnarray}\n", "\n", "\n", "Given that $\\mathrm{KL}\\left(q(z|x)||p(z|x)\\right)\\geq 0$, \n", "\n", "\n", "\n", "\n", "\\begin{eqnarray}\n", "-\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) &\\geq& 0 \\\\\n", "\\log p(x) &\\geq& \\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz + \\int q(z|x)\\log{p(x|z)}dz\\\\\n", "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\int q(z|x)\\log p(x|z)dz \\nonumber\\\\\n", "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \\nonumber\\\\\n", "\\end{eqnarray}\n", "\n", "This is the *variational lower-bound*, or the evidence of lower bound (ELBO).  This remains as the objective function for the VAE. However, frameworks like TensorFlow or PyTorch need a loss function to be minimized. Maximising the log likelihood of the model evidence $p(x)$ is same as minimizing the $-\\log p(x)$. The first term of the ELBO, namely,  $\\mathrm{KL}(q(z|x)||p(z))$ is the *regularising* term and constrains the posterior distribution. The second term of the ELBO models the reconstruction loss. \n", "\n", "Now, this leaves fair bit of freedom on the choice of the prior $p(z)$. Let's assume:\n", "\n", "\n", "$$\n", "p(z)={\\cal N}(\\mu_p, \\sigma_p^2)\n", "$$\n", "\n", "and \n", "\n", "$$\n", "q(z|x)={\\cal N}(\\mu_q, \\sigma_q^2)\n", "$$\n", "\n", "Thus, \n", "\n", "$$\n", "p(z)=\\frac{1}{\\sqrt{2\\pi\\sigma_p^2}}\\exp\\left(\\frac{(x-\\mu_p)^2}{2\\sigma_p^2}\\right)\n", "$$\n", "\n", "and \n", "\n", "$$\n", "q(z|x)=\\frac{1}{\\sqrt{2\\pi\\sigma_q^2}}\\exp\\left(\\frac{(x-\\mu_q)^2}{2\\sigma_q^2}\\right)\n", "$$\n", "\n", "The direct derivation of $\\mathrm{KL}(q(z|x)||p(z))$ will give (with some simplifications)\n", "\n", "\n", "$$\n", "-\\mathrm{KL}(q(z|x)||p(z)) = \\log\\frac{\\sigma_q}{\\sigma_p} - \\frac{\\left(\\log\\sigma_q^2-(\\mu_p-\\mu_q)^2\\right)}{2\\sigma_p^2} +\\frac{1}{2} \n", "$$\n", "\n", "By fixing the prior distribution $p(z)={\\cal N}(0,1^2)$, \n", "\n", "$$\n", "-\\mathrm{KL}(q(z|x)||p(z)) = \\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right]\n", "$$\n", "\n", "Hence, the new ELBO is\n", "\n", "\n", "$$\n", "\\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right] + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \n", "$$\n", "\n", "\n", "Let $J, B$ and $\\cal{L}$ be the dimension of the latent space, and the batch size over which the sampling is done. The loss function we need to minimise (from the point of  implementation) is\n", "\n", "$$\n", "{\\cal L} = - \\sum_{j=1}^J \\frac{1}{2}\\Bigl[ 1 + \\log\\sigma_j^2 - \\sigma_j^2 -\\mu_j^2\\Bigr] - \\frac{1}{B}\\sum_{l}\\mathbb{E}_{q(z|x_i)}\\left[\\log p(x_i|z^{(i,l)})\\right] \n", "$$\n", "\n", "\n", "\n", "This can be observed in the code implementation below (see function implementation ``loss_function`` below)\n", "\n", "### Reparameterisation\n", "\n", "A valid reparameterization would be \n", "\n", "$$\n", "z = \\mu+\\sigma\\epsilon\n", "$$\n", "\n", "\n", "where $\\epsilon$ is an auxiliary noise variable $\\epsilon\\sim{\\cal{N}}(0, 1)$, which actually enables the reparameterization technique. Although it is possible to use $\\sigma$ or more specifically $\\sigma^2$, working on log scales improves the stability. i.e. \n", "\n", "\\begin{eqnarray}\n", "p &=& \\log(\\sigma^2)\\\\\n", "&=& 2  \\log(\\sigma)\n", "\\end{eqnarray}\n", "\n", "To get the log standard deviation, $\\log(\\sigma)$, \n", "\\begin{eqnarray}\n", "\\log(\\sigma) &=&  p/2 \\\\\n", "\\label{eqn:log_sigma}\n", "\\end{eqnarray} \n", "\n", "and hence\n", "\n", "$$\n", "\\sigma = \\exp^{p/2}\n", "$$\n", "\n", "The resulting estimator (or the loss function) becomes (see Page 5 of [Auto-Encoding Variational Bayes Paper](https://arxiv.org/abs/1312.6114)),\n", "\n", "$$\n", "-\\text{KLD} = \\frac{1}{2}\\sum_{j=1}^{J}(1+\\log(\\sigma_j^i)^2 - (\\mu_j^i)^2 -(\\sigma_j^i)^2)\n", "$$\n", "\n", "\n", "It is important to see that the KL divergence can be computed and differentiated without estimation. This is a very remarkable thing (no esimtation!).\n", "\n", "The $\\boldsymbol{\\epsilon}$ must be sampled from a zero-mean, unit-variance Gaussian distribution, and should be of the same size as $\\boldsymbol{\\sigma}$. \n", "\n", "\n", "<br>\n", "\n", "\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Build and train a VAE\n", "\n", "Now we will implement a VAE for `mnist-digits`. \n", "\n", "\n", "### The encoder\n", "\n", "To implement the probabilistic encoder, we first need a custom function to sample the latent distribution, as implemented by the `Sampling` class. Note that here we are using $\\ln\\sigma$ instead of $\\sigma$ in the network; otherwise, the implementation will be complicated as we have to impose positiveness on $\\sigma$.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# sampling z with (z_mean, z_log_var)\n", "class Sampling(layers.Layer):\n", "    def call(self, inputs):\n", "        z_mean, z_log_var = inputs\n", "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n", "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n", "    \n", "    \n", "# build the encoder\n", "image_input = keras.Input(shape=(28, 28))\n", "x = layers.Flatten()(image_input)\n", "x = layers.Dense(128, activation='relu')(x)\n", "x = layers.Dense(16, activation=\"relu\")(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_VAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "encoder_VAE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The decoder\n", "\n", "The docoder is the same as that of AE."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the decoder\n", "z_input = keras.Input(shape=(latent_dim,))\n", "x = layers.Dense(16, activation=\"relu\")(z_input)\n", "x = layers.Dense(128, activation=\"relu\")(x)\n", "x = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n", "image_output = layers.Reshape((28, 28))(x)\n", "decoder_VAE = keras.Model(z_input, image_output)\n", "decoder_VAE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The VAE\n", "\n", "To add the KL divergence to the loss, we create a class `VAE` derived from `keras.Model` and overwrite its `train_step()` method:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# VAE class\n", "class VAE(keras.Model):\n", "    # constructor\n", "    def __init__(self, encoder, decoder, **kwargs):\n", "        super(VAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            # decoding\n", "            x_prime = self.decoder(z)\n", "            # reconstruction error by binary crossentropy loss\n", "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime)) * 28 * 28\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            loss = reconstruction_loss + kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"kl_loss\": kl_loss}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can build, compile and train our VAE:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the VAE\n", "vae_model = VAE(encoder_VAE, decoder_VAE)\n", "\n", "# compile the VAE\n", "vae_model.compile(optimizer=keras.optimizers.Adam())\n", "\n", "# train the VAE\n", "vae_model.fit(train_images, train_images, epochs=50, batch_size=128)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Inspect the latent space\n", "\n", "Next, we can inspect the latent space following the same steps we did for the AE. Clearly, the latent distributions become much more regular than before."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encode images by VAE\n", "train_encodings_VAE = encoder_VAE.predict(train_images)\n", "\n", "# scatter plot of encodings by VAE\n", "scatter_plot_encodings_latent(train_encodings_VAE[2], train_labels)\n", "\n", "# histogram plot of encodings by VAE\n", "fig, axes = plt.subplots(5, 4, dpi=100, figsize=(15, 12), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for digit in range(10):\n", "    hist_plot_encodings_latent(train_encodings_VAE[2], train_labels, digit, 0, \n", "                               axes[digit // 2, digit % 2 * 2 + 0])\n", "    hist_plot_encodings_latent(train_encodings_VAE[2], train_labels, digit, 1, \n", "                               axes[digit // 2, digit % 2 * 2 + 1])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Generate new images\n", "\n", "Finally, we can generate new images with our VAE. The result shows that, compared to the AE, the numbers of the generated digits have become more in unison and the number of non-digit images has been greatly reduced."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images by VAE\n", "generate_images_latent(decoder_VAE, x0=-2, x1=2, dx=.1, y0=-2, y1=2, dy=.1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "* Use convolutional layers to form the encoder and the decoder.\n", "\n", "* Learn advanced VAEs from [VAE_advanced.ipynb](VAE_advanced.ipynb):\n", "\n", "    * **disentangled VAEs**: how to balance between reconstruction loss (image quality) and variational loss (latent regularity);\n", "    \n", "    * **conditional VAEs**: how to control image generation, e.g., generating an image of a handwritten digit given that digit from 0 to 9."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 4}