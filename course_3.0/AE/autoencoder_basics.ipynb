{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Autoencoder: the Basics\n", "\n", "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \"noise\". Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. -- from [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder)\n", "\n", "An autoencoder can be used for \n", "\n", "* feature analysis and dimensionality reduction\n", "* denoising\n", "* generating artificial contents (such as faces, musics, scenes)\n", "\n", "\n", "\n", "![autoencoder.jpg](https://github.com/stfc-sciml/sciml-workshop-v3/blob/master/course_3.0_with_solutions/markdown_pic/vaee.png?raw=1)\n", "\n", "\n", "In this notebook, we will use an autoencoder in two different ways:\n", "\n", "* **Semi-supervised**: train an autoencoder using images from one specific class in the `fashion-mnist` dataset; the trained autoencoder can then be used to decide whether an image belongs to this class based on the reconstruction error; the autoencoder can also be used to generate new images of that class by changing the *features* in the latent space (the bottleneck);\n", "\n", "* **Unsupervised**: train an autoencoder using all images in `fashion-mnist` and cluster the images based on the features."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# k-means\n", "from sklearn.cluster import KMeans\n", "from sklearn import metrics\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')\n", "\n", "# widgets for dynamic controls\n", "import ipywidgets as widgets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The Dataset\n", "\n", "We start by loading the `fashion-mnist` dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "fashion_mnist = keras.datasets.fashion_mnist\n", "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0\n", "\n", "# string labels\n", "string_labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n", "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0].shape))\n", "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image, cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=12)\n", "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Semi-supervised\n", "\n", "In this part, we will train an autoencoder using one specific class in `fashion-mnist`. The autoencoder will use the images from this class as **both the input and the output**. After training the autoencoder, we can feed in any image to decide whether it belongs to this class based on whether it can be well reconstructed by the autoencoder.\n", "\n", "Such a strategy can be useful when \n", "\n", "* only one or a few classes in the dataset can be reliably labelled;\n", "* no data in the dataset can be reliably labelled but we can generate synthetic data for some specific classes.\n", "\n", "Besides, the autoencoder can be used to generate new images of that class by changing the **features** in the latent space (the bottleneck). "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Choose a target class\n", "\n", "First, we choose a target class for training and extract the corresponding image subsets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["###############################\n", "# choose a target class below #\n", "###############################\n", "# 'T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n", "# 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n", "target_class = 'Sneaker'\n", "\n", "# get image subset of the chosen class\n", "target_label = string_labels.index(target_class)\n", "target_train_images = train_images[train_labels == target_label]\n", "target_test_images = test_images[test_labels == target_label]\n", "print('Number of images labelled \"%s\":' % (target_class,))\n", "print('%d in training data' % (target_train_images.shape[0],))\n", "print('%d in test data' % (target_test_images.shape[0],))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Build and train the autoencoder\n", "\n", "As shown in the opening figure, an autoencoder contains three parts:\n", "\n", "* the encoder\n", "* the bottleneck\n", "* the decoder\n", "\n", "Here we use three dense layers to implement these parts. The layers in the encoder and the decoder are reciprocal."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Build the autoencoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["########################################\n", "# specify the size of the latent space #\n", "########################################\n", "nfeature = 8\n", "\n", "# build the network architecture\n", "autoencoder = Sequential()\n", "\n", "# encoder --------------------------------------------/\n", "autoencoder.add(Flatten(input_shape=(28, 28)))\n", "autoencoder.add(Dense(128, activation='relu'))\n", "# ----------------------------------------------------/\n", "\n", "# bottleneck -----------------------------------------/\n", "autoencoder.add(Dense(nfeature))\n", "# ----------------------------------------------------/\n", "\n", "# decoder --------------------------------------------/\n", "autoencoder.add(Dense(128, activation='relu'))\n", "autoencoder.add(Dense(784, activation='sigmoid'))\n", "autoencoder.add(Reshape((28, 28)))\n", "# ----------------------------------------------------/\n", "\n", "# print summary\n", "autoencoder.summary()\n", "\n", "# compile the model\n", "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', \n", "                    metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the autoencoder"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**NOTE**: The input and output images are identical in this case."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the model\n", "training_history = autoencoder.fit(target_train_images, target_train_images,\n", "                                   epochs=100, batch_size=32,\n", "                                   validation_data=(target_test_images, target_test_images))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Reconstruction and decision\n", "\n", "Next, we use our autoencoder to reconstruct all the test images and decide whether they belong to the target class based on the reconstruction error. Here we use the loss (computed by `model.evaluate()`) as the reconstruction error."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# reconstruct test images\n", "test_images_reconstructed = autoencoder.predict(test_images)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["############################\n", "# specify a threshold loss #\n", "############################\n", "# we suppose that an image should belong to the target class\n", "# if the loss is smaller than this threshold\n", "threshold_loss = 0.3\n", "\n", "# randomly plot original and reconstructed images and decisions\n", "nrows = 5\n", "ncols = 3\n", "plt.figure(dpi=100, figsize=(6 * ncols, 2.5 * nrows))\n", "for iplot, itest in enumerate(np.random.choice(len(test_images), nrows * ncols)):\n", "    # plot original and reconstructed images\n", "    subplot_image(test_images[itest], 'Origional', nrows, ncols * 3, iplot * 3)\n", "    subplot_image(test_images_reconstructed[itest], 'Reconstructed', nrows, ncols * 3, iplot * 3 + 1)\n", "    \n", "    # truth\n", "    truth = (test_labels[itest] == target_label)\n", "    # compute loss\n", "    loss = autoencoder.evaluate(np.array([test_images[itest]]), \n", "                                np.array([test_images[itest]]), verbose=0)[0]\n", "    # prediction\n", "    pred = (loss < threshold_loss)\n", "    # correct or not?\n", "    correct = (truth == pred)\n", "    \n", "    # write information\n", "    subplot_image([[0]], '', nrows, ncols * 3, iplot * 3 + 2)\n", "    textcolor = 'g' if correct else 'r'\n", "    plt.text(-.5, -.3, 'Truth: %s' % string_labels[test_labels[itest]], c='k')\n", "    plt.text(-.5, -.1, 'Loss: %.2f' % loss, c=textcolor)\n", "    plt.text(-.5, .2, 'Prediction:\\n%s' % (target_class if pred else 'Non-%s' % target_class), c=textcolor)\n", "    plt.text(-.5, .4, \"CORRECT\" if correct else 'INCORRECT', c=textcolor, weight='bold')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**NOTE**:\n", "It is clear that the autoencoder tends to reshape every input image into the target class. This is why we can use the reconstruction error for decision."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Generate new images\n", "\n", "We can also use the autoencoder to generate new images with the following steps:\n", "\n", "1. encode an input image by the encoder;\n", "2. change its features in the latent space;\n", "3. decode the changed features to generate a new image."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Extract encoder and decoder from autoencoder\n", "\n", "We must first extract the encoding and decoding parts from the autoencoder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encoder\n", "encoder = Sequential([\n", "    autoencoder.layers[0],\n", "    autoencoder.layers[1],\n", "    autoencoder.layers[2]],\n", "    name = 'encoder')\n", "encoder.summary()\n", "print('\\n\\n')\n", "\n", "# decoder\n", "decoder = Sequential([\n", "    Input(shape=(nfeature,)),\n", "    autoencoder.layers[-3],\n", "    autoencoder.layers[-2],\n", "    autoencoder.layers[-1]], \n", "    name='decoder')\n", "decoder.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Encode the images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encode training images \n", "target_train_images_encoded = encoder.predict(target_train_images)\n", "\n", "# reconstruct training images\n", "target_train_images_reconstructed = autoencoder.predict(target_train_images)\n", "\n", "# print the shapes\n", "print('Shape of original: %s' % str(target_train_images.shape))\n", "print('Shape of encoded: %s' % str(target_train_images_encoded.shape))\n", "print('Shape of reconstructed: %s' % str(target_train_images_reconstructed.shape))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Generate new images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# pick the index of the original image (0~5999)\n", "image_index = 5999\n", "\n", "# print original features\n", "encoded_original = target_train_images_encoded[image_index]\n", "print('The %d features in the latent space:' % nfeature)\n", "print(list(encoded_original))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the following cell, we will implement a UI with slider controls so that we can dynamically change the features and see how the decoded image reacts to such changes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create sliders\n", "sliders = []\n", "argdict = {}\n", "# slider range\n", "smin = encoded_original.min() - (encoded_original.max() - encoded_original.min())\n", "smax = encoded_original.max() + (encoded_original.max() - encoded_original.min())\n", "for i in np.arange(nfeature):\n", "    sliders.append(widgets.FloatSlider(min=smin, max=smax, step=.01, value=encoded_original[i], \n", "                                       description='Feature %d:' % (i + 1,),\n", "                                       layout=widgets.Layout(width='auto', height='auto')))\n", "    # associate sliders with varaibles passed to the respondent function\n", "    argdict['v%d' % (i + 1)] = sliders[i]\n", "ui = widgets.VBox(sliders)\n", "\n", "# the UI-respondent function\n", "def respond(**kwargs):\n", "    ##### decoding happens here #####\n", "    # decode\n", "    encoded_modified = np.array(list(kwargs.values()))\n", "    decoded_modified = decoder.predict(np.array([encoded_modified]))[0]\n", "    # plot images\n", "    plt.figure(dpi=100, figsize=(10, 3))\n", "    subplot_image(target_train_images[image_index], 'Original', 1, 3, 0)\n", "    subplot_image(target_train_images_reconstructed[image_index], 'Reconstructed original', 1, 3, 1)\n", "    subplot_image(decoded_modified, 'Reconstructed modified', 1, 3, 2)\n", "    plt.show()\n", "\n", "# craete UI\n", "out = widgets.interactive_output(respond, argdict)\n", "display(ui, out)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "* The size of the latent space, i.e., `nfeature` in the code, is the most critical network parameter. Try a smaller one (such as 4) and a larger one (such as 16) to see their influences.\n", "\n", "* Implement the autoencoder using convolutional layers. For decoding, you can use `Conv2DTranspose` and `UpSampling2D` as the reciprocal layers of `Conv2D` and `MaxPool2D`, respectively.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Unsupervised"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this part, we will use an autoencoder for unsupervised learning. We have learnt two clustering algorithms in [clustering_kmeans_GMM.ipynb](../CLASSICAL/clustering_kmeans_GMM.ipynb), k-means and the Gaussian mixture method. Clustering algorithms are challenged by a large input dimensionality, such as 28 $\\times$ 28 for `fashion-mnist`. To reduce the dimensionality, we can first train an autoencoder to encode the images and conduct clustering in the latent space."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Build and train the autoencoder\n", "\n", "We use the same architecture as the previous one. The only difference is that we will use all the 10 classes for training."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["########################################\n", "# specify the size of the latent space #\n", "########################################\n", "nfeature_all = 8\n", "\n", "# build the network architecture\n", "autoencoder_all = Sequential()\n", "\n", "# encoder --------------------------------------------/\n", "autoencoder_all.add(Flatten(input_shape=(28, 28)))\n", "autoencoder_all.add(Dense(128, activation='relu'))\n", "# ----------------------------------------------------/\n", "\n", "# bottleneck -----------------------------------------/\n", "autoencoder_all.add(Dense(nfeature_all))\n", "# ----------------------------------------------------/\n", "\n", "# decoder --------------------------------------------/\n", "autoencoder_all.add(Dense(128, activation='relu'))\n", "autoencoder_all.add(Dense(784, activation='sigmoid'))\n", "autoencoder_all.add(Reshape((28, 28)))\n", "# ----------------------------------------------------/\n", "\n", "# print summary\n", "autoencoder_all.summary()\n", "\n", "# compile the model\n", "autoencoder_all.compile(optimizer='adam', loss='binary_crossentropy', \n", "                        metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the model with all the 10 classes\n", "training_history_all = autoencoder_all.fit(train_images, train_images,\n", "                                           epochs=50, batch_size=32,\n", "                                           validation_data=(test_images, test_images))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Encode the images\n", "\n", "Extract the encoder from the autoencoder and use it to encode the images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# extract encoder\n", "encoder_all = Sequential([\n", "    autoencoder_all.layers[0],\n", "    autoencoder_all.layers[1],\n", "    autoencoder_all.layers[2]])\n", "encoder_all.summary()\n", "\n", "# encode images\n", "train_images_encoded = encoder_all.predict(train_images)\n", "print('\\n* Shape of the encoded images: %s' % str(train_images_encoded.shape))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Cluster the encoded  images\n", "\n", "Now we can use k-means to cluster the encoded images. Note that the number of clusters does not have to be the number of the original classes (10). Here we will use 20 to relax the constraints for k-means."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build k-means\n", "n_clusters = 20\n", "kmeans_model = KMeans(n_clusters=n_clusters, max_iter=500)\n", "\n", "# train and predict\n", "kmeans_labels = kmeans_model.fit_predict(train_images_encoded)\n", "\n", "# print scores\n", "print('Homogeneity score = %.3f' % metrics.homogeneity_score(train_labels, kmeans_labels))\n", "print('Completeness score = %.3f' % metrics.completeness_score(train_labels, kmeans_labels))\n", "print('V-measure score = %.3f' % metrics.v_measure_score(train_labels, kmeans_labels))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Further, we can plot a few images in each cluster to get a sense of the quality of clustering:\n", "\n", "* It is shown that some similar-looking classes such as \"T-shirts\", \"Shirts\", \"Coats\" and \"Pullovers\" are not well resolved, which implies that we may need a larger latent space in the autoencoder. \n", "\n", "* Also, it is interesting that some classes are clustered into sub-classes, such as \"Bags\" into the ones with and without handles and the clothes into dark- and light-coloured ones; this explains why we need more than 10 clusters for k-means."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_image_per_cluster = 15\n", "for cluster_label in np.arange(n_clusters):\n", "    # get indices of the first n_image_per_cluster in this cluster\n", "    itrains = np.where(kmeans_labels == cluster_label)[0][0:n_image_per_cluster]\n", "    print('\\n\\nImages in Cluster %d:' % cluster_label)\n", "    fig = plt.figure(dpi=100, figsize=(n_image_per_cluster * 2, n_clusters * 2))\n", "    for iplot, itrain in enumerate(itrains):\n", "        subplot_image(train_images[itrain], string_labels[train_labels[itrain]], \n", "                      n_clusters, n_image_per_cluster, iplot)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualise clustering in the latent space\n", "\n", "Finally, we can visualise the clustering results in the latent space. These plots can manifest which features in the latent space are dominating the formation of certain clusters. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# specify total number of data points to show\n", "show_ndata = 200\n", "\n", "# specify the clusters to show\n", "# showing all 20 clusters will make the figures unreadable; just select a few\n", "show_clusters = np.array([1, 2, 3])\n", "\n", "# find the data subset\n", "show_data_indices = np.where(np.isin(kmeans_labels, show_clusters))[0][0:show_ndata]\n", "\n", "# normalise features to [0, 1]\n", "train_images_encoded_norm = (train_images_encoded - train_images_encoded.min()) / \\\n", "                            (train_images_encoded.max() - train_images_encoded.min())\n", "# for better visualisation\n", "train_images_encoded_norm = np.power(train_images_encoded_norm, .5)\n", "\n", "# plot\n", "plt.figure(dpi=100, figsize=(15, 15))\n", "iplot = 0\n", "for feature_x in np.arange(nfeature_all):\n", "    for feature_y in np.arange(nfeature_all):\n", "        plt.subplot(nfeature_all, nfeature_all, iplot + 1)\n", "        x = train_images_encoded_norm[show_data_indices, feature_x]\n", "        y = train_images_encoded_norm[show_data_indices, feature_y]\n", "        plt.scatter(x, y, c=kmeans_labels[show_data_indices], s=10, cmap=plt.cm.Paired)\n", "        plt.xticks([])\n", "        plt.yticks([])\n", "        iplot += 1\n", "        if feature_y == 0:\n", "            plt.ylabel('Feature %d' % feature_x, c='k')\n", "        if feature_x == 0:\n", "            plt.gca().xaxis.set_label_position('top') \n", "            plt.xlabel('Feature %d' % feature_y, c='k')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "Try to increase the quality of clustering by using\n", "1. a larger latent space in the autoencoder\n", "2. convolutional layers for encoding and decoding\n", "3. GMM for clustering the encodings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 2}