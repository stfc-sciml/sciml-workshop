{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# DNN: backprop\n", "\n", "Deep learning applications have been tremendously facilitated by various open-source libraries such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/) and [Apache MXNet](https://mxnet.apache.org/). This notebook aims at delivering a deeper understanding of what happens inside the black box by \n", "\n", "* demonstrating Gradient Descent in 1D;\n", "* demonstrating different levels of implementing the training loop with TensorFlow;\n", "* creating a fully-connected DNN from scratch based on `numpy`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Flatten, Dropout\n", "\n", "# helpers\n", "from tqdm import trange\n", "import time\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Gradient Descent in 1D\n", "\n", "Gradient Descent is a first-order iterative approach to solve a non-linear programming problem with continuous differentiability. To better understand it, let us solve a 1D problem:\n", "\n", "> $\\underset{x}{\\mathrm{argmin}}\\ f(x)$\n", "\n", "The alogrithm is\n", "\n", "> $x_{k+1} = x_{k}-\\eta \\left . \\dfrac{\\partial f}{\\partial x} \\right\\vert _{x=x_k},\\quad \\eta\\in R^+$\n", "\n", "The algorithm can be simply implemented as"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def GD_1D(f, df, x0, eta, n_iter, alpha=0.):\n", "    \"\"\"\n", "    GD in 1D\n", "    :param f: target function\n", "    :param df: derivative of target function\n", "    :param x0: inital guess\n", "    :param eta: learning rate\n", "    :param n_iter: number of interations\n", "    :param alpha: momentum decay factor  (default=0., no momentum)\n", "    :return: trajactory of convergence\n", "    \"\"\"\n", "    # allocate arrays\n", "    x = np.zeros((n_iter + 1))\n", "    y = np.zeros((n_iter + 1))\n", "    \n", "    # initial guess\n", "    x[0] = x0\n", "    y[0] = f(x0)\n", "    \n", "    # iteration\n", "    dx = 0.\n", "    for k in range(n_iter):\n", "        dx = - eta * df(x[k]) + alpha * dx\n", "        x[k + 1] = x[k] + dx\n", "        y[k + 1] = f(x[k + 1])\n", "        \n", "    # return trajactory\n", "    return x, y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we can choose a non-linear function to try out GD:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# f(x)\n", "def f(x):\n", "    return 3 * x ** 4 - 6 * x ** 2 - 2 * x + 2\n", "\n", "# \u2202f/\u2202x\n", "def df(x):\n", "    return 12 * x ** 3 - 12 * x - 2\n", "\n", "# \u2202f/\u2202x by AutoDiff\n", "def df_auto(x):\n", "    # tell TensorFlow that x is a variable \n", "    x_ = tf.Variable(x)\n", "    # build graph with GradientTape\n", "    with tf.GradientTape() as tape:\n", "        y = f(x_)\n", "    # compute gradient\n", "    dy_dx = tape.gradient(y, x_)\n", "    return dy_dx.numpy()\n", "    \n", "# initial guess\n", "x0 = 0\n", "\n", "# learning rate\n", "eta = 0.01\n", "\n", "# num iter\n", "n_iter = 10\n", "\n", "# perform GD\n", "x, y = GD_1D(f, df_auto, x0, eta, n_iter, alpha=0.)\n", "\n", "# display results\n", "print(f'final x: {x[-1]}')\n", "print(f'final y: {y[-1]}')\n", "\n", "# plot range\n", "x_range = np.linspace(-1.5, 1.5, 1000)\n", "plt.figure(dpi=200)\n", "plt.plot(x_range, f(x_range), ls='--', label='$f(x)$')\n", "plt.plot(x, y, label='Trajectory', marker='.')\n", "plt.scatter(x[0], y[0], c='b', zorder=100, s=100, marker='+')\n", "plt.scatter(x[-1], y[-1], c='r', zorder=100, s=100, marker='+')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "Implement [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) and [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) for 1D Gradient Descent.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Training Loop in TensorFlow\n", "\n", "In [DNN_basics.ipynb](DNN_basics.ipynb) and [DNN_practical.ipynb](DNN_practical.ipynb), we have conducted training by simply calling `tf.keras.Model.fit()`. It is the most convenient way for simple network architectures and loss functions. When a more complicated architecture or loss function is required, we may have to use some lower-level APIs rather than a simple call of `model.fit()`. Here we introduce the two other levels: \n", "\n", "* overloaded member function `train_step()`, which exposes backpropagation to users, and\n", "* handcoded training loop, which exposes epoch and mini-batch loops to users.\n", "\n", "\n", "### Load dataset \n", "\n", "It is the same as in [DNN_basics.ipynb](DNN_basics.ipynb):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "fashion_mnist = keras.datasets.fashion_mnist\n", "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Method 1: `model.fit()`\n", "\n", "The simplest way to train a DNN is to create a build-in `Sequential` model and call `model.fit()`, as what we did in [DNN_basics.ipynb](DNN_basics.ipynb):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the network architecture\n", "model = Sequential()\n", "model.add(Flatten(input_shape=(28, 28)))\n", "model.add(Dense(10, activation='sigmoid'))\n", "\n", "# compile the model\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])\n", "\n", "# train\n", "history = model.fit(train_images, train_labels, epochs=2, batch_size=32)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Method 2: `train_step()`\n", "\n", "Still calling `model.fit()`, we can customize what happens in `model.fit()` by overloading the member function `train_step()` of the class `tf.keras.Model`. This function contains the process of forward and back propagation, useful for more complex architectures and loss functions. In [VAE_basics.ipynb](../VAE/VAE_basics.ipynb), for example, we use this method to implement the KL-divergence loss in a variational autoencoder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CustomModel(keras.Model):\n", "    def train_step(self, data):\n", "        # Unpack the data. Its structure depends on your model and\n", "        # on what you pass to `fit()`.\n", "        x, y = data\n", "\n", "        with tf.GradientTape() as tape:\n", "            y_pred = self(x, training=True)  # Forward pass\n", "            # Compute the loss value\n", "            # (the loss function is configured in `compile()`)\n", "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n", "\n", "        # Compute gradients\n", "        trainable_vars = self.trainable_variables\n", "        gradients = tape.gradient(loss, trainable_vars)\n", "        # Update weights\n", "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n", "        # Update metrics (includes the metric that tracks the loss)\n", "        self.compiled_metrics.update_state(y, y_pred)\n", "        # Return a dict mapping metric names to current value\n", "        return {m.name: m.result() for m in self.metrics}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the network architecture\n", "inputs = keras.Input(shape=(28, 28))\n", "flattened = Flatten(input_shape=(28, 28))(inputs)\n", "outputs = Dense(10)(flattened)\n", "model = CustomModel(inputs, outputs)\n", "\n", "# compile the model\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])\n", "\n", "# train\n", "history = model.fit(train_images, train_labels, epochs=2, batch_size=32)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Method 3: Handcoded training loop\n", "\n", "The `train_step()` method can become insufficiently flexible for even more complicated architectures or loss functions. For example, `train_step()` only allows for one argument, but we may want more arguments to control the workflow. One can reach the maximum flexibility by coding training loop explicitly. In `GAN_basics.ipynb`, for example, we use this method to implement a Generative Adversarial Network or GAN.\n", "\n", "First, we create a model, not necessarily a `Sequential` one:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the network architecture\n", "model = Sequential()\n", "model.add(Flatten(input_shape=(28, 28)))\n", "model.add(Dense(10, activation='sigmoid'))\n", "\n", "# compile the model\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we write our own `train_step()` alternative, named `my_own_train_step()`, which is a generic Python function with any number of arguments (here we pass images and labels) and returns (here we return the loss):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Notice the use of `tf.function`\n", "# This annotation causes the function to be \"compiled\".\n", "@tf.function\n", "def my_own_train_step(images, labels):\n", "    # Unpack the data. Its structure depends on your model and\n", "    # on what you pass to `fit()`.\n", "    x, y = images, labels\n", "\n", "    with tf.GradientTape() as tape:\n", "        y_pred = model(x)  # Forward pass\n", "        # Compute the loss value\n", "        # (the loss function is configured in `compile()`)\n", "        loss = model.compiled_loss(y, y_pred, regularization_losses=model.losses)\n", "\n", "    # Compute gradients\n", "    trainable_vars = model.trainable_variables\n", "    gradients = tape.gradient(loss, trainable_vars)\n", "    # Update weights\n", "    model.optimizer.apply_gradients(zip(gradients, trainable_vars))\n", "    # Update metrics (includes the metric that tracks the loss)\n", "    model.compiled_metrics.update_state(y, y_pred)\n", "    \n", "    # return loss\n", "    return loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, the training loop can be implemented as follows:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# epochs\n", "EPOCHS = 2\n", "BATCH_SIZE = 32\n", "NUM_IMAGES = train_images.shape[0]\n", "\n", "# epoch loop\n", "tstart = time.time()\n", "for epoch in range(EPOCHS):\n", "    # mini-batch loop\n", "    epochs = trange(0, NUM_IMAGES, BATCH_SIZE)\n", "    for begin_index in epochs:\n", "        end_index = min(begin_index + BATCH_SIZE, NUM_IMAGES)\n", "        images = train_images[begin_index:end_index]\n", "        labels = train_labels[begin_index:end_index]\n", "        # call my_own_train_step\n", "        loss = my_own_train_step(images, labels)\n", "        epochs.set_description(f\"Epoch %d, Loss=%.2f\" % (epoch, loss))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "In the above implementation, we manually calculated the starting and ending indices of the mini-batch. Use [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to automate this process -- and it can do much more than just this."]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Gradient Descent and Backprop for DNN\n", "\n", "Before the implementation, we must first understand how a neural network works. In a nutshell, a neural network trys to minimise the prediction error or the loss, $\\epsilon$, by tuning the model parameters $\\mathbf{w}$. **Gradient Descent** is the most fundamental algorithm for this purpose, which iteratively updates $\\mathbf{w}$ in the \"gradient\" direction $\\nabla\\epsilon=\\dfrac{\\partial \\epsilon}{\\partial \\mathbf{w}}$. This can be achieved through cycles of forward and backward propagations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Forward propagation\n", "\n", "For a fully-connected dense layer, the trainable model parameters include the **weights** and the **biases**. Let $N_l$ denote the number of neurons in the $l$-th layer. The $l$-th layer then contains $N_{l}\\times N_{l-1}$ weights and $N_l$ biases, as denoted respectively by $w^l_{ij}$ and $b^l_i$, where $1\\le i \\le N_{l}$ and $1\\le j \\le N_{l-1}$, as shown in the following figure.\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop-v3/blob/master/course_3.0_with_solutions/markdown_pic/fwdd.png?raw=1\" width=\"40%\">\n", "\n", "The forward propagation passes the input data to the first hidden layer and computes the values on its neurons (using the current weights and biases), which are then \"**activated**\" and passed to the second layer and so on until the output layer is reached. The pre-activation value at the $i$-th neuron in the $l$-th layer, as denoted by $z^l_{i}$, is computed by\n", "\n", "$$z^l_{i}=\\sum_{j=1}^{N_{l-1}} w^l_{ij} a^{l-1}_{j} + b^l_i,\\quad i=1,2,\\cdots,N_{l},\n", "\\label{eq:zi} \\tag{1}\n", "$$\n", "\n", "where $a^{l-1}_{j}$ is the post-activation value at the $j$-th neuron in the $(l-1)$-th layer. Then the post-activation values of the $l$-th layer are computed using the given activation function $f_l$, such as `ReLU` and `sigmoid`:\n", "\n", "$$a^l_{i}=f_l\\left(z^l_{i}\\right),\\quad i=1,2,\\cdots,N_{l}.\n", "\\label{eq:ai} \\tag{2}\n", "$$\n", "\n", "Next, $a^{l}_{i}$ will be passed to the $(l+1)$-th layer to compute $z^{l+1}_{k}$, $k=1,2,\\cdots,N_{l+1}$.\n", "\n", "#### Implementation in Python\n", "\n", "Assume that we have a class called `Layer` that has properties `w` and `b` and a member function `activation_func`. The forward propagation through this layer can be implemented as follows, passing `a` from the previous layer:\n", "\n", "```python\n", "def forward(self, a_prev):\n", "    # pre-activation\n", "    self.z = np.dot(self.w, a_prev) + self.b\n", "    # post-activation\n", "    self.a = self.activation_func(self.z)\n", "    # return a to feed the next layer\n", "    return self.a\n", "```\n", "\n", "Note that we store `z` and `a` (by using `self.`) because they will be needed for backpropagation.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Backward propagation of errors\n", "\n", "Suppose that our DNN has $M$ layers in total, so the $M$-th layer will be the output layer and $a^{M}_{k}$ the final prediction. The vector-valued error is computed by subtracting the prediction $a^{M}_{k}$ and the ground truth $y_{k}$. Let us use the Mean Squared Error (MSE) as the scalar-valued loss function, i.e.,\n", "\n", "$$\\epsilon=\\dfrac{1}{N_M}\\sum_{k=1}^{N_M}(a^{M}_{k}-y_{k})^2.\n", "\\label{eq:eps}\\tag{3}\n", "$$\n", "\n", "\n", "The purpose of backpropagation is to find the model gradients $\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}$ and $\\dfrac{\\partial \\epsilon}{\\partial b^l_{i}}$. They can be evaluated based on the **chain rule**:\n", "\n", "\n", "$$\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}\n", "=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}\n", "=\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}\\dfrac{d a^l_{i}}{d z^l_{i}}\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}.\n", "\\label{eq:w}\\tag{4}\n", "$$\n", "\n", "The second term on the R.H.S. of eq. $\\eqref{eq:w}$, $\\dfrac{d a^l_{i}}{d z^l_{i}}$, is the derivative of the activation funciton, $f_l'\\left({z^l_{i}}\\right)$, and the third term $\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}$ simply $a^{l-1}_{j}$. The first term is more complicated because $a^l_{i}$ contributes to $\\epsilon$ via the whole $(l+1)$-th layer, that is,\n", "$\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}= \\sum\\limits_{k=1}^{N_{l+1}} \n", "\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}\\dfrac{\\partial z^{l+1}_{k}}{\\partial a^l_{i}}  =\n", "\\sum\\limits_{k=1}^{N_{l+1}} \n", "\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}w_{ki}^{l+1}.\n", "$\n", "Inserted with all these three terms, the above gradient can be eventually rearrange as (also considering the output layer, $l=M$):\n", "\n", "$$\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}\n", "=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}a^{l-1}_{j}\n", "=\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}\n", "f'_l\\left({z^l_{i}}\\right)a^{l-1}_{j}, \\quad \\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}=\n", "\\begin{cases}\n", "\\dfrac{2}{N_M}(a^{l}_{i}-y_{i}),&l=M;\\\\\n", "\\sum\\limits_{k=1}^{N_{l+1}} w_{ki}^{l+1}\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}},&l<M.\n", "\\end{cases}\n", "\\label{eq:wij}\\tag{5}\n", "$$\n", "\n", "\n", "Obviously, the above gradient must be computed in a backward sequence (from $l=M$ to $l=1$), with the \"errors\" $\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}$ passed from the $(l+1)$-th layer to the $l$-th layer; this is why the process is called backpropagtion. For the biases, it is straightforward to show that \n", "$\\dfrac{\\partial \\epsilon}{\\partial b^l_{i}}=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}$ because \n", "$\\dfrac{\\partial z^l_{i}}{\\partial b^l_{i}}=1$.\n", "\n", "\n", "#### Implementation in Python\n", "\n", "Based on eq. $\\eqref{eq:wij}$, the backpropagation can be coded as follows:\n", "\n", "```python\n", "# input: 1) \u2202\u03b5/\u2202a of this layer but computed in the next layer\n", "#        2) a of the previous layer\n", "def backward(self, de_da, a_prev):\n", "    # \u2202\u03b5/\u2202z (which is also \u2202\u03b5/\u2202b)\n", "    de_dz = de_da * self.activation_func(self.z, derivative=True)\n", "    # accumulate \u2202\u03b5/\u2202w, \u2202\u03b5/\u2202b\n", "    self.de_dw += np.outer(de_dz, a_prev)\n", "    self.de_db += de_dz\n", "    # \u2202\u03b5/\u2202a to be passed to the previous layer\n", "    de_da_prev = np.dot(self.w.T, de_dz)\n", "    return de_da_prev\n", "```\n", "\n", "Here we accumulate the gradients (instead of directly updating the parameters) because we will employ Mini-batch Gradient Descent for parameter update, as introduced below."]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  Mini-batch Gradient Descent\n", "\n", "Through one iteration of forward and backward propagations, each data in the dataset will yield a gradient. Then the question is which gradient we should use to update the model parameters. To answer this question, we will use an algorithm called **Mini-batch Gradient Descent**:\n", "\n", "$$\n", "\\Delta w_{ij}=-\\eta \\dfrac{1}{B} \\sum_{m=1}^{B}  \\dfrac{\\partial\\epsilon_m}{\\partial w_{ij}},\n", "\\label{eq:mini}\\tag{6}\n", "$$\n", "\n", "where $B$ is a given **batch size**, $\\dfrac{\\partial\\epsilon_m}{\\partial w_{ij}}$ the gradient computed with the $m$-th data in the mini-batch and $\\eta$ the **learning rate**. \n", "\n", "\n", "#### Batch size $B$\n", "\n", "When $B$ is selected to be the total number of data in the dataset, the algorithm is usually referred to as **Batch Gradient Descent**. For a non-convex optimisation problem (which is generally the case in deep learning), Batch Gradient Descent can easily be trapped by local minima.\n", "\n", "To help the algorithm to escape from local minima, we can add a bit noise to the trajectory of gradient descent by using a gradient averaged over a *random subset* of the dataset -- the so called **Mini-batch Stochastic Gradient Descent** or **Mini-batch Gradient Descent**. The noise level decreases with $B$. When $B=1$, the algorithm is commonly known as **Stochastic Gradient Descent**, which introduces the highest noise level and thus may suffer from slow convergence.  \n", "\n", "\n", "#### Learning rate $\\eta$\n", "Now we have found the direction to update $w^l_{ij}$ and $b^l_{i}$, but we still need to determine the magnitude of the update. This will introduce another network parameter called the **learning rate**. In our implementation, we will use a constant learning rate. In applications, it is usually more efficient to use an adaptive learning rate, such as by using the Adam optimiser. \n", "\n", "> \"The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.\" -- Goodfellow, Deep Learning\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Implementation from Scratch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Activation functions\n", "\n", "We first define some activation functions. They can be passed as an argument to create a `Layer` object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# relu\n", "def relu(x, derivative=False):\n", "    if derivative:\n", "        return 1. * (x > 0)\n", "    else:\n", "        return x * (x > 0.)\n", "\n", "# sigmoid\n", "def sigmoid(x, derivative=False):\n", "    if derivative:\n", "        s = sigmoid(x)\n", "        return s * (1. - s)\n", "    else:\n", "        return 1. / (1. + np.exp(-x))\n", "    \n", "# linear\n", "def linear(x, derivative=False):\n", "    if derivative:\n", "        return np.full(x.shape, 1.)\n", "    else:\n", "        return x.copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Class `Layer`\n", "The `Layer` class is where the model parameters and neuron values are stored and the layer-wise operations happen."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# layer class\n", "class Layer:\n", "    # constructor\n", "    def __init__(self, size, activation=linear, name='Untitled_Layer'):\n", "        # we cannot allocate data here because \n", "        # the size of the previous layer is unknown\n", "        self.size = size\n", "        self.activation_func = activation\n", "        self.name = name\n", "        \n", "    # intialise parameters\n", "    def initParameters(self, prev_size):\n", "        # weights and biases,\n", "        # intialised with random numbers from the \u201cstandard normal\u201d distribution\n", "        self.w = np.random.randn(self.size, prev_size) * np.sqrt(1. / self.size)\n", "        self.b = np.zeros((self.size,))\n", "        \n", "        # neuron values\n", "        self.a = np.zeros((self.size,))\n", "        self.z = np.zeros((self.size,))\n", "        \n", "        # accumulated gradients\n", "        self.de_dw = np.zeros((self.size, prev_size))\n", "        self.de_db = np.zeros((self.size,))\n", "        \n", "    # forward propagation\n", "    def forward(self, a_prev):\n", "        # pre-activation\n", "        self.z = np.dot(self.w, a_prev) + self.b\n", "        # post-activation\n", "        self.a = self.activation_func(self.z)\n", "        # return a to feed the next layer\n", "        return self.a\n", "    \n", "    # backward propagation\n", "    def backward(self, de_da, a_prev):\n", "        # \u2202\u03b5/\u2202z (which is also \u2202\u03b5/\u2202b)\n", "        de_dz = de_da * self.activation_func(self.z, derivative=True)\n", "        # accumulate \u2202\u03b5/\u2202w, \u2202\u03b5/\u2202b\n", "        self.de_dw += np.outer(de_dz, a_prev)\n", "        self.de_db += de_dz\n", "        # \u2202\u03b5/\u2202a to be passed to the previous layer\n", "        de_da_prev = np.dot(self.w.T, de_dz)\n", "        return de_da_prev\n", "    \n", "    # update parameters\n", "    def updateParameters(self, learning_rate, batch_size):\n", "        # update\n", "        self.w -= (learning_rate / batch_size) * self.de_dw\n", "        self.b -= (learning_rate / batch_size) * self.de_db\n", "        \n", "        # reset accumulated for the next mini-batch\n", "        self.de_dw.fill(0.)\n", "        self.de_db.fill(0.)\n", "        \n", "    # print\n", "    def __str__(self):\n", "        s = '%s\\n' % self.name\n", "        s += 'Size = %d\\n' % self.size\n", "        s += 'Activation = %s\\n' % (self.activation_func.__name__,)\n", "        try:\n", "            s += 'Number of weights = %d\\n' % self.w.size\n", "            s += 'Number of biases = %d\\n' % self.b.size\n", "        except:\n", "            s += 'Not set in a model.\\n'\n", "        return s"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Class `Model`\n", "\n", "The `Model` class contains many `Layer` objects and controls the workflow of training."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# model class\n", "class Model:\n", "    # constructor\n", "    def __init__(self, input_size, layers, name='Untitled_Model'):\n", "        self.input_size = input_size\n", "        self.layers = layers\n", "        self.name = name\n", "        \n", "        # initialise layers\n", "        prev_size = input_size\n", "        for layer in self.layers:\n", "            layer.initParameters(prev_size)\n", "            prev_size = layer.size\n", "            \n", "    # forward propagation\n", "    def forward(self, input_data):\n", "        a_prev = input_data.copy()\n", "        for layer in self.layers:\n", "            a_prev = layer.forward(a_prev)\n", "        return a_prev\n", "    \n", "    # backward propagation\n", "    def backward(self, input_data, error):\n", "        de_da = error.copy()\n", "        # from the last to the second layers\n", "        for ilayer in np.arange(len(self.layers) - 1, 0, -1):\n", "            de_da = self.layers[ilayer].backward(de_da, self.layers[ilayer - 1].a)\n", "        # the first layer\n", "        self.layers[0].backward(de_da, input_data)\n", "        \n", "    # update parameters\n", "    def updateParameters(self, learning_rate, batch_size):\n", "        for layer in self.layers:\n", "            layer.updateParameters(learning_rate, batch_size)\n", "    \n", "    # compute loss and accuracy for history\n", "    def computeLossAccHistory(self, xs, ys, history, key_prefix, iep):\n", "        for x, y in zip(xs, ys):\n", "            pred_y = self.forward(x)\n", "            history[key_prefix + 'loss'][iep] += np.linalg.norm(pred_y - y) / len(pred_y)\n", "            history[key_prefix + 'acc'][iep] += int(np.argmax(pred_y) == np.argmax(y))\n", "        history[key_prefix + 'loss'][iep] /= xs.shape[0]\n", "        history[key_prefix + 'acc'][iep] /= xs.shape[0]\n", "            \n", "    # train\n", "    def train(self, train_x, train_y, \n", "              epochs=2, batch_size=32, learning_rate=0.1, validation_data=None, verbose=1):\n", "        # number of data\n", "        ndata = train_x.shape[0]\n", "        \n", "        # number of data in mini-batches\n", "        n_mini_batch = ndata // batch_size + int(ndata % batch_size > 0)\n", "        n_data_mini_batches = np.full((n_mini_batch,), batch_size)\n", "        # the last one may have fewer\n", "        n_data_mini_batches[-1] = ndata - (n_mini_batch - 1) * batch_size\n", "        \n", "        # history\n", "        history = {'loss': np.zeros((epochs,)), 'acc': np.zeros((epochs,))}\n", "        if validation_data is not None:\n", "            history['val_loss'] = np.zeros((epochs,))\n", "            history['val_acc'] = np.zeros((epochs,))\n", "        \n", "        # epoch loop\n", "        start_time = time.time()\n", "        for iep in np.arange(epochs):\n", "            # data must be shuffled before each epoch\n", "            permute = np.random.permutation(ndata)\n", "            train_x_sh = train_x[permute].copy()\n", "            train_y_sh = train_y[permute].copy()\n", "            \n", "            # mini-batch loop\n", "            for ibatch in np.arange(n_mini_batch):\n", "                # data loop\n", "                for idata in np.arange(ibatch * batch_size, n_data_mini_batches[ibatch]):\n", "                    # forward\n", "                    pred_y = self.forward(train_x_sh[idata])\n", "                    # compute MSE\n", "                    error = (pred_y - train_y_sh[idata]) * 2. / len(pred_y)\n", "                    # backward\n", "                    self.backward(train_x_sh[idata], error)\n", "                # update parameters\n", "                self.updateParameters(learning_rate, batch_size)\n", "            \n", "            # history on training data\n", "            self.computeLossAccHistory(train_x, train_y, history, '', iep)\n", "            \n", "            # history on validation data\n", "            if validation_data is not None:\n", "                self.computeLossAccHistory(validation_data[0], validation_data[1], history, 'val_', iep)\n", "                \n", "            # print training info\n", "            if verbose > 0 and (iep % verbose == 0 or iep == epochs - 1):\n", "                print('Epoch %d: ' % iep, end='')\n", "                print('loss = %f; acc = %f' % (history['loss'][iep], history['acc'][iep]), end='')\n", "                if validation_data is not None:\n", "                    print('; val_loss = %f; val_acc = %f; ' % (history['val_loss'][iep], \n", "                                                             history['val_acc'][iep]), end='')\n", "                print('elapsed time = %f' % (time.time() - start_time,))\n", "        if (verbose > 0):\n", "            print('Finished %d epochs, elapsed time = %f' % (epochs, time.time() - start_time))\n", "            \n", "        # return history\n", "        return history\n", "    \n", "    # predict\n", "    def predict(self, pred_x):\n", "        pred_y = []\n", "        for x in zip(pred_x):\n", "            pred_y.append(self.forward(x[0]))\n", "        return np.array(pred_y) \n", "    \n", "    # print\n", "    def __str__(self):\n", "        s = 'Model name: %s\\n' % self.name\n", "        s += 'Input size = %d\\n' % (self.input_size,)\n", "        s += 'Number of layers = %d\\n' % (len(self.layers),)\n", "        s += '========================================\\n'\n", "        for ilayer, layer in enumerate(self.layers):\n", "            s += 'Layer %d: ' % ilayer\n", "            s += str(layer)\n", "            s += '----------------------------------------\\n'\n", "        return s"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Application\n", "\n", "Now we can use our own DNN to classify the `fashion-mnist` dataset as we did in [DNN_basics.ipynb](DNN_basics.ipynb) based on Keras."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow (only for the dataset)\n", "from tensorflow import keras"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 0. Load  the dataset\n", "\n", "Our implementation does not do input flattening and output one-hot encoding internally, so we have to do them manually after loading the data. Also, because our implementation is unoptimised, we will only use 20% of the dataset. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "fashion_mnist = keras.datasets.fashion_mnist\n", "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0\n", "\n", "# flatten images by numpy.ndarray.reshape()\n", "npix = train_images.shape[1]\n", "input_size = npix * npix\n", "train_images = train_images.reshape((train_images.shape[0], input_size))\n", "test_images = test_images.reshape((test_images.shape[0], input_size))\n", "\n", "# one-hot encoding for labels by numpy.eye()\n", "train_labels = np.eye(10)[train_labels]\n", "test_labels = np.eye(10)[test_labels]\n", "\n", "# use 20% of the dataset\n", "train_images = train_images[0:12000]\n", "train_labels = train_labels[0:12000]\n", "test_images = test_images[0:2000]\n", "test_labels = test_labels[0:2000]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Build the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# layers\n", "hidden_layer = Layer(128, activation=relu, name='Hidden')\n", "output_layer = Layer(10, activation=sigmoid, name='Output')\n", "\n", "# model\n", "model = Model(input_size, [hidden_layer, output_layer], name=\"DNN for fashion-mnist\")\n", "\n", "# print summary\n", "print(model)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Train the model\n", "\n", "Here we need a large number of epochs (1000) to obtain an accuracy comparable to what we had done in [DNN_basics.ipynb](DNN_basics.ipynb) with 50 epochs. Given that we only use 20% of the dataset, our Mini-batch Gradient Descent algorithm is still three times less efficient in convergence than the Adam optimiser implemented in Keras. Use a smaller `epochs` (100~200) for a faster but less accurate result."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the model\n", "history = model.train(train_images, train_labels, \n", "                      epochs=200, batch_size=32, learning_rate=.3,\n", "                      validation_data=(test_images, test_labels))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(history['acc'], label='Accuracy on training data')\n", "plt.plot(history['val_acc'], label='Accuracy on test data')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(history['loss'], label='Loss on training data')\n", "plt.plot(history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Make predictions\n", "\n", "After training, we can use `model.predict` to make predictions:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# make predictions\n", "pred_labels = model.predict(test_images)\n", "print('Number of test images: %d' % test_images.shape[0])\n", "print('Number of correct predictions: %d' % \n", "      np.where(np.argmax(pred_labels, axis=1) == np.argmax(test_labels, axis=1))[0].size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "* **Easy**: Add an option to `Model` class to allow for a different method to initialise weights and biases, such as Glorot uniform [original paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). \n", "\n", "* **Medium**: Add the dropout rate as a property of `Layer`. Randomly select neurons based on this rate and drop them out by zeroing their values in both forward and backward propagations. \n", "\n", "* **Hard**: Implement Implement [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) or [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 2}