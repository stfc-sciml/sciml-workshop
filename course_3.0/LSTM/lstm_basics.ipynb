{"cells": [{"cell_type": "markdown", "metadata": {"id": "0hjVNKY5cqPP"}, "source": ["# LSTM: The basics\n", "\n", "In this notebook, we will learn the basics of a Long Short Term Memory (LSTM) based on [Keras](https://keras.io/), a high-level API for building and training deep learning models, running on top of [TensorFlow](https://www.tensorflow.org/), an open source platform for machine learning. \n", "\n", "We will build a few examples of an LSTM\n", "\n", "* First we will build a basic LSTM to predict a stock price in the future. The data is provided in your training environment - but in future you can also access it in [this github repo](https://github.com/mwitiderrick/stockprice).\n", "* We will then build some more complex LSTM structures, which take multiple datasources as input and also stacck up the LSTM layers within the network.\n", "\n", "### Contents\n", "\n", "1. [Converting and preparing data](#convert_data)\n", "2. [A simple uni-variate LSTM](#simple_lstm)\n", "3. [A multi-variate LSTM](#multi_variate)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "GYCCLvh0cqPU"}, "outputs": [], "source": ["# Importing the libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.layers import LSTM\n", "from tensorflow.keras.layers import Dropout\n", "\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {"id": "o_KwpTqbcqPV"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "sLlCYCHFcqPW"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "USgu8B3WcqPX", "outputId": "fc878711-c094-46fa-e151-0158c6d81de2"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "SUKycDDmcqPY"}, "source": ["<a id='convert_data'></a>\n", "## Converting data to sequence structure\n", "\n", "\n", "One of the critical features of $RNNs$ and $LSTMs$ is that they work on sequences of data. In the lecture notes we saw that the network takes input at a time $t$ and the hiden layer state from $t-1$ and produces an output:\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstms-1.png?raw=1\" alt=\"lstms-1\" width=\"500\"/>\n", "\n", "However we may also want to include measured data from further back in time to help with remembering; so we could want input data from $t-1 \\cdots t-n$. We might also have more than one channel of input data, this is called the  **window** of the data. Imagine for example we were predicting stock prices, we could have the history of that stock, but we might also want to know the central bank interest rate, or the strength of one currency relative to another, in this case we have a **multi-variate** problem. So our input data looks more like:\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstms-2.png?raw=1\" alt=\"lstms-2\" width=\"500\"/>\n", "\n", "\n", "Finally, we might also want to make our $LSTM$ predict more than just one step forward, so we will want to be able to have multiple steps in the output.\n", "\n", "We write a function to convert dataframe series into data that is suitable for $LSTM$ training.\n", "\n", "This function is quite flexible and can be useful in many scenarios, so it is one that you might like to reuse in future if you are training time series models.\n", "\n", "As input we pass the data as a list or `numpy` array. We then also specify how far back in time we wish to look for each prediction `n_in`, so `n_in = 1` means we take just $t$ as input, `n_in=2` means we take $t, t-1$ as input and so on. We specify how far into the future we wish to predict, with the `n_out` variable. `n_out=1` means we predict for $t$, `n_out=2` means we predict for $t, t+1$ and so on."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "iaz0Sbv8cqPZ"}, "outputs": [], "source": ["# convert series to supervised learning\n", "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n", "    \"\"\"\n", "    Frame a time series as a supervised learning dataset.\n", "    The function automatically checks if we are dealing with a univariate or a multi-variate problem, by  \n", "    checing the shape and type of `data`.\n", "    Adapted from: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n", "    Arguments:\n", "       data: Sequence of observations as a list or NumPy array.\n", "       n_in: Number of lag observations as input (X).\n", "       n_out: Number of observations as output (y).\n", "       dropnan: Boolean whether or not to drop rows with NaN values.\n", "    Returns:\n", "       Pandas DataFrame of series framed for supervised learning.\n", "    \"\"\"\n", "    n_vars = 1 if type(data) is list else data.shape[1]\n", "    df = pd.DataFrame(data)\n", "    cols, names = list(), list()\n", "    # input sequence (t-n, ... t-1)\n", "    for i in range(n_in, 0, -1):\n", "        cols.append(df.shift(i))\n", "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # forecast sequence (t, t+1, ... t+n)\n", "    for i in range(0, n_out):\n", "        cols.append(df.shift(-i))\n", "        if i == 0:\n", "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n", "        else:\n", "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # put it all together\n", "    agg = pd.concat(cols, axis=1)\n", "    agg.columns = names\n", "    # drop rows with NaN values\n", "    if dropnan:\n", "        agg.dropna(inplace=True)\n", "    return agg"]}, {"cell_type": "markdown", "metadata": {"id": "0MIURIiecqPb"}, "source": ["<a id='simple_lstm'></a>\n", "\n", "## A simple LSTM\n", "\n", "### Importing and treating the data\n", "\n", "We first read in our data and inspect it using `pandas`.\n", "\n", "The next step is to normalise our data - we use the `MinMaxScaler` from `scikit-learn` to do this.\n", "\n", "For an $LSTM$ we then want to pass our data to the network in chunks of time. This is so that the prediction at time $t$ draws on the input data from time $t-n$ all the way to $t$ where $n$ is the size of the chunk or \"window\" of time that we use. This is easy to achieve with a simple `for` loop, but it is a slightly different step in pre-processing that is required for $LSTMs$ or time-series problems compared to other ML tasks. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "2zhnRepbcqPb"}, "outputs": [], "source": ["# Importing the training set\n", "dataset_train = pd.read_csv(data_path + 'lstm-data/data-train-lstm.csv')\n", "training_set = dataset_train.iloc[:, 1:2].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "id": "C57mZoWxcqPc", "outputId": "f9210dc5-f54e-4eab-b03a-eeee6403562d"}, "outputs": [], "source": ["dataset_train.head()"]}, {"cell_type": "markdown", "metadata": {"id": "Lu6MKgKKcqPd"}, "source": ["We are only interested in predicing the `Open` price. So we can drop all other columns.\n", "\n", "We then want to convert the data to a numpy array of type `float32`. \n", "\n", "Next we will transform the data to be in a range $0-1$, using the `MinMaxScaler`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "fitXgmTAcqPd"}, "outputs": [], "source": ["dataset_train.drop(dataset_train.columns[[0, 2, 3, 4, 5, 6, 7]], axis=1, inplace=True)\n", "values = dataset_train.values\n", "# ensure all data is float\n", "values = values.astype('float32')\n", "# normalize features\n", "scaler = MinMaxScaler(feature_range=(0, 1))\n", "scaled = scaler.fit_transform(values)"]}, {"cell_type": "markdown", "metadata": {"id": "ugmmIj_BcqPd"}, "source": ["### Converting to $LSTM$ structure data\n", "\n", "We now want to convert the data to a strucuture that can be fed to the $LSTM$. To do this we use our `series_to_supervised` function. In this case we want to look back 80 steps and project forward one step. How would you do this?\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# frame as supervised learning\n", "reframed = series_to_supervised(scaled, 80, 1)\n", "print(reframed.head())\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6inbjtqpcqPe", "outputId": "febcfa1a-43fe-4e23-c67a-7afd4bc7cd6b"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "XIAU-41fcqPe"}, "source": ["### Convert to numpy arrays\n", "\n", "Again we want to convert from the `pandas` dataframe to a numpy array. We will also plot the data to have a look ."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 462}, "id": "G28uWRx6cqPe", "outputId": "25c1148e-853e-4e36-efd0-795163a6a9c4"}, "outputs": [], "source": ["# Convert the dataframe to numpy arrays\n", "values = reframed.values\n", "X_train = values[:, :-1]\n", "y_train = values[:, -1:]\n", "X_train = np.expand_dims(X_train, axis=2)\n", "y_train = np.squeeze(y_train)\n", "fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n", "plt.plot(X_train[:, 0, 0])\n", "plt.xlabel('Day')\n", "plt.ylabel('Normalised price')"]}, {"cell_type": "markdown", "metadata": {"id": "He68IW0qcqPf"}, "source": ["### Building the network\n", "<a id='build_lstm'></a>\n", "\n", "Note that as before we start off from a `Sequential` type model in `Keras`.\n", "\n", "The $LSTM$ layers are already coded in `Keras` so we do not need to worry about writing the complicated structure. We just need to consider a few hyper-parametes we want to set,\n", "\n", "* Number of LSTM layers - we can stack LSTM layers in this case we will start with 2 stacked LSTMs\n", "* units - this is the dimensionality of the hidden state and memory cell of the LSTM\n", "* return_sequences - should we return the full output sequence or just the final value in the sequence. Generally, if the LSTM layer is feeding to another layer in the network, this would be `True` if the LSTM layer is the final layer then this is `False`, note default is `False`  \n", "\n", "In the network strucure below, we add the second (final) $LSTM$ layer by\n", "\n", "```python\n", "regressor.add(LSTM(units = 50, return_sequences = False))\n", "```"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "wfCnikimcqPf"}, "outputs": [], "source": ["# Initialising the LSTM\n", "regressor = Sequential()\n", "\n", "# Adding the first LSTM layer and some Dropout regularisation\n", "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n", "\n", "##########\n", "# Adding a second LSTM layer and some Dropout regularisation\n", "regressor.add(LSTM(units = 50, return_sequences = False))\n", "##########\n", "\n", "# Adding the output layer\n", "regressor.add(Dense(units = 1))"]}, {"cell_type": "markdown", "metadata": {"id": "-pDxl6sjcqPg"}, "source": ["### Compile the network\n", "\n", "As ususal we need to compile the network, choosing an optimiser and a loss function.\n", "\n", "We use `adam` as our optimiser and `mean_squared_error` as our loss."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KbiDJj3tcqPg"}, "outputs": [], "source": ["# Compiling the RNN\n", "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"]}, {"cell_type": "markdown", "metadata": {"id": "_zdioBcfcqPg"}, "source": ["### Train the network"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0p6_8SYTcqPg", "outputId": "2db8a6a7-fbbf-4da9-b29f-0561f1c16609"}, "outputs": [], "source": ["# Fitting the RNN to the Training set\n", "history = regressor.fit(X_train, y_train, epochs = 30, batch_size = 128)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 462}, "id": "mE-ngYaOcqPh", "outputId": "48908b70-2d3c-44d0-d3a7-8d69ae3ad86f"}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n", "plt.plot(history.history['loss'])\n", "plt.xlabel('epoch')\n", "plt.ylabel('mae')"]}, {"cell_type": "markdown", "metadata": {"id": "-Qegdaf2cqPh"}, "source": ["### Making predictions with our model\n", "\n", "We now use model that we just built to predict on previously un-seen data. We read in `data-test.csv` and get the stock prices from that data.\n", "\n", "Next we reshape that data and convert it into windows of size 60, as before.\n", "\n", "Finally we scale the data using the same scaler that we set up earlier."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "172pVUt4cqPh"}, "outputs": [], "source": ["# Part 3 - Making the predictions and visualising the results\n", "\n", "# Getting the real stock price of 2017\n", "dataset_test = pd.read_csv(data_path + 'lstm-data/data-test-lstm.csv')\n", "real_stock_price = dataset_test.iloc[:, 1:2].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "id": "hSvn4HdrcqPh", "outputId": "a6585f61-6665-453d-8375-0965334e2b4d"}, "outputs": [], "source": ["dataset_test.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "P4a_AFTMcqPh"}, "outputs": [], "source": ["# Getting the predicted stock price of 2017\n", "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n", "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 80:].values\n", "scaled = scaler.transform(inputs.reshape(-1, 1))\n", "test_rf = series_to_supervised(scaled, 80, 1)\n", "values = test_rf.values\n", "X_test = values[:, :-1]\n", "X_test = np.expand_dims(X_test, axis=2)\n", "\n", "predicted_stock_price = regressor.predict(X_test)\n", "predicted_stock_price = scaler.inverse_transform(predicted_stock_price)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 462}, "id": "AcrVDx2-cqPi", "outputId": "d3073c8f-2531-481b-8fb5-f89df94f3412"}, "outputs": [], "source": ["# Visualising the results\n", "fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n", "plt.plot(real_stock_price, label = 'Real TATA Stock Price')\n", "plt.plot(predicted_stock_price, label = 'Predicted TATA Stock Price')\n", "plt.title('TATA Stock Price Prediction')\n", "plt.xlabel('Time')\n", "plt.ylabel('TATA Stock Price')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "F_EpvUpqcqPi"}, "source": ["### Exercise\n", "\n", "* Build a network with three or four $LSTM$ layers. How does this affect the performance?\n", "* Change the size of the data window - how does a longer/shorter window affect the predictions?\n", "\n", "<a id='multi_variate'></a>\n", "\n", "## A more complex LSTM\n", "\n", "This was a nice intro to using $LSTMs$. But in reality the input and output we want can be more complex. What happens if we have multiple factors that can affect our output, or what happens if we want to predict more than one step into the future? Luckily all of this is relatively straightforward to deal with. All we need to do is make sure that we can wrangle the data structres correctly to fit the network.\n", "\n", "\n", "### The Data\n", "\n", "In this case we are going to look at air pollution. We will spend a bit of time working out how to treat the data. In the cell below we deal with the fact that some of the data in the original frame is missing -- we fill in `na` values with zeros usign the convenient `pandas` function `fillna`. We also select to only look at certain columns of the data and also to drop the first 24 hours of data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "yg8imDjmcqPi", "outputId": "6d28ef2a-85dc-45ef-a052-d86bf19367ae"}, "outputs": [], "source": ["from pandas import read_csv\n", "from datetime import datetime\n", "# load data\n", "def parse(x):\n", "    return datetime.strptime(x, '%Y %m %d %H')\n", "dataset = read_csv(data_path + 'lstm-data/pollution.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n", "dataset.drop('No', axis=1, inplace=True)\n", "# manually specify column names\n", "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n", "dataset.index.name = 'date'\n", "# mark all NA values with 0\n", "dataset['pollution'].fillna(0, inplace=True)\n", "# drop the first 24 hours\n", "dataset = dataset[24:]\n", "# summarize first 5 rows\n", "print(dataset.head(5))"]}, {"cell_type": "markdown", "metadata": {"id": "RS-dOnwGcqPj"}, "source": ["#### Look at the data\n", "\n", "Now we want to take a look at the data. We can load up the file we just created and plot the datasets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 701}, "id": "KNxbh0MNcqPj", "outputId": "9583af35-8a3c-4e78-a714-6d3e2b9bf0e3"}, "outputs": [], "source": ["values = dataset.values\n", "# specify columns to plot\n", "groups = [0, 1, 2, 3, 5, 6, 7]\n", "i = 1\n", "# plot each column\n", "plt.figure(figsize=(12, 12))\n", "for group in groups:\n", "    plt.subplot(len(groups), 1, i)\n", "    plt.plot(values[:, group])\n", "    plt.title(dataset.columns[group], y=0.5, loc='right')\n", "    i += 1\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "k-TuygN0cqPj"}, "source": ["#### Encoding the data\n", "\n", "We want to convert the data to formats work with our ML model. A notable case is wind direction `wnd_dir` - we will integer encode this using `scikit-learn`. You could also one-hot encode this if you want to.\n", "\n", "Then we normalise the data using the `MinMaxScaler` as before."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "DcFRkpIhcqPj"}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "\n", "values = dataset.values\n", "# integer encode direction\n", "encoder = LabelEncoder()\n", "values[:,4] = encoder.fit_transform(values[:,4])\n", "# ensure all data is float\n", "values = values.astype('float32')\n", "# normalize features\n", "scaler = MinMaxScaler(feature_range=(0, 1))\n", "scaled = scaler.fit_transform(values)"]}, {"cell_type": "markdown", "metadata": {"id": "Rxpj2iKpcqPj"}, "source": ["#### Converting to LSTM structure"]}, {"cell_type": "markdown", "metadata": {"id": "GhjU9W2ncqPj"}, "source": ["In this case we want to look back just one step and predict forward just one step. How would you achieve this?\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# frame as supervised learning\n", "reframed = series_to_supervised(scaled, 1, 1)\n", "print(reframed.head())\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "vvycsAMCcqPk", "outputId": "6edb1529-54b9-449a-d702-560c60d60db5"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "iuIf9IGXcqPk"}, "source": ["In our case we only want to predict the pollution in the future - this is `var1` so we do not require `var2, var3, etc` at time t. We can simply drop these from the dataframe:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "5Ozbgq7LcqPk", "outputId": "541c15e5-bd57-4b1b-d078-ed38780a7546"}, "outputs": [], "source": ["# drop columns we don't want to predict\n", "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n", "print(reframed.head())"]}, {"cell_type": "markdown", "metadata": {"id": "X2Lm8dqDcqPk"}, "source": ["### Split into training and test sets. \n", "\n", "In order to make this trianing run in a reasonable time we will take only the first year's worth of data as training data - this is $365*24$ hours of data. We then reshape the datasets so that they are in the shape expected for the $LSTM$ model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6aPkuCmPcqPk", "outputId": "7fb6b3b1-deaf-47bc-a9bf-2c9a223a60c9"}, "outputs": [], "source": ["# split into train and test sets\n", "values = reframed.values\n", "n_train_hours = 365 * 24\n", "train = values[:n_train_hours, :]\n", "test = values[n_train_hours:, :]\n", "# split into input and outputs\n", "train_X, train_y = train[:, :-1], train[:, -1]\n", "test_X, test_y = test[:, :-1], test[:, -1]\n", "# reshape input to be 3D [samples, timesteps, features]\n", "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n", "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n", "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"]}, {"cell_type": "markdown", "metadata": {"id": "3Vgs7P3vcqPk"}, "source": ["### Build the network\n", "\n", "We now build the model $LSTM$ - we will build a simple model.\n", "\n", "Can you adapt the code from [the build lstm section](#build_lstm) to make a single layer $LSTM$, with 50 units in the $LSTM$ and an `input_shape=(train_X.shape[1], train_X.shape[2])`?\n", "\n", "If this is not working for you you can reveal the suggested answer below:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model = Sequential()\n", "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n", "model.add(Dense(1))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tHqeLNyPcqPk"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "2XcKX6TBcqPl"}, "source": ["### Compile and fit the network\n", "\n", "As ever we have to now compile and train the model. We use the `adam` optimiser and mean absolute error as our loss function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "o_alwOxXcqPl", "outputId": "37679ba3-e29c-4e72-d8e2-6626d1f840ae"}, "outputs": [], "source": ["model.compile(loss='mae', optimizer='adam')\n", "history = model.fit(train_X, train_y, epochs=50, \n", "                    batch_size=72, validation_data=(test_X, test_y), \n", "                    shuffle=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 267}, "id": "MDdupjXRcqPl", "outputId": "e0b928c2-1588-4068-ca0a-303f35fb3075"}, "outputs": [], "source": ["plt.plot(history.history['loss'], label='train')\n", "plt.plot(history.history['val_loss'], label='test')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "tpT3ps9lcqPl", "outputId": "65d111ba-8a30-4899-cd65-e03b00e0fe19"}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error\n", "yhat = model.predict(test_X)\n", "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n", "# invert scaling for forecast\n", "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n", "inv_yhat = scaler.inverse_transform(inv_yhat)\n", "inv_yhat = inv_yhat[:,0]\n", "# invert scaling for actual\n", "test_y = test_y.reshape((len(test_y), 1))\n", "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n", "inv_y = scaler.inverse_transform(inv_y)\n", "inv_y = inv_y[:,0]\n", "# calculate RMSE\n", "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n", "print('Test RMSE: %.3f' % rmse)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 391}, "id": "Na2ZtBFQcqPl", "outputId": "0449ddae-1952-4d75-cace-5a3ca1d218b1"}, "outputs": [], "source": ["start_time = 1200\n", "stop_time = 1500\n", "plt.subplots(1, 1, figsize=(8, 6))\n", "test_y.shape\n", "plt.plot(yhat[start_time+1:stop_time+1, 0], label=\"Prediction\")\n", "plt.plot(test_y[start_time:stop_time, 0], label=\"True\")\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {"id": "-sd_fSyjcqPl"}, "source": ["### Exercise\n", "\n", "Try using a longer time frame of previous steps for training the model - how does that affect performance?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eggk9Xo0cqPm", "outputId": "8691b823-c840-44c3-cbbf-e93342ca1254"}, "outputs": [], "source": ["# frame as supervised learning\n", "reframed = series_to_supervised(scaled, 2, 1)\n", "print(reframed.head())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "gInuOqRjcqPm", "outputId": "3a5b9dc3-ba69-4a93-cad2-0478634d513b"}, "outputs": [], "source": ["# drop columns we don't want to predict\n", "reframed.drop(reframed.columns[[-7,-6,-5,-4,-3,-2,-1]], axis=1, inplace=True)\n", "print(reframed.head())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4ffRz2eVcqPm", "outputId": "586bdf7f-7a38-4a39-de5d-8c2e5667529d"}, "outputs": [], "source": ["# split into train and test sets\n", "values = reframed.values\n", "n_train_hours = 365 * 24\n", "train = values[:n_train_hours, :]\n", "test = values[n_train_hours:, :]\n", "# split into input and outputs\n", "train_X, train_y = train[:, :-1], train[:, -1]\n", "test_X, test_y = test[:, :-1], test[:, -1]\n", "# reshape input to be 3D [samples, timesteps, features]\n", "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n", "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n", "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "GqyhIbjfcqPm"}, "outputs": [], "source": ["# design network\n", "model = Sequential()\n", "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n", "model.add(Dense(1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "jjKiya3JcqPm", "outputId": "b25f2f65-503a-448a-e546-8586d4132347"}, "outputs": [], "source": ["model.compile(loss='mae', optimizer='adam')\n", "history = model.fit(train_X, train_y, epochs=50, \n", "                    batch_size=72, validation_data=(test_X, test_y), \n", "                    shuffle=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Eo6Puc5tcqPm"}, "outputs": [], "source": []}], "metadata": {"colab": {"name": "lstm_basics.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}