{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## LSTM Practical: Predicting beam profile in synchrotron beam\n", "\n", "In this practical we will attempt to build a time series model that can predict the profile of the beam, specifically the RMS of the x-coordinate profile.\n", "\n", "This is a multivariate problem, where we have several parameters as inputs. We have the horizontal and vertical control of the injector that control the incoming beam. We also have the time that the acceleration has been running for and finally we have an estimate of the number of the profile.\n", "\n", "The aim of this work is to take the results of the low cost simulation and using the LSTM make them match more closely to the high cost accurate simulation - which is your ground truth Y values in this example. To give a sense of the sppedup, the low cost simulations take about 1 second to run, the ground truth simulations take about 16 hours each.\n", "\n", "The data for this practical is plotted below. On the left you can see the vertical and horizontal injector control paramaters. On the right you can see the X$_{RMS}$ from the low cost model and those from the full physics simulation.\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstm-practical-data.png?raw=1\" alt=\"lstm-practical-data\" width=\"900\"/>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Importing the libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.layers import LSTM\n", "from tensorflow.keras.layers import Dropout\n", "\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The dataset\n", "\n", "The data has been preprocessed a bit - so the values are all normalised to between 0 and 1. Use `pandas` to read the csv `<data_path>/lstm-data/injector-data.csv`\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "data = pd.read_csv(data_path + '/lstm-data/injector-data.csv')\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Our data processing for LSTM function\n", "\n", "This is the same fuction that we used in the lecture notebook and will allow us to play with different options for the data. You can copy and paste the function from [lstm_basics.ipynb](lstm_basics.ipynb)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# convert series to supervised learning\n", "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n", "    \"\"\"\n", "    Frame a time series as a supervised learning dataset.\n", "    The function automatically checks if we are dealing with a univariate or a multi-variate problem, by  \n", "    checing the shape and type of `data`.\n", "    Adapted from: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n", "    Arguments:\n", "       data: Sequence of observations as a list or NumPy array.\n", "       n_in: Number of lag observations as input (X).\n", "       n_out: Number of observations as output (y).\n", "       dropnan: Boolean whether or not to drop rows with NaN values.\n", "    Returns:\n", "       Pandas DataFrame of series framed for supervised learning.\n", "    \"\"\"\n", "    n_vars = 1 if type(data) is list else data.shape[1]\n", "    df = pd.DataFrame(data)\n", "    cols, names = list(), list()\n", "    # input sequence (t-n, ... t-1)\n", "    for i in range(n_in, 0, -1):\n", "        cols.append(df.shift(i))\n", "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # forecast sequence (t, t+1, ... t+n)\n", "    for i in range(0, n_out):\n", "        cols.append(df.shift(-i))\n", "        if i == 0:\n", "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n", "        else:\n", "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # put it all together\n", "    agg = pd.concat(cols, axis=1)\n", "    agg.columns = names\n", "    # drop rows with NaN values\n", "    if dropnan:\n", "        agg.dropna(inplace=True)\n", "    return agg"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Split into x and y values\n", "\n", "In this example the coluumns 1-4 of the data are the input variables and column 5 is the value we wish to predict. In the first instance we will just use the first 100000 data points to train the model. Limiting the data this way results in over-fitting, **but** it will allow us to get the model working and training in a reasonable time.\n", "\n", "When we are happy that the model is actually training and learning (albeit over-fitting) we can return and add more data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_limit = 100000\n", "values = data.values\n", "xvalues = values[:data_limit, 1:5]\n", "yvalues = values[:data_limit, 5]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Refreame the training data\n", "\n", "Use the `series_to_supervised` function to convert the `xvalues` to a datframe called `reframedx` which has a past window of 0 steps into the past and a future prediction window of 1 step.\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "reframedx = series_to_supervised(xvalues, 0, 1)\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Convert to arrays for training\n", "\n", "We now reshape the `reframedx` to work with the netword. **Note** we also need to remove the first $n$ values from the y data, where $n$ is the size of the window defined above. When you are tyring different window sizes in future, don't forget to alter this value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["past_steps = 0\n", "x_train = reframedx.values\n", "x_train = x_train.reshape((x_train.shape[0], 4, x_train.shape[1]//4))\n", "ytrain = yvalues[past_steps:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build the network\n", "\n", "We will build an $LSTM$ using the template of the network from [lstm_basics.ipynb](lstm_basics.ipynb). In the first instance try building a network with one $LSTM$ layer, with 50 units in the layer. The `input_shape` of the network should be `(x_train.shape[1], x_train.shape[2])`.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# Initialising the LSTM\n", "model = Sequential()\n", "\n", "# Adding the first LSTM layer and some Dropout regularisation\n", "model.add(LSTM(units = 50, return_sequences = False, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "\n", "# Adding the output layer\n", "model.add(Dense(units = 1))\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compile and fit the network\n", "\n", "Compile the network to use `mae` as the loss and `adam` as the optimiser.\n", "\n", "For training initially run for 30 epochs with a batch size of 128 and a validation split of 0.2. Set `shuffle` to `False` for the fitting.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "\n", "model.compile(loss='mae', optimizer='adam')\n", "history = model.fit(x_train, ytrain, epochs=30, \n", "                    batch_size=128, validation_split=0.2, \n", "                    shuffle=False)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Take a look at some results\n", "\n", "We can look at how the model peroforms on the training set and on an independent test set.\n", "\n", "### Training set\n", "\n", "First see how we are doing on the training data - if you cannot fit the training data there is no hope on anything else. \n", "By chaniging the value of `index` below we can choose different samples from the training set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index = 1\n", "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n", "preds = model.predict(x_train[(index-1)*786+100:index*786])\n", "plt.plot(preds[:, 0], label='Prediction')\n", "plt.plot(ytrain[(index-1)*786:index*786], label='True')\n", "\n", "plt.legend()\n", "plt.xlim(0, 660)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Independent test set\n", "\n", "If the model worked okay on the training data a true test is on independent data that was not used for training.\n", "\n", "**Note** you will have to alter window size here to match your training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["window_size = 0\n", "values = data.values\n", "xtestvalues = values[370913:, 1:5]\n", "ytestvalues = values[370913:, 5]\n", "reframedxtest = series_to_supervised(xtestvalues, window_size, 1)\n", "x_test = reframedxtest.values\n", "x_test = x_test.reshape((x_test.shape[0], 4, x_test.shape[1]//4))\n", "ytest = ytestvalues[window_size:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index = 10\n", "preds = model.predict(x_test[(index-1)*786:index*786])\n", "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n", "plt.plot(preds[100:, 0], label='Prediction')\n", "plt.plot(ytest[(index-1)*786:index*786], label='True')\n", "plt.legend()\n", "plt.xlim(100, 680)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hyper-parameter tuning\n", "\n", "The network is running, but the results are not very good. Let's try doing some hyper-parameter tuning. \n", "\n", "Here are some things to try:\n", "\n", "* Different window sizes\n", "    - [0, 1, 2, 5, 10]\n", "    - Use just 30 epochs for each of these\n", "    \n", "* Increase the number of LSTM layers\n", "    - 1, 2, 3 layers of lstm\n", "    - Use just 30 epochs for each of there\n", "    \n", "When you have found the best options - try to increase the training dataset to 400000 and leave the model to train for 1000 epochs. This will probably take quite a long time, so you might want to leave this running overnight.\n", "\n", "**Sample code for hyper-parameter tuning: window size** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "import tqdm as tqdm\n", "steps = [0, 1, 2, 5, 10]\n", "histories = []\n", "for step in tqdm.tqdm(steps):\n", "    reframedx = series_to_supervised(xvalues, step, 1)\n", "    past_step = step\n", "    x_train = reframedx.values\n", "    x_train = x_train.reshape((x_train.shape[0], 4, x_train.shape[1]//4))\n", "    ytrain = yvalues[past_step:]\n", "    # Initialising the LSTM\n", "    model = Sequential()\n", "\n", "    # Adding the first LSTM layer and some Dropout regularisation\n", "    model.add(LSTM(units = 50, return_sequences = False, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "\n", "    model.add(Dense(units = 1))\n", "    model.compile(loss='mae', optimizer='adam')\n", "    history = model.fit(x_train, ytrain, epochs=30, \n", "                    batch_size=128, validation_split=0.2, \n", "                    shuffle=False, verbose=0)\n", "    histories.append(history)\n", "    \n", "fig, ax, = plt.subplots(1, 1, figsize=(7, 7))\n", "for i in range(5):\n", "    plt.plot(histories[i].history['val_loss'], label='%s'%i, lw=2)\n", "plt.legend()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Sample code for hyper-parameter tuning: number of LSTMs** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "lstms = [1, 2, 3]\n", "step = 5\n", "histories_lstm = []\n", "for lstm in tqdm.tqdm(lstms):\n", "    reframedx = series_to_supervised(xvalues, step, 1)\n", "    past_step = step\n", "    x_train = reframedx.values\n", "    x_train = x_train.reshape((x_train.shape[0], 4, x_train.shape[1]//4))\n", "    ytrain = yvalues[past_step:]\n", "    # Initialising the LSTM\n", "    model = Sequential()\n", "    if lstm == 1:\n", "    # Adding the first LSTM layer and some Dropout regularisation\n", "        model.add(LSTM(units = 50, return_sequences = False, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "    elif lstm == 2:\n", "        model.add(LSTM(units = 50, return_sequences = True, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "        model.add(LSTM(units = 50, return_sequences = False))\n", "    elif lstm == 3:\n", "        model.add(LSTM(units = 50, return_sequences = True, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "        model.add(LSTM(units = 50, return_sequences = True))\n", "        model.add(LSTM(units = 50, return_sequences = False))\n", "        \n", "        \n", "    model.add(Dense(units = 1))\n", "    model.compile(loss='mae', optimizer='adam')\n", "    history = model.fit(x_train, ytrain, epochs=30, \n", "                    batch_size=128, validation_split=0.2, \n", "                    shuffle=False, verbose=0)\n", "    histories_lstm.append(history)    \n", "    \n", "fig, ax, = plt.subplots(1, 1, figsize=(7, 7))\n", "for i in range(3):\n", "    plt.plot(histories_lstm[i].history['val_loss'], label='%s'%i, lw=2)\n", "plt.legend()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}