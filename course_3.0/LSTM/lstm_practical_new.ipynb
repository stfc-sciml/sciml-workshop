{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## LSTM Practical: Predicting beam profile in synchrotron beam\n", "\n", "In this practical we will attempt to build a time series model that can predict the profile of the beam, specifically the RMS of the x-coordinate profile.\n", "\n", "This is a multivariate problem, where we have several parameters as inputs. We have the horizontal and vertical control of the injector that control the incoming beam. We also have the time that the acceleration has been running for and finally we have an estimate of the number of the profile.\n", "\n", "The aim of this work is to take the results of the low cost simulation and using the LSTM make them match more closely to the high cost accurate simulation - which is your ground truth Y values in this example. To give a sense of the sppedup, the low cost simulations take about 1 second to run, the ground truth simulations take about 16 hours each.\n", "\n", "The data for this practical is plotted below. On the left you can see the vertical and horizontal injector control paramaters. On the right you can see the X$_{RMS}$ from the low cost model and those from the full physics simulation.\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstm-practical-data.png?raw=1\" alt=\"lstm-practical-data\" width=\"900\"/>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Importing the libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.layers import LSTM\n", "from tensorflow.keras.layers import Dropout\n", "\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The dataset\n", "\n", "The data has been preprocessed a bit - so the values are all normalised to between 0 and 1. Use `pandas` to read the csv `<data_path>/lstm-data/injector-data.csv`\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "data = pd.read_csv(data_path + '/lstm-data/injector-data.csv')\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Our data processing for LSTM function\n", "\n", "This is the same fuction that we used in the lecture notebook and will allow us to play with different options for the data. You can copy and paste the function from [lstm_basics.ipynb](lstm_basics.ipynb)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def series_to_tensorflow(data, timesteps=10, n_in=1, n_out=1, feature_indices=None):\n", "    \"\"\"\n", "    Convert a series to tensorflow input.\n", "    Arguments:\n", "       data: Sequence of observations as a 2D NumPy array of shape (n_times, n_features)\n", "       n_in: Window size of input X\n", "       n_out: Window size of output y\n", "       feature_indices: select features by indices; pass None to use all features\n", "       timesteps: timesteps of LSTM\n", "    Returns:\n", "       X and y for tensorflow.keras.layers.LSTM\n", "    \"\"\"\n", "    # sizes\n", "    n_total_times, n_total_features = data.shape[0], data.shape[1]\n", "    n_batches = n_total_times - timesteps - (n_in - 1) - (n_out - 1)\n", "    \n", "    # feature selection\n", "    if feature_indices is None:\n", "        feature_indices = list(range(n_total_features))\n", "    \n", "    # data\n", "    X = np.zeros((n_batches, timesteps, n_in, len(feature_indices)), dtype='float32')\n", "    y = np.zeros((n_batches, n_out, len(feature_indices)), dtype='float32')\n", "    for i_batch in range(n_batches):\n", "        for i_in in range(n_in):\n", "            X_start = i_batch + i_in\n", "            X[i_batch, :, i_in, :] = data[X_start:X_start + timesteps, feature_indices]\n", "        y_start = i_batch + timesteps + n_in - 1\n", "        y[i_batch, :, :] = data[y_start:y_start + n_out, feature_indices]\n", "    \n", "    # flatten the last two dimensions\n", "    return X.reshape(n_batches, timesteps, -1), y.reshape(n_batches, -1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Refreame the training data\n", "\n", "Use the `series_to_tensorflow` function to construct the training data. Here we will set the timesteps to 1000.\n", "\n", "We also set a training data limit at 10000.Limiting the data this way results in some over-fitting, but it will allow us to get the model working and training in a reasonable time.\n", "\n", "We just use a window of one step backwards and one forwards and we are using featres 1, 2, 3 and 4 in the data as input.\n", "\n", "We can use the same function to construct the targets that we are fitting to, in this case we use all of the same settings, but the feature_indices is just feature 5.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# choose timesteps of LSTM\n", "timesteps = 100\n", "\n", "# set data limit for training\n", "data_limit_train = 10000\n", "\n", "# x using columns 1,2,3,4\n", "x_train, _ = series_to_tensorflow(data.values[:data_limit_train], \n", "                                  timesteps=timesteps, n_in=1, n_out=1, \n", "                                  feature_indices=[1, 2, 3, 4])\n", "print(x_train.shape)\n", "\n", "\n", "# y using column 5\n", "_, y_train = series_to_tensorflow(data.values[:data_limit_train], \n", "                                  timesteps=timesteps, n_in=1, n_out=1, \n", "                                  feature_indices=[5])\n", "print(y_train.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build the network\n", "\n", "We will build an $LSTM$ using the template of the network from [lstm_basics.ipynb](lstm_basics.ipynb). In the first instance try building a network with one $LSTM$ layer, with 50 units in the layer. The `input_shape` of the network should be `(x_train.shape[1], x_train.shape[2])`.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# Initialising the LSTM\n", "model = Sequential()\n", "\n", "# Adding the first LSTM layer and some Dropout regularisation\n", "model.add(LSTM(units = 50, return_sequences = False, input_shape=(x_train.shape[1], x_train.shape[2])))\n", "\n", "# Adding the output layer\n", "model.add(Dense(units = 1))\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compile and fit the network\n", "\n", "Compile the network to use `mae` as the loss and `adam` as the optimiser.\n", "\n", "For training initially run for 30 epochs with a batch size of 128 and a validation split of 0.2. Set `shuffle` to `False` for the fitting.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "\n", "model.compile(loss='mae', optimizer='adam')\n", "history = model.fit(x_train, ytrain, epochs=30, \n", "                    batch_size=128, validation_split=0.2, \n", "                    shuffle=False)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Take a look at some results\n", "\n", "We can look at how the model peroforms on the training set and on an independent test set.\n", "\n", "### Training set\n", "\n", "First see how we are doing on the training data - if you cannot fit the training data there is no hope on anything else. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n", "preds = model.predict(x_train[2000:5000])\n", "plt.plot(preds[:, 0], label='Prediction')\n", "plt.plot(y_train[2000:5000], label='True')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Independent test set\n", "\n", "If the model worked okay on the training data a true test is on independent data that was not used for training.\n", "\n", "Use `series_to_tensorflow` as above to create the test inputs and outputs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# set data limit for testing\n", "data_limit_test = 20000\n", "\n", "# x using columns 1,2,3,4\n", "x_test, _ = series_to_tensorflow(data.values[data_limit_train:data_limit_test], \n", "                                  timesteps=timesteps, n_in=1, n_out=1, \n", "                                  feature_indices=[1, 2, 3, 4])\n", "print(x_test.shape)\n", "\n", "\n", "# y using column 5\n", "_, y_test = series_to_tensorflow(data.values[data_limit_train:data_limit_test], \n", "                                  timesteps=timesteps, n_in=1, n_out=1, \n", "                                  feature_indices=[5])\n", "print(y_test.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preds = model.predict(x_test)\n", "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n", "plt.plot(preds, label='Prediction')\n", "plt.plot(y_test, label='True')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hyper-parameter tuning\n", "\n", "The network is running, but the results could be better. Let's try doing some hyper-parameter tuning. \n", "\n", "Here are some things to try:\n", "\n", "* Increase the number of LSTM layers\n", "    - 1, 2, 3 layers of lstm\n", "    - Use just 30 epochs for each of there\n", "    \n", "When you have found the best options - try to increase the training dataset to 400000 and leave the model to train for 1000 epochs. This will probably take quite a long time, so you might want to leave this running overnight.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}