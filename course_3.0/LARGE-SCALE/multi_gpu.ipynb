{"cells": [{"cell_type": "markdown", "id": "cb9d96f6-e310-4c84-88bd-6d716e290b2a", "metadata": {}, "source": ["# Multi-GPU\n", "\n", "In this notebook, we learn how to train a model with multiple devices in TensorFlow.\n", "\n", "In machine learning, there are two types of parallelism:\n", "\n", "**Data parallelism**: A single model is replicated on multiple devices, each processing different batches of data, and the gradients obtained from backprop are merged before updating the model weights. \n", "\n", "**Model parallelism**: Different parts of a single model run on different devices. This is used for very large models (such as LLMs).\n", "\n", "We will consider both in the following sections."]}, {"cell_type": "code", "execution_count": null, "id": "50f6a07a-892f-48ce-b382-7aa1c64687c7", "metadata": {"tags": []}, "outputs": [], "source": ["import tensorflow as tf\n", "import tensorflow.keras as keras"]}, {"cell_type": "markdown", "id": "9fd36181-a948-43e0-8975-4b6133a454a5", "metadata": {"tags": []}, "source": ["## Create virtual GPUs\n", "\n", "In this training course, we may not able to provide multiple physical GPUs to everyone. Therefore, we use virtual or logical GPUs instead. TensorFlow will see these virtual GPUs as logically independent, so the code in the later sections should also work on multiple physical GPUs.\n", "\n", "Now we create 4 virtual GPUs, each taking 1GB memory."]}, {"cell_type": "code", "execution_count": null, "id": "b3aebf48-1151-46ae-a9ff-6cbef47034b0", "metadata": {"tags": []}, "outputs": [], "source": ["# specify number of GPUs\n", "N_GPUS = 4\n", "MEM_GPU = 1024\n", "\n", "physical_gpus = tf.config.list_physical_devices('GPU')\n", "tf.config.set_logical_device_configuration(\n", "        physical_gpus[0],\n", "        [tf.config.LogicalDeviceConfiguration(memory_limit=MEM_GPU)] * N_GPUS)\n", "\n", "logical_gpus = tf.config.list_logical_devices('GPU')"]}, {"cell_type": "code", "execution_count": null, "id": "04278e4d-b150-46c9-8b3a-cc795b247612", "metadata": {"tags": []}, "outputs": [], "source": ["# print the GPUs\n", "print('Physical GPUs:')\n", "for device in physical_gpus:\n", "    print(device)\n", "\n", "print('\\nLogical GPUs:')\n", "for device in logical_gpus:\n", "    print(device)"]}, {"cell_type": "markdown", "id": "026a6975-5204-459b-8983-1a20977ec5d0", "metadata": {}, "source": ["## Data parallelism"]}, {"cell_type": "markdown", "id": "5d9b6243-dc19-4b37-9877-7a9e1bf44b96", "metadata": {}, "source": ["First, we define the functions to create the model and datasets. If you are unsure about any lines, please revisit `DNN/DNN_basics.ipynb`."]}, {"cell_type": "code", "execution_count": null, "id": "bec332dc-7df4-429e-8a92-a22dc4b841d6", "metadata": {"tags": []}, "outputs": [], "source": ["def get_compiled_model():\n", "    # Make a simple 4-layer densely-connected neural network.\n", "    inputs = keras.Input(shape=(784,))\n", "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n", "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n", "    outputs = keras.layers.Dense(10)(x)\n", "    model = keras.Model(inputs, outputs)\n", "    model.compile(\n", "        optimizer=keras.optimizers.Adam(),\n", "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n", "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n", "    )\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "id": "fa3f06e2-559e-474c-9069-beccc9cfde6e", "metadata": {"tags": []}, "outputs": [], "source": ["def get_dataset():\n", "    batch_size = 256\n", "    num_val_samples = 10000\n", "\n", "    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\n", "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n", "\n", "    # Preprocess the data (these are Numpy arrays)\n", "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n", "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n", "    y_train = y_train.astype(\"float32\")\n", "    y_test = y_test.astype(\"float32\")\n", "\n", "    # Reserve num_val_samples samples for validation\n", "    x_val = x_train[-num_val_samples:]\n", "    y_val = y_train[-num_val_samples:]\n", "    x_train = x_train[:-num_val_samples]\n", "    y_train = y_train[:-num_val_samples]\n", "    \n", "    # datasets\n", "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n", "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n", "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n", "    \n", "    # disable auto-share policy for a tensorflow issue. This may be fixed in the future.\n", "    options = tf.data.Options()\n", "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n", "    train_dataset = train_dataset.with_options(options)\n", "    val_dataset = val_dataset.with_options(options)\n", "    test_dataset = test_dataset.with_options(options)\n", "    return train_dataset, val_dataset, test_dataset"]}, {"cell_type": "markdown", "id": "98009533-4b02-465d-beec-6f0ecfa1d159", "metadata": {}, "source": ["Next, from `tf.distribute`, we create a strategy for parallelism. Here we use `MirroredStrategy`, which is for synchronous training across multiple replicas on one machine. For distributed training on multiple machines, one needs `MultiWorkerMirroredStrategy` and select devices from the machines.\n"]}, {"cell_type": "code", "execution_count": null, "id": "cafb9337-b72b-44cc-be6d-c8235611d9f1", "metadata": {"tags": []}, "outputs": [], "source": ["# devices=None will use all avialable GPUs; \n", "# devices=['GPU:0', 'GPU:1'] will use two GPUs\n", "strategy = tf.distribute.MirroredStrategy(devices=None)\n", "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"]}, {"cell_type": "markdown", "id": "d16f6ae4-620e-4981-a9c9-124dfa942f77", "metadata": {}, "source": ["Now we create the model within the scope of the `MirroredStrategy`. Note that **this is the only difference from the single GPU case**. "]}, {"cell_type": "code", "execution_count": null, "id": "cb2e1b13-2f8b-4229-a562-16e74bb22161", "metadata": {"tags": []}, "outputs": [], "source": ["with strategy.scope():\n", "    model = get_compiled_model()"]}, {"cell_type": "markdown", "id": "70fe010f-ef34-4880-9c1b-f55d4953c4c8", "metadata": {}, "source": ["Now start training. "]}, {"cell_type": "code", "execution_count": null, "id": "1e47ac10-e199-4718-8cd5-1770bb953c26", "metadata": {"tags": []}, "outputs": [], "source": ["# Train the model on all available devices.\n", "train_dataset, val_dataset, test_dataset = get_dataset()\n", "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n", "\n", "# Test the model on all available devices.\n", "model.evaluate(test_dataset)"]}, {"cell_type": "markdown", "id": "597df6b2-2540-46e6-9d45-91b09b0874cf", "metadata": {"tags": []}, "source": ["## Model parallelism\n", "\n", "In this section, we demonstrate model parallelism by manually assigning each layer a different device. "]}, {"cell_type": "code", "execution_count": null, "id": "d63f66b7-fae0-4463-a32a-5b6c5491ac7a", "metadata": {"tags": []}, "outputs": [], "source": ["# input and first hidden on GPU:0\n", "with tf.device(\"GPU:0\"):\n", "    inputs = keras.Input(shape=(784,))\n", "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n", "# second hidden on GPU:1\n", "with tf.device(\"GPU:1\"):\n", "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n", "# third hidden on GPU:2\n", "with tf.device(\"GPU:2\"):\n", "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n", "# output on GPU:3\n", "with tf.device(\"GPU:3\"):\n", "    outputs = keras.layers.Dense(10)(x)\n", "model = keras.Model(inputs, outputs)\n", "model.compile(\n", "        optimizer=keras.optimizers.Adam(),\n", "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n", "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n", "    )"]}, {"cell_type": "markdown", "id": "9df00c87-ea68-4202-b745-9d61eebef7fc", "metadata": {}, "source": ["Now start training:"]}, {"cell_type": "code", "execution_count": null, "id": "ec14efa9-c138-46ec-9dec-2218c773480a", "metadata": {"tags": []}, "outputs": [], "source": ["# Train the model on all available devices.\n", "train_dataset, val_dataset, test_dataset = get_dataset()\n", "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n", "\n", "# Test the model on all available devices.\n", "model.evaluate(test_dataset)"]}, {"cell_type": "markdown", "id": "26318644-eff6-4250-afac-6a99530a828d", "metadata": {}, "source": ["Final notes:\n", "1. On multiple machines, the GPUs can be selected with full paths with machine names. \n", "2. Overheads for data transfer between physical GPUs can be greatly reduced by new communication hardware technologies such as NVLink and NVSwitch.\n", "3. Model distribution is more common in LLMs. Luckily, `HuggingFace` provides automatic device mapping based on layer sizes and available devices, so one usually does not need to configure model distribution manually."]}, {"cell_type": "code", "execution_count": null, "id": "31ffef30-191e-4a0b-9227-3a7618175ad7", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.10"}}, "nbformat": 4, "nbformat_minor": 5}