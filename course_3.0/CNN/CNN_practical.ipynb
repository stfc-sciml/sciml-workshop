{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# CNN Practical: Inelastic Neutron Scattering\n", "\n", "Inelastic neutron scattering (INS) can be used to infer information about the forces present in a material. Neutrons scatter off a sample, exchanging energy with certain fundamental vibrational modes of the sample. These vibraional modes include phonons (interatomic boding forces) and magnons (spin coupling between magnetic nuclei). \n", "\n", "[Johnstone et al. (2012)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.109.237202) have simulated magnon spectra from a double perovskite systems, where INS was used to distinguish between two possible magnetic Hamiltonians of the system. For this practical, we have simulated datasets for each of the possible Hamiltonians. We are going to train a CNN to classify the system correctly."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import h5py\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The dataset\n", "\n", "We have already split the data into training and validation sets and saved them into two HDF5 files, `ins-data/train.h5` and `ins-data/test.h5`, containing respectively 20,000 and 6,676 INS images and their one-hot encoded labels identifying an image as either being of the *Dimer* or *Goodenough* Hamiltonian. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The `tf.data.Dataset` class\n", "The number of images is so large that we may not be able to simultaneously load the whole dataset into memory on a small machine. To solve this issue, we will use [tensorflow.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to create an interface pointing to the files, which can load the data from disk on the fly when they are actually required."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# define image size\n", "IMG_HEIGHT = 20\n", "IMG_WIDTH = 200\n", "N_CHANNELS = 1\n", "N_CLASSES = 2\n", "\n", "# generator\n", "def hdf5_generator(path, buffer_size=32):\n", "    \"\"\" Load data INS data from disk\n", "    \n", "    Args:\n", "        path: path of the HDF5 file on disk\n", "        buffer_size: number of images to read from disk\n", "    \"\"\"\n", "    with h5py.File(path, 'r') as handle:\n", "        n_samples, h, w, c = handle['images'].shape\n", "        for i in range(0, n_samples, buffer_size):\n", "            images = handle['images'][i:i+buffer_size, ..., :1]\n", "            labels = handle['labels'][i:i+buffer_size]\n", "            yield images, labels\n", "\n", "# training data\n", "train_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=data_path + 'ins-data/train.h5'), \n", "                                               output_types=(tf.float32, tf.float32),\n", "                                               output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n", "                                                              (None, N_CLASSES,)))\n", "\n", "# test data\n", "test_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=data_path + 'ins-data/test.h5'), \n", "                                              output_types=(tf.float32, tf.float32),\n", "                                              output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n", "                                                             (None, N_CLASSES,)))\n", "# print\n", "print(train_dataset)\n", "print(test_dataset)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Load and use data\n", "\n", "In the following cell, we will load the first buffer (with 32 data by default) to memory and plot some images and labels from it:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load the first buffer (with 32 data by default)\n", "images, labels = list(test_dataset.take(1))[0]\n", "\n", "# plot some images and labels from it\n", "nplot = 10\n", "fig, axes = plt.subplots(nplot // 2, 2, figsize=(16, nplot / 1.5), dpi=100)\n", "for ax, image, label in zip(axes.flatten(), images, labels):\n", "    ax.matshow(np.squeeze(image))\n", "    ax.set_xlabel('0: Dimer' if label[0] < .5 else '1: Goodenough', c='k')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Classification by CNN\n", "\n", "The task is to build and train a CNN to solve this binary classification problem. \n", "\n", "## 1. Network architecture\n", "\n", "First, design the network architecture. Note that the image height is quite small, so we need to preserve the image size by passing `padding='same'` to `Conv2D`. A suggested architecture is provided:\n", "\n", "- Conv2D\n", " - 8 filters\n", " - kernel size 5$\\times$5\n", " - ReLU activation\n", "- MaxPool2D \n", "- BatchNormalization\n", "- Conv2D \n", " - 16 filters\n", " - kernel size 3$\\times$3\n", " - ReLU activation\n", "- MaxPool2D\n", "- BatchNormalization\n", "- Conv2D \n", " - 16 filters\n", " - kernel size 3$\\times$3\n", " - ReLU activation\n", "- MaxPool2D\n", "- BatchNormalization\n", "- Flatten \n", "- Dense\n", " - 16 units\n", " - ReLU activation\n", "- Dense\n", " - 8 units\n", " - ReLU activation\n", "- Dense\n", " - N classes units\n", " - sigmoid activation\n", "\n", "\n", "     \n", "     \n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# network architecture\n", "model = Sequential()\n", "model.add(Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same',\n", "                 input_shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)))\n", "model.add(MaxPool2D(pool_size=(2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'))\n", "model.add(MaxPool2D(pool_size=(2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'))\n", "model.add(MaxPool2D(pool_size=(2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Flatten())\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(8, activation='relu'))\n", "model.add(Dense(N_CLASSES, activation='sigmoid'))\n", "\n", "# print summary\n", "model.summary()\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.  Compile the model\n", "\n", "We can add the following metrics to see how the network performs for the two classes:\n", "\n", "* `TruePositives`: number of right predictions for Dimer\n", "* `FalsePositives`: number of wrong predictions for Dimer\n", "* `TrueNegatives`: number of right predictions for Goodenough\n", "* `FalseNegatives`: number of wrong predictions for Goodenough\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# compile the model\n", "model.compile(optimizer='adam', loss='binary_crossentropy', \n", "              metrics= ['accuracy',\n", "                        keras.metrics.TruePositives(name='true_positives'),\n", "                        keras.metrics.FalsePositives(name='false_positives'),\n", "                        keras.metrics.TrueNegatives(name='true_negatives'),\n", "                        keras.metrics.FalseNegatives(name='false_negatives')])\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Train the model\n", "\n", "Using the suggested architecture, we can achieve an accuracy greater than 95% with 2 epochs. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# train the model\n", "training_history = model.fit(train_dataset, validation_data=test_dataset, \n", "                             epochs=2, batch_size=32)\n", "    \n", "# print final values of metrics for validation data\n", "print('Right for Tetragonal: %d' % training_history.history['val_true_positives'][-1])\n", "print('Wrong for Tetragonal: %d' % training_history.history['val_false_positives'][-1])\n", "print('Right for Monoclinic: %d' % training_history.history['val_true_negatives'][-1])\n", "print('Wrong for Monoclinic: %d' % training_history.history['val_false_negatives'][-1])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. More exercises\n", "\n", "If you have completed all of these exercises, you may like to return to the [CNN_basics.ipynb](CNN_basics.ipynb) notebook and attempt some of the exercises there."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 2}