{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Application: Denoising Graphene EM Images\n",
    "\n",
    "\n",
    "Electron microscopy (EM) images of graphene can be used to constrain the lifetime and evolution of structural defects. However, exposing them to the EM beam will induce their decay and thus interfere with any conclusions made. To prevent such interference, the samples can be imaged at lower doses, but doing so will also decrease the signal-to-noise ratio (SNR). We wish to reduce the noise while preserving the underlying atomic structure in the image.\n",
    "\n",
    "In this notebook, we will train an autoencoder that transforms a noisy EM image of graphene to a clean one. Different from [05_autoencoder_basics.ipynb](05_autoencoder_basics.ipynb), the input and output images will be different in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Concatenate, Conv2DTranspose\n",
    "\n",
    "# check version\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n",
    "\n",
    "# helpers\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Boilerplate\n",
    "\n",
    "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. To access the data, you need to:\n",
    "\n",
    "1. Run the first cell;\n",
    "2. Follow the link when prompted (you may be asked to log in with your Google account);\n",
    "3. Copy the Google SDK token back into the prompt and press `Enter`;\n",
    "4. Run the second cell and wait until the data folder appears.\n",
    "\n",
    "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will take no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables passed to bash; do not change\n",
    "project_id = 'sciml-workshop'\n",
    "bucket_name = 'sciml-workshop'\n",
    "colab_data_path = '/content/sciml-workshop-data'\n",
    "\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    google_colab_env = 'true'\n",
    "    data_path = colab_data_path\n",
    "except:\n",
    "    google_colab_env = 'false'\n",
    "    ###################################################\n",
    "    ######## specify your local data path here ########\n",
    "    ###################################################\n",
    "    data_path = './sciml-workshop-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n",
    "\n",
    "# running locally\n",
    "if ! $1; then\n",
    "    echo \"Running notebook locally.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# already mounted\n",
    "if [ -d $2 ]; then\n",
    "    echo \"Data already mounted.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# mount the bucket\n",
    "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "apt -qq update\n",
    "apt -qq install gcsfuse\n",
    "gcloud config set project $3\n",
    "mkdir $2\n",
    "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "\n",
    "Similar to [04_CNN_practical.ipynb](04_CNN_practical.ipynb), we open the data file using the `tf.data.Dataset` class, which supports on-demand data loading from disk to memory. The dataset contains two sets of images, respectively named \"noise\" and \"clean\". There are totally 10,000 pairs of noisy and clean images in the file.\n",
    "\n",
    "### Open data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "N_CHANNELS = 1\n",
    "\n",
    "# generator\n",
    "def hdf_generator(path, buffer_size=128):\n",
    "    \"\"\" Load data DMS data from disk\n",
    "    \n",
    "    Args:\n",
    "        path: path of the HDF file on disk\n",
    "        buffer_size: number of images to read from disk\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as handle:\n",
    "        n_samples, h, w, c = handle['noise'].shape\n",
    "        for i in range(0, n_samples, buffer_size):\n",
    "            noise = handle['noise'][i:i+buffer_size]\n",
    "            clean = handle['clean'][i:i+buffer_size]\n",
    "            yield noise, clean\n",
    "\n",
    "# dataset\n",
    "dataset_file = tf.data.Dataset.from_generator(lambda: hdf_generator(path=data_path + '/graphene/train.h5'), \n",
    "                                              output_types=(tf.float32, tf.float32),\n",
    "                                              output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n",
    "                                                             (None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)))\n",
    "print(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset\n",
    "\n",
    "Next, we use the methods of `tf.data.Dataset` to process the input data using the following recipe:\n",
    "\n",
    "1. Unbatch the dataset;\n",
    "2. Shuffle the dataset with a buffer size of 500;\n",
    "3. Extract a subset with 1000 pairs of images (only to save training time for this course);\n",
    "4. Normalise the pixel values to [-1, 1] using the following formula:\n",
    "> $x_\\text{norm}=2 \\dfrac{x -  \\min(x)}{\\max(x) - \\min(x)} - 1;$\n",
    "5. Decimate the images, e.g., from 256$\\times$256 to 64$\\times$64 (only to save training time for this course);\n",
    "6. Re-batch the dataset to size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbatch\n",
    "dataset = dataset_file.unbatch()\n",
    "\n",
    "# shuffle\n",
    "dataset = dataset.shuffle(500)\n",
    "\n",
    "# subset\n",
    "dataset = dataset.take(1000)\n",
    "\n",
    "# normalize\n",
    "def normalize(X, Y):\n",
    "    X = 2 * (X - tf.reduce_min(X)) / (tf.math.reduce_max(X) - tf.math.reduce_min(X)) - 1\n",
    "    Y = 2 * (Y - tf.reduce_min(Y)) / (tf.math.reduce_max(Y) - tf.math.reduce_min(Y)) - 1\n",
    "    return X, Y\n",
    "dataset = dataset.map(normalize)\n",
    "\n",
    "# decimate\n",
    "########################################\n",
    "# use N_DECIMATE = 1 to train with the #\n",
    "# original images of shape (256, 256)  #\n",
    "########################################\n",
    "N_DECIMATE = 4\n",
    "def decimate(X, Y, n):\n",
    "    X = X[::n, ::n, :]\n",
    "    Y = Y[::n, ::n, :]\n",
    "    return X, Y\n",
    "dataset = dataset.map(lambda X, Y: decimate(X, Y, N_DECIMATE))\n",
    "\n",
    "# re-batch\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples\n",
    "\n",
    "Now we can plot a could of examples. The noisy images displayed in the top row will be the input for our autoencoder, and the corresponding clean ones in the bottom row the output for our autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first buffer\n",
    "noisy, clean = list(dataset.take(1))[0]\n",
    "\n",
    "# plot 10 pairs\n",
    "nplot = 10\n",
    "fig, axes = plt.subplots(2, nplot, figsize=(nplot * 2, 4))\n",
    "for ax, img_noise, img_clean in zip(axes.T, noisy[:nplot], clean[:nplot]):\n",
    "    ax[0].matshow(np.squeeze(img_noise))\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].matshow(np.squeeze(img_clean))\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])   \n",
    "axes[0, 0].set_ylabel('Noisy', c='k')\n",
    "axes[1, 0].set_ylabel('Clean', c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising by Autoencoder\n",
    "\n",
    "## 1. Network architecture\n",
    "\n",
    "We provide the following suggested network architecture, using multiple levels of convolutional layers for encoding and decoding. \n",
    "\n",
    "In this network, we employ a technique called **skip connections**, as implemented by the variable `skip_layers`. Here \"skip\" means that the information will be teleported from the encoder to the decoder, skipping the bottleneck. The skip connections are introduced to compensate for the information loss because of encoding and decoding, which can help the autoencoder to reconstruct denoised images with less artefacts (such as aliasing and patching).\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# skip connections\n",
    "skip_layers = []\n",
    "    \n",
    "# input\n",
    "input_layer = Input((IMG_HEIGHT // N_DECIMATE, IMG_WIDTH // N_DECIMATE, N_CHANNELS))\n",
    "x = input_layer\n",
    "\n",
    "\n",
    "# encoder ------------------------------------------------------------------- #\n",
    "x = Conv2D(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "# encoder ------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# bottleneck ---------------------------------------------------------------- #\n",
    "x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "# bottleneck ---------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# decoder ------------------------------------------------------------------- #\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "# decoder ------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# output\n",
    "x = Conv2DTranspose(filters=1, kernel_size=3, activation='linear', padding='same')(x)\n",
    "\n",
    "# model\n",
    "model = Model(input_layer, x)\n",
    "model.summary()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip connections\n",
    "skip_layers = []\n",
    "    \n",
    "# input\n",
    "input_layer = Input((IMG_HEIGHT // N_DECIMATE, IMG_WIDTH // N_DECIMATE, N_CHANNELS))\n",
    "x = input_layer\n",
    "\n",
    "\n",
    "# encoder ------------------------------------------------------------------- #\n",
    "x = Conv2D(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "skip_layers.append(x)\n",
    "x = MaxPool2D()(x)\n",
    "# encoder ------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# bottleneck ---------------------------------------------------------------- #\n",
    "x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "# bottleneck ---------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# decoder ------------------------------------------------------------------- #\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "\n",
    "x = UpSampling2D()(x)\n",
    "x = Concatenate()([x, skip_layers.pop(-1)])\n",
    "x = Conv2DTranspose(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "x = Conv2DTranspose(filters=8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)  \n",
    "# decoder ------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# output\n",
    "x = Conv2DTranspose(filters=1, kernel_size=3, activation='linear', padding='same')(x)\n",
    "\n",
    "# model\n",
    "model = Model(input_layer, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile the model\n",
    "\n",
    "Now we compile the model using a custom loss function based on the structural similarity index measure ([SSIM](https://en.wikipedia.org/wiki/Structural_similarity)). Compared to pixel-wise measurements (such as mean squared error) that estimate absolute errors, SSIM quantifies the similarity in structural information  and thus better serves our purpose. Our implementation of SSIM is based on `tf.image.ssim`.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# SSIM loss\n",
    "def ssim_loss(y_actual, y_pred):\n",
    "    ssim = tf.image.ssim(y_actual, y_pred, max_val=2.0)\n",
    "    return 1 - tf.reduce_mean(ssim)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss=ssim_loss, metrics=['accuracy'])\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSIM loss\n",
    "def ssim_loss(y_actual, y_pred):\n",
    "    ssim = tf.image.ssim(y_actual, y_pred, max_val=2.0)\n",
    "    return 1 - tf.reduce_mean(ssim)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=.01), \n",
    "              loss=ssim_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "For `N_DECIMATE=4`, we need `epochs=20` to obtain an acceptable denoising quality, which will take about 15 minutes for training. Use more epochs (>100) to improve the accuracy.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# train the model\n",
    "training_history = model.fit(dataset, epochs=20, batch_size=32)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "training_history = model.fit(dataset, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Denoise the images\n",
    "\n",
    "Finally, we can use our trained autoencoder to denoise the images:\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# denoise by predict\n",
    "denoised = model.predict(noisy)\n",
    "    \n",
    "# plot 10 pairs\n",
    "nplot = 10\n",
    "fig, axes = plt.subplots(3, nplot, figsize=(nplot * 2, 6))\n",
    "for ax, img_noise, img_clean, img_denoised in zip(axes.T, noisy[:nplot], clean[:nplot], denoised[:nplot]):\n",
    "    ax[0].matshow(np.squeeze(img_noise))\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].matshow(np.squeeze(img_clean))\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[2].matshow(np.squeeze(img_denoised))\n",
    "    ax[2].set_xticks([])\n",
    "    ax[2].set_yticks([])\n",
    "axes[0, 0].set_ylabel('Noisy', c='k')\n",
    "axes[1, 0].set_ylabel('Clean', c='k')\n",
    "axes[2, 0].set_ylabel('Denoised', c='k')\n",
    "plt.show()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoise by predict\n",
    "denoised = model.predict(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the noisy, clean and denoised images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 10 pairs\n",
    "nplot = 10\n",
    "fig, axes = plt.subplots(3, nplot, figsize=(nplot * 2, 6))\n",
    "for ax, img_noise, img_clean, img_denoised in zip(axes.T, noisy[:nplot], clean[:nplot], denoised[:nplot]):\n",
    "    ax[0].matshow(np.squeeze(img_noise))\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].matshow(np.squeeze(img_clean))\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[2].matshow(np.squeeze(img_denoised))\n",
    "    ax[2].set_xticks([])\n",
    "    ax[2].set_yticks([])\n",
    "axes[0, 0].set_ylabel('Noisy', c='k')\n",
    "axes[1, 0].set_ylabel('Clean', c='k')\n",
    "axes[2, 0].set_ylabel('Denoised', c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Suppose that we have a set of noisy images and a set of clean images but they are completely independent; e.g., they can be obtained from different samples and can be different in size. Such a situation commonly occurs when the \"noisy\" data are collected from the nature while we can synthesise the \"clean\" data in the laboratory. \n",
    "\n",
    "Design and train an autoencoder using the clean images as *both* its input and output and use the autoencoder to denoise the noisy images. Because the input and output are now identical, the network architecture can be simpler and the skip connections are no longer required. Doing everything properly, you will find that the autoencoder trained only with the clean images can outperform the one trained with both for many cases. \n",
    "\n",
    "**Hint**: To get the training dataset with the clean images as both input and output, we can simply do\n",
    "\n",
    "```python\n",
    "dataset_clean_as_both_IO = dataset.map(lambda X, Y: (Y, Y))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
