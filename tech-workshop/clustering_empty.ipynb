{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of clustering-empty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRl0R68G_Rri",
        "colab_type": "text"
      },
      "source": [
        "# Clustering Lesson\n",
        "\n",
        "## Google Cloud Storage Boilerplate\n",
        "\n",
        "This first cell has some boilerplate to connect the Google Cloud Storage bucket containing the data used for this tutorial to the Google Colab environment. \n",
        "\n",
        "In order to access the data for this workshop you'll need to run this cell, follow the link when prompted and copy the Google SDK token into the prompt. If everything works correctly a new folder called `data` should appear in the file browser on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBUZdwZF_Mba",
        "colab_type": "code",
        "outputId": "a248ba4c-9765-4f83-905b-5afc5d15be68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'sciml-workshop'\n",
        "bucket_name = 'sciml-workshop'\n",
        "\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!mkdir data\n",
        "!gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 {bucket_name} /content/data\n",
        "!gsutil cp gs://sciml-workshop/utils.py /content/utils.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0  17648      0 --:--:-- --:--:-- --:--:-- 17648\n",
            "OK\n",
            "58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n",
            "Updated property [core/project].\n",
            "Using mount point: /content/data\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n",
            "Copying gs://sciml-workshop/utils.py...\n",
            "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
            "Operation completed over 1 objects/1.8 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyP7BOtp_GNX",
        "colab_type": "text"
      },
      "source": [
        "# Unsupervised learning\n",
        "\n",
        "In this practical we will look at some methods for clustering un-labelled data.\n",
        "We will start with the simple k-means method for clustering and then progress to the Gaussian Mixture Method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-c_lAA9_GNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from utils import plot_kmeans, plot_gmm\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2PLAfQ_GNe",
        "colab_type": "text"
      },
      "source": [
        "## Generate some data\n",
        "\n",
        "`scikit-learn` allows us to automatically generate a nice unlabeled data set.\n",
        "```\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y_true = make_blobs(n_samples=400, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "X = X[:, ::-1] # flip axes for better plotting\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOtZyoS_GNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx8dHxOh_GNh",
        "colab_type": "text"
      },
      "source": [
        "## Inspect data\n",
        "\n",
        "* Plot the data using `plt.scatter(X[:, 0], X[:, 1])`\n",
        "* Look at the data and decide how many clusters are appropriate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1I2fQHv_GNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxK19PsU_GNl",
        "colab_type": "text"
      },
      "source": [
        "## Cluster the data with k-means\n",
        "\n",
        "* Import the `kmeans` function from `scikit-learn` : `from sklearn.cluster import KMeans`\n",
        "* Now set up an isntance of `Kmeans` with the number n of clusters you choose : `kmeans = KMeans(<n>, random_state=0)`\n",
        "* Fit the lables: `labels = kmeans.fit(X).predict(X)`\n",
        "* Plot the result - colour the points according to cluster: `plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBKWyDNr_GNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIBlyei_GNo",
        "colab_type": "text"
      },
      "source": [
        "## Look at the cluster regions\n",
        "\n",
        "* There is a helper function in `utils` that will plot the region of each cluster\n",
        "* `plot_kmeans(kmeans, X)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmaSU0IZ_GNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSUYyfe2_GNr",
        "colab_type": "text"
      },
      "source": [
        "# Transform the data and cluster\n",
        "\n",
        "* Apply a stretching function to your data using the code:\n",
        "```\n",
        "rng = np.random.RandomState(13)\n",
        "X_stretched = np.dot(X, rng.randn(2, 2))\n",
        "```\n",
        "* Take a look at the resulting raw data with a scatter plot \n",
        "* Try clustering with k-means, use the same functions and plotting function as above - is it satisfactory?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-COi5DOl_GNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cI25w8M_GNv",
        "colab_type": "text"
      },
      "source": [
        "## Introducing the Gaussian mixture method\n",
        "\n",
        " * Import the GMM from scikit-learn: `from sklearn.mixture import GaussianMixture as GMM`\n",
        " * Set up a GMM and fit the dataset `X` with n centres, using the same n as above: `gmm = GMM(n_components=<n>).fit(X)`\n",
        " * Obtain the labels using `labels = gmm.predict(X)`\n",
        " * Plot a scatter plot with the colours based on labels, as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv_1yT6X_GNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-VACMQW_GNy",
        "colab_type": "text"
      },
      "source": [
        "## Look at probabilities\n",
        "\n",
        "But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignmentsâ€”in Scikit-Learn this is done using the `predict_proba` method. This returns a matrix of size `[n_samples, n_clusters]` which measures the probability that any point belongs to the given cluster. use the code below to see the probability assignments of the first five points:\n",
        "\n",
        "```\n",
        "probs = gmm.predict_proba(X)\n",
        "print(probs[:5].round(3))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzBPgmmY_GNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPa9avPf_GN1",
        "colab_type": "text"
      },
      "source": [
        "## Plot with probabilities\n",
        "\n",
        "You can set the size of the points in the plot based on the likelihod that it belongs to a cluster - larger means greater certainty.\n",
        "```\n",
        "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n6Tj0B4_GN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHiak_Gj_GN3",
        "colab_type": "text"
      },
      "source": [
        "## Apply the GMM to the more complicated data set\n",
        "\n",
        "We now use the GMM to try to cluster the more difficult to separate data set.\n",
        "You can run `gmm = GMM(n_components=<n>, covariance_type='full', random_state=6)` with n set to the number of clusters you want.\n",
        "We have a helper function `plot_gmm` to plot the results `plot_gmm(gmm, <input_data>)` here `input_data` will be `X_stretched`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJZ_v7-y_GN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOYkXt0Y_GN7",
        "colab_type": "text"
      },
      "source": [
        "## Choosing covariance type\n",
        "\n",
        "You may have noticed in that we set a parameter called `covariance_type`, this controls the shape of clustering that is possible. It can take values of `spherical`/`diag`/`full`. These get progressively more flexible, but also more expensive to calculate, particularly if the nnumber of clusters is large it can be important to coose the right one. Try playing with this setting and see how the clustering changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wk-urd_GN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqTyetGC_GN-",
        "colab_type": "text"
      },
      "source": [
        "## How many clusters to choose?\n",
        "\n",
        "It seems a little frustrating that we need to choose the number of clusters by hand. Is there a rigorous way of selecting this?\n",
        "\n",
        "The answer is yes - because the GMM gives us a distribution of probabilties it can be used to generate new samples within that distribution. We can then estimate the likelihood that the data we have observed would be generated by a particular GMM. So we can generate a set of GMMs with different numbers of clusters and see which one has the maximum likelihood of generating our observed data.\n",
        "\n",
        "### Step 1\n",
        "\n",
        "Make a list of GMMs with different numbers of clusters\n",
        "```\n",
        "n_components = np.arange(1, 8)\n",
        "models = [GMM(n, covariance_type='full', random_state=6).fit(X_stretched)\n",
        "          for n in n_components]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7aI3rLW_GN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZkhcT9E_GOA",
        "colab_type": "text"
      },
      "source": [
        "### Step 2\n",
        "\n",
        "See how well each of the models matches the data. The GMM in `scikit-learn` has a couple of built-in estimators for telling us exactly this, they are the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\n",
        "\n",
        "We can get a list of `aic` and `bic` values for each of the models\n",
        "```\n",
        "bics = [m.bic(X_stretched) for m in models]\n",
        "aics = [m.aic(X_stretched) for m in models]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWG0vYAa_GOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpEiKwBF_GOG",
        "colab_type": "text"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "Plot your results, the AIC/BIC against the number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ4vReK6_GOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc1DwwVh_GOJ",
        "colab_type": "text"
      },
      "source": [
        "## Extra work\n",
        "\n",
        "If you managed to get through all of that, why not try playing with these methods on a slightly more complicated dataset. The famous 'Two Moons' dataset can be generated in `scikit-learn`. Work out the best number of GMM clusters for replicating this data.\n",
        "\n",
        "```\n",
        "from sklearn.datasets import make_moons\n",
        "Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n",
        "plt.scatter(Xmoon[:, 0], Xmoon[:, 1])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UZr-WwP_GOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}